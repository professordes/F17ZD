<!DOCTYPE html> 
<html lang="en-US" xml:lang="en-US" > 
<head><title></title> 
<meta  charset="utf-8" /> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" /> 
<link rel="stylesheet" type="text/css" href="F17ZD_main.css" /> 
<meta name="src" content="F17ZD_main.tex" /> 
<script>window.MathJax = { tex: { tags: "ams", }, }; </script> 
 <script type="text/javascript" async="async" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"></script>  
</head><body 
>
<div  
class="centerline">                                        <span 
class="rm-lmssbx-10x-x-172">F17ZD</span>                                        </div>
<div  
class="centerline">                   <span 
class="rm-lmssbx-10x-x-172">Mathematics for Data Scientists 4</span>                   </div>
<div  
class="centerline">                                   <span 
class="rm-lmssbx-10x-x-172">Course Notes</span>                                   </div>
<div  
class="centerline">                                  <span 
class="rm-lmssbx-10x-x-120">Semester 2, 2025/26</span>                                  </div>
<div class="center" 
>
<!--l. 9--><p class="noindent" >
</p><!--l. 10--><p class="noindent" ><img 
src="F17ZD_main0x.svg" alt="⃗a⃗b⃗chϕ
"  /></p></div>
                                                                                          
                                                                                          
<!--l. 35--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-144">Name:</span> \(\rule {14cm}{0.15mm}\)
</p><!--l. 53--><p class="noindent" ></p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 53--><p class="noindent" ><span 
class="rm-lmss-10x-x-109">Copyright Heriot Watt University 2026. All rights reserved. For use of students of Xidian</span>
      <span 
class="rm-lmss-10x-x-109">and Heriot Watt Universities enrolled in the course F17ZD.</span>
      </p></li>
      <li class="itemize">
      <!--l. 53--><p class="noindent" ><span 
class="rm-lmss-10x-x-109">Based on notes by Jack Carr, Des Johnston, Alex Martin and Thomas Wong.</span></p></li></ul>
                                                                                          
                                                                                          
<h3 class="likesectionHead"><a 
 id="x1-1000"></a>Contents</h3>
<div class="tableofcontents">
 <span class="sectionToc" >1 <a 
href="#x1-20001" id="QQ2-1-2">Complex Numbers</a></span>
<br />  <span class="subsectionToc" >1.1 <a 
href="#x1-30001.1" id="QQ2-1-3">Definitions and Arithmetic</a></span>
<br />  <span class="subsectionToc" >1.2 <a 
href="#x1-40001.2" id="QQ2-1-4">Solving Quadratic Equations</a></span>
<br />  <span class="subsectionToc" >1.3 <a 
href="#x1-50001.3" id="QQ2-1-5">The Argand Diagram</a></span>
<br />  <span class="subsectionToc" >1.4 <a 
href="#x1-60001.4" id="QQ2-1-6">Polar Form</a></span>
<br />  <span class="subsectionToc" >1.5 <a 
href="#x1-70001.5" id="QQ2-1-7">Exponential Form</a></span>
<br />  <span class="subsectionToc" >1.6 <a 
href="#x1-110001.6" id="QQ2-1-11">De Moivre’s Theorem</a></span>
<br />  <span class="subsectionToc" >1.7 <a 
href="#x1-120001.7" id="QQ2-1-16">Complex Arguments in Trigonometric/Hyperbolic Functions</a></span>
<br />  <span class="subsectionToc" >1.8 <a 
href="#x1-130001.8" id="QQ2-1-17">Complex Mappings</a></span>
<br />  <span class="subsectionToc" >1.9 <a 
href="#x1-140001.9" id="QQ2-1-18">Iterating complex maps</a></span>
<br /> <span class="sectionToc" >2 <a 
href="#x1-150002" id="QQ2-1-19">An aside: Quaternions</a></span>
<br />  <span class="subsectionToc" >2.1 <a 
href="#x1-160002.1" id="QQ2-1-20">Definitions and Arithmetic</a></span>
<br />  <span class="subsectionToc" >2.2 <a 
href="#x1-170002.2" id="QQ2-1-21">Conjugate, Norm and Inverse</a></span>
<br />  <span class="subsectionToc" >2.3 <a 
href="#x1-180002.3" id="QQ2-1-22">Unit Quaternions and 3D Rotations</a></span>
<br /> <span class="sectionToc" >3 <a 
href="#x1-190003" id="QQ2-1-23">Vectors, Spans and Bases in \(\mathds {R}^2\), \(\mathds {R}^3\) and \(\mathds {R}^n\).</a></span>
<br />  <span class="subsectionToc" >3.1 <a 
href="#x1-200003.1" id="QQ2-1-24">Vectors in \(\mathds {R}^2\)</a></span>
<br />  <span class="subsectionToc" >3.2 <a 
href="#x1-210003.2" id="QQ2-1-25">Vectors in \(\mathds {R}^3\)</a></span>
<br />  <span class="subsectionToc" >3.3 <a 
href="#x1-220003.3" id="QQ2-1-26">Vectors in \(\mathds {R}^n\)</a></span>
<br />  <span class="subsectionToc" >3.4 <a 
href="#x1-240003.4" id="QQ2-1-28">Systems of linear equations in vector form.</a></span>
<br />  <span class="subsectionToc" >3.5 <a 
href="#x1-250003.5" id="QQ2-1-29">Spans of vectors and spanning families in \(\mathds {R}^n\)</a></span>
<br />  <span class="subsectionToc" >3.6 <a 
href="#x1-260003.6" id="QQ2-1-30">Spanning families.</a></span>
<br />  <span class="subsectionToc" >3.7 <a 
href="#x1-270003.7" id="QQ2-1-31">Linearly independent families in \(\mathds {R}^n\)</a></span>
<br />  <span class="subsectionToc" >3.8 <a 
href="#x1-280003.8" id="QQ2-1-32">Bases of \(\mathds {R}^n\).</a></span>
<br />  <span class="subsectionToc" >3.9 <a 
href="#x1-290003.9" id="QQ2-1-33">Characterisation of bases of \(\mathds {R}^n\)</a></span>
<br /> <span class="sectionToc" >4 <a 
href="#x1-300004" id="QQ2-1-34">Vector Spaces in General</a></span>
<br />  <span class="subsectionToc" >4.1 <a 
href="#x1-310004.1" id="QQ2-1-35">Examples and definition</a></span>
                                                                                          
                                                                                          
<br />  <span class="subsectionToc" >4.2 <a 
href="#x1-330004.2" id="QQ2-1-37">Linear combinations in abstract vector spaces</a></span>
<br />  <span class="subsectionToc" >4.3 <a 
href="#x1-340004.3" id="QQ2-1-38">Basis and dimension</a></span>
<br />  <span class="subsectionToc" >4.4 <a 
href="#x1-350004.4" id="QQ2-1-39">Dimension of a vector space.</a></span>
<br />  <span class="subsectionToc" >4.5 <a 
href="#x1-360004.5" id="QQ2-1-40">Vector subspaces</a></span>
<br /> <span class="sectionToc" >5 <a 
href="#x1-410005" id="QQ2-1-45">The solution space of a homogeneous system of linear equations.</a></span>
<br /> <span class="sectionToc" >6 <a 
href="#x1-450006" id="QQ2-1-49">Matrices</a></span>
<br />  <span class="subsectionToc" >6.1 <a 
href="#x1-460006.1" id="QQ2-1-50">Image and rank of a matrix.</a></span>
<br />  <span class="subsectionToc" >6.2 <a 
href="#x1-470006.2" id="QQ2-1-51">Rank of a matrix.</a></span>
<br />  <span class="subsectionToc" >6.3 <a 
href="#x1-480006.3" id="QQ2-1-52">Rank and systems of linear equations.</a></span>
<br />  <span class="subsectionToc" >6.4 <a 
href="#x1-490006.4" id="QQ2-1-53">The Rank-Nullity Theorem for matrices.</a></span>
<br /> <span class="sectionToc" >7 <a 
href="#x1-500007" id="QQ2-1-54">Eigenvalues and diagonalisability</a></span>
<br />  <span class="subsectionToc" >7.1 <a 
href="#x1-510007.1" id="QQ2-1-55">A motivating problem: predicting the dynamics of a population</a></span>
<br />  <span class="subsectionToc" >7.2 <a 
href="#x1-520007.2" id="QQ2-1-56">Eigenvectors and Eigenvalues</a></span>
<br />  <span class="subsectionToc" >7.3 <a 
href="#x1-530007.3" id="QQ2-1-57">Diagonalisability.</a></span>
<br />  <span class="subsectionToc" >7.4 <a 
href="#x1-570007.4" id="QQ2-1-61">Eigenspaces and diagonalisability.</a></span>
<br />  <span class="subsectionToc" >7.5 <a 
href="#x1-580007.5" id="QQ2-1-62">Characteristic polynomial</a></span>
<br />  <span class="subsectionToc" >7.6 <a 
href="#x1-620007.6" id="QQ2-1-66">Trace, determinants, and consistency check.</a></span>
<br />  <span class="subsectionToc" >7.7 <a 
href="#x1-630007.7" id="QQ2-1-67">Diagonalisability of a matrix</a></span>
<br />  <span class="subsectionToc" >7.8 <a 
href="#x1-640007.8" id="QQ2-1-68">Application: Dynamical systems</a></span>
<br />  <span class="subsectionToc" >7.9 <a 
href="#x1-650007.9" id="QQ2-1-69">Application: Dynamics of populations.</a></span>
<br />  <span class="subsectionToc" >7.10 <a 
href="#x1-690007.10" id="QQ2-1-73">Powers of a matrix.</a></span>
<br /> <span class="sectionToc" >8 <a 
href="#x1-700008" id="QQ2-1-74">Stochastic Matrices</a></span>
<br />  <span class="subsectionToc" >8.1 <a 
href="#x1-710008.1" id="QQ2-1-75">A motivating problem: Optimising a distribution of bikes</a></span>
<br />  <span class="subsectionToc" >8.2 <a 
href="#x1-720008.2" id="QQ2-1-76">Markov diagrams and transition matrices</a></span>
<br />  <span class="subsectionToc" >8.3 <a 
href="#x1-730008.3" id="QQ2-1-77">The Perron-Frobenius Theorem for stochastic matrices</a></span>
<br />  <span class="subsectionToc" >8.4 <a 
href="#x1-740008.4" id="QQ2-1-78">Application: stationary distributions in dynamical systems.</a></span>
<br />  <span class="subsectionToc" >8.5 <a 
href="#x1-750008.5" id="QQ2-1-79">Application: The PageRank algorithm</a></span>
</div>
                                                                                          
                                                                                          
<!--l. 38--><p class="noindent" >
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p>
<h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-20001"></a>Complex Numbers</h3>
<!--l. 3--><p class="noindent" >To introduce complex numbers, we’ll take a quick (and somewhat abridged) tour through the history of
mathematics. The first numbers that appeared historically (evidence in cave art etc.), and the first
numbers that you learned about, are the ‘counting numbers’ or natural numbers. Aside from counting
objects, </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 7--><p class="noindent" >5 cookies
      </p></li>
      <li class="itemize">
      <!--l. 7--><p class="noindent" >2 sheep, </p></li></ul>
<!--l. 8--><p class="noindent" >basic operations can also be performed, </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 12--><p class="noindent" >add 2 sheep and 3 sheep to get 5 sheep
      </p></li>
      <li class="itemize">
      <!--l. 12--><p class="noindent" >7 cookies take away 3 cookies leaves 4 cookies. </p></li></ul>
<!--l. 13--><p class="noindent" >With these operations comes the ability to solve equations: \(?+3=8\) can be solved with \(?=5\). What about trying to
solve the following? \(?+5=3\).
</p><!--l. 15--><p class="noindent" >We are now all quite happy with the idea of negative numbers and the solution \(-2\). However this hasn’t
always been the case; negative numbers initially appeared to some to be ‘unnatural’ (what does \(-2\) cookies
mean?), and even a recently as the Renaissance in Europe they were still widely mistrusted. But now
their use has been recognised. For example, a bank account with a negative balance makes perfect
sense!
</p><!--l. 17--><p class="noindent" >We now write \(x+3=8\) and solve to get \(x=5\). How about solving the equation \begin {equation*}  x^2-2x-3=0  \end {equation*}
We have a nice formula for solving such equations (courtesy of thousands of years of work dating from
the Babylonians, the Indians, the Greeks, the Chinese, the Persians and the Egyptians):
\begin {equation*}  x =\frac {2\pm \sqrt {4+12}}{2} = 1\pm 2 = -1,3.  \end {equation*}
Now let us try to solve the equation \begin {equation*}  \frac {x^2}{2}-x+1=0.  \end {equation*}
Proceeding as before gives \begin {align*}  x &amp;= \frac {1\pm \sqrt {1-2}}{1} = 1\pm \sqrt {-1}.  \end {align*}
</p><!--l. 33--><p class="noindent" >Up until now, we would stop here and say that the equation has no (real) roots. This is consistent with
most of human history. It took many years from initial work by Bombelli (1572 AD) for them to be
accepted in the work of Euler (1707-1783) and Gauss (1777-1855).
                                                                                          
                                                                                          
</p><!--l. 35--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">1.1   </span> <a 
 id="x1-30001.1"></a>Definitions and Arithmetic</h4>
<!--l. 37--><p class="noindent" >We first define some notation of complex numbers:
</p>
<div class="tcolorbox df" id="tcolobox-1">    
<div class="tcolorbox-title">
<!--l. 44--><p class="noindent" >Definition 1.1  Complex Number</p></div> 
<div class="tcolorbox-content"><!--l. 40--><p class="noindent" >We define \(i = +\sqrt {-1}\) to be the positive square root of \(-1\). So \(i^2 = -1\). A <span 
class="rm-lmsso-12">complex number</span>, \(z\), is any expression of the
form \(z=a+ib\) for real numbers \(a\) and \(b\). For a complex number \(z=a+ib\), we say \(a\) is the <span 
class="rm-lmsso-12">real part </span>and \(b\) is the
<span 
class="rm-lmsso-12">imaginary part</span>. This is denoted \begin {align*}  \RE {z} &amp;= a &amp; \IM {z} = b  \end {align*}
</p>
 
</div> 
</div>
<img 
src="F17ZD_main1x.svg" alt="  "  />The definition allows for \(b=0\). Hence all real numbers are included in the definition of complex
numbers.
<!--l. 47--><p class="noindent" ><img 
src="F17ZD_main2x.svg" alt="  "  />In
Electrical Engineering the notation \(j=\sqrt {-1}\) often used instead of \(i\).
</p>
<!--l. 72--><p class="noindent" >
</p><!--l. 72--><p class="noindent" ><a 
 id="x1-3002r2"></a><!--l. 51--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.2</span>                                                                           </p><!--l. 51--><p class="noindent" >Examples of complex numbers include: \begin {align*}  5+i2,&amp;&amp;7-i,&amp;&amp;-4+3i,&amp;&amp; e,&amp;&amp;3, &amp;&amp;i\pi .  \end {align*}
</p><!--l. 55--><p class="noindent" >The real and imaginary parts of the above numbers are as follows: </p>
<div class="center" 
>
<!--l. 56--><p class="noindent" >
</p>
<div class="tabular"> <table id="TBL-1" class="tabular" 
 
><colgroup id="TBL-1-1g"><col 
id="TBL-1-1" /></colgroup><colgroup id="TBL-1-2g"><col 
id="TBL-1-2" /></colgroup><colgroup id="TBL-1-3g"><col 
id="TBL-1-3" /></colgroup><colgroup id="TBL-1-4g"><col 
id="TBL-1-4" /></colgroup><colgroup id="TBL-1-5g"><col 
id="TBL-1-5" /></colgroup><colgroup id="TBL-1-6g"><col 
id="TBL-1-6" /></colgroup><colgroup id="TBL-1-7g"><col 
id="TBL-1-7" /></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-1-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-1"  
class="td11"> \(z\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-2"  
class="td11"> \(5+i2\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-3"  
class="td11">  \(7-i\)  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-4"  
class="td11">  \(-4+3i\)  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-5"  
class="td11"> \(e\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-6"  
class="td11"> 3  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-7"  
class="td11"> \(i\pi \) </td></tr><tr class="hline" style="border-top:1px solid #000"><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-1"  
class="td11">  \(\RE {z}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-2"  
class="td11"> 5 </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-3"  
class="td11"> 7 </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-4"  
class="td11"> -4 </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-5"  
class="td11">  \(e\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-6"  
class="td11">  \(3\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-7"  
class="td11"> 0</td>
</tr><tr class="hline" style="border-top:1px solid #000"><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-1"  
class="td11"> \(\IM {z}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-2"  
class="td11"> 2  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-3"  
class="td11"> -1  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-4"  
class="td11"> 3  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-5"  
class="td11"> 0  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-6"  
class="td11"> 0  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-1-3-7"  
class="td11"> \(\pi \) </td></tr></table>
</div></div>
<!--l. 66--><p class="noindent" >Notes: </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 71--><p class="noindent" >\(7-i\) is the same as \(7+(-1)i\).
      </p></li>
      <li class="itemize">
      <!--l. 71--><p class="noindent" >\(i3=3i\).                                                                                  
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 72--><p class="noindent" >      </p></li>
      <li class="itemize">
      <!--l. 71--><p class="noindent" >\(e=e+i0\) and \(i\pi =0+i\pi \) </p></li></ul>
 
</div> 
</div>                                                                                        
</p><!--l. 74--><p class="noindent" >As for real numbers, we can apply some basic arithmetic operations to complex numbers.
</p>
<div class="tcolorbox df" id="tcolobox-2">    
<div class="tcolorbox-title">
<!--l. 78--><p class="noindent" >Definition 1.3  Equality</p></div> 
<div class="tcolorbox-content"><!--l. 77--><p class="noindent" >Two complex numbers \(z = a+ib\) and \(w = c+id\) are equal, denoted \(z=w\) if \(a=c\) <span 
class="rm-lmsso-12">and</span> \(b=d\). </p> 
</div> 
</div>
<!--l. 80--><p class="noindent" >For example: We have \(1+2i=2i+1\) and \(1+2i \neq 2+2i\).
</p><!--l. 82--><p class="noindent" >Basic operations on complex numbers behave the way we expect as per real numbers
</p>
<div class="tcolorbox df" id="tcolobox-3">    
<div class="tcolorbox-title">
<!--l. 103--><p class="noindent" >Definition 1.4  Basic operations</p></div> 
<div class="tcolorbox-content"><!--l. 85--><p class="noindent" >Given complex numbers \(z=a+ib\) and \(w=c+id\), we have the following operations: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 102--><p class="noindent" >Addition: \begin {align*}  z+w &amp;= (a+ib)+(c+id)\\ &amp;=(a+c)+i(b+d)  \end {align*}
        </p></li>
        <li class="itemize">
        <!--l. 102--><p class="noindent" >Subtraction: \begin {align*}  z-w &amp;= (a+ib)-(c+id)\\ &amp;=(a-c)+i(b-d)  \end {align*}
        </p></li>
        <li class="itemize">
        <!--l. 102--><p class="noindent" >Multiplication: \begin {align*}  z\cdot w &amp;= (a+ib)\cdot (c+id)\\ &amp;=(ac-bd)+i(ad+bc)  \end {align*}
</p>
        </li></ul>
 
</div> 
</div>
<!--l. 105--><p class="noindent" >We can verify these operations by expanding the brackets and then regrouping by real and imaginary
components.
</p>
<!--l. 115--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="tcolorbox example" id="tcolobox-4">    
<div class="tcolorbox-title">
<!--l. 115--><p class="noindent" >Example 1.5</p></div> 
<div class="tcolorbox-content"></p><!--l. 108--><p class="noindent" >Take complex multiplication for example: \begin {align*}  (a+ib)\cdot (c+id)&amp;= ac + iad + ibc + i^2bd\\ &amp;= ac+iad+ibc-bd\\ &amp;=(ac-bd)+i(ad+bc)  \end {align*}
</p><!--l. 114--><p class="noindent" >where we used the identity \(i^2=-1\). </p> 
</div> 
</div>                                                                                 
<!--l. 135--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-5">    
<div class="tcolorbox-title">
<!--l. 135--><p class="noindent" >Example 1.6</p></div> 
<div class="tcolorbox-content"></p><!--l. 118--><p class="noindent" >Addition and subtraction: \begin {align*}  (5+4i)+(2-3i)&amp;=(5+2)+(4-3)i =7+i\\ (\pi +2i)-(\pi +i)&amp;=(\pi -\pi )+(2-1)i =i  \end {align*}
</p><!--l. 124--><p class="noindent" >Multiplication: \begin {align*}  (1-i)(2+3i)&amp;=2+3+3i-2i =5+i.\\ (1-i)^2=(1-i)(1-i)&amp;=(1-1)+i(-1-1)=-2i\\ (1+2i)(1-2i)&amp;=(1+4)+i(2-2) =5.  \end {align*}
</p><!--l. 131--><p class="noindent" >We can also combine operations together using our standard order of operations \begin {align*}  (1+i)\left ((2-3i)+(12+8i)\right ) &amp;= (1+i)(14+5i) =9+19i.  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 137--><p class="noindent" >You’ll notice that division is not on this list yet. That’s because division is a bit more complicated (as for real
numbers)<span class="footnote-mark"><a 
href="F17ZD_main2.html#fn1x0"><sup class="textsuperscript">1</sup></a></span><a 
 id="x1-3007f1"></a> .
Recall from real numbers, we can convert division into multiplication by using the reciprocal. That is \(\frac {a}{b} = a \times \frac {1}{b}\).
For complex numbers we will use a similar approach. Given \(z = a+ib\), we will attempt to find a number \(w = p+iq\) such that
\(w = \frac {1}{z}\).
</p><!--l. 139--><p class="noindent" >Before we get to that, we will introduce a new concept that will be useful
</p>
<div class="tcolorbox df" id="tcolobox-6">    
<div class="tcolorbox-title">
<!--l. 147--><p class="noindent" >Definition 1.7  Complex conjugate</p></div> 
<div class="tcolorbox-content"><!--l. 142--><p class="noindent" >Given a complex number \(z = a+ib\), the complex conjugate, \(\conj {z}\), is defined to be \begin {equation*}  \conj {z} = a - ib  \end {equation*}
In particular, we have \(\RE {\conj {z}} = \RE {z}\) and \(\IM {\conj {z}} = -\IM {z}\). </p> 
</div> 
</div>
<img 
src="F17ZD_main3x.svg" alt=" "  />Some resources will use \(z^*\) to denote complex conjugate
<!--l. 157--><p class="noindent" >
</p><!--l. 157--><p class="noindent" ><a 
 id="x1-3009r8"></a><!--l. 151--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.8</span>                                                                           </p><!--l. 151--><p class="noindent" ></p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 155--><p class="noindent" >If \(z=1+i\) then \(\conj {z}=1-i\).
      </p></li>
      <li class="itemize">
      <!--l. 155--><p class="noindent" >If \(z=2i\), then \(\conj {z}=-2i\).                                                                         
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 157--><p class="noindent" >      </p></li>
      <li class="itemize">
      <!--l. 155--><p class="noindent" >If \(z=4\), then \(\conj {z}=4\). </p></li></ul>
<!--l. 156--><p class="noindent" >Note that if \(r\) is a real number (i.e. \(\IM {r} = 0\)), then \(r = \conj {r}\). This is why the conjugate does not make an appearance
prior to our study of complex numbers. </p> 
</div> 
</div>                                                  
</p><!--l. 159--><p class="noindent" >Equipped with the complex conjugate, we can now rewrite the reciprocal of a complex number.
</p>
<!--l. 174--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-7">    
<div class="tcolorbox-title">
<!--l. 174--><p class="noindent" >Example 1.9</p></div> 
<div class="tcolorbox-content"></p><!--l. 162--><p class="noindent" >Let \(z = a+ib\). We can rewrite \(\frac {1}{z}\) by multiplying by 1 in a specific way. \begin {align*}  \frac {1}{z} &amp;= \frac {1}{a+ib}\\ &amp;= \frac {1}{a+ib} \cdot \frac {a-ib}{a-ib} &amp; {\text {Complex Conjugate}}\\ &amp;= \frac {a-ib}{(a+ib)(a-ib)}\\ &amp;= \frac {a-ib}{a^2 + b^2}\\ &amp;= \frac {a}{a^2 + b^2} + i \frac {-b}{a^2 + b^2}  \end {align*}
</p><!--l. 170--><p class="noindent" >In particular, if \(w = \frac {1}{z}\), we can express \(w = p+iq\) with \begin {align*}  p &amp;=\frac {a}{a^2 + b^2} &amp; q=&amp;\frac {-b}{a^2 + b^2}  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 175--><p class="noindent" >The essence of this process is to transfer the imaginary number \(i\) from the denominator to the numerator.
The denominator \(a^2 +b^2\) is derived from the difference of squares and is guaranteed to be a real number. This
process is often referred to as <span 
class="rm-lmsso-12">realising the denominator</span>.
</p><!--l. 177--><p class="noindent" >Further, the denominator tells us that \(z\conj {z} = a^2 +b^2\) which will be a real number.
</p>
<div class="tcolorbox exercise" id="tcolobox-8">    
<div class="tcolorbox-title">
<!--l. 181--><p class="noindent" >Exercise 1.10 </p></div> 
<div class="tcolorbox-content"><!--l. 180--><p class="noindent" >Let \(z = 1+2i\). Express \(\frac {1}{z}\) as a complex number \(w = c+id\). </p> 
</div> 
</div>
<!--l. 189--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-9">    
<div class="tcolorbox-title">
<!--l. 189--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 183--><p class="noindent" >Using the derivation we had above: \begin {align*}  \frac {1}{z} &amp;= \frac {1}{1+2i}\\ &amp;= \frac {1}{1^2 + 2^2} + i \frac {-2}{1^2 + 2^2}\\ &amp;= \frac {1}{5} + i \frac {-2}{5}  \end {align*}
</p>
 
</div> 
</div>                                                                                                               
<!--l. 191--><p class="noindent" >We can also fill in the details as we go. </p>
<div class="tcolorbox exercise" id="tcolobox-10">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 194--><p class="noindent" >Exercise 1.11 </p></div> 
<div class="tcolorbox-content"><!--l. 193--><p class="noindent" >Express \(\frac {1}{3+4i}\) in the form \(a+ib\). </p> 
</div> 
</div>
<!--l. 202--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-11">    
<div class="tcolorbox-title">
<!--l. 202--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 196--><p class="noindent" >The complex conjugate of \(3+4i\) is \(3-4i\). Hence, we have \begin {align*}  \frac {1}{3+4i} &amp;= \frac {1}{3+4i}\cdot \frac {3-4i}{3-4i}\\ &amp;=\frac {3-4i}{3^2+4^2}\\ &amp;=\frac {3}{25}-\frac {4}{25}i  \end {align*}
</p>
 
</div> 
</div>                                                                                                               
<div class="tcolorbox exercise" id="tcolobox-12">    
<div class="tcolorbox-title">
<!--l. 206--><p class="noindent" >Exercise 1.12 </p></div> 
<div class="tcolorbox-content"><!--l. 205--><p class="noindent" >Write \(\frac {1}{i}\) in the form \(a+ib\). </p> 
</div> 
</div>
<!--l. 211--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-13">    
<div class="tcolorbox-title">
<!--l. 211--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 210--><p class="noindent" >\begin {equation*}  \frac {1}{i}=\frac {1}{i} \frac {-i}{(-i)^2}=\frac {-i}{1}=-i.  \end {equation*}
</p> 
</div> 
</div>                                                                                                               
<!--l. 212--><p class="noindent" >This property \(\frac {1}{i}=-i\) is a particularly useful one to remember.
</p><!--l. 214--><p class="noindent" >With this in mind, we can now look at complex division
</p>
<!--l. 226--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-14">   <a 
 id="x1-3015x1.1"></a> 
<div class="tcolorbox-title">
<!--l. 226--><p class="noindent" >Example 1.13</p></div> 
<div class="tcolorbox-content"></p><!--l. 216--><p class="noindent" >To evaluate the following: \begin {align*}  \frac {4+3i}{1+2i} &amp;= (4+3i) \cdot \frac {1}{1+2i}\\ &amp;= (4+3i) \cdot \left (\frac {1}{5} + i \frac {-2}{5}\right )\\ &amp;= \left (4\cdot \frac {1}{5} - 3\cdot \frac {-2}{5}\right ) + i \left (4\frac {-2}{5} + 3\cdot \frac {1}{5}\right )\\ &amp;= \left (\frac {4}{5} +\frac {6}{5}\right ) + i \left (\frac {-8}{5} + \frac {3}{5}\right )\\ &amp;= \frac {10}{5} + i \frac {-5}{5}\\ &amp;=2-i.  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 235--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-15">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 235--><p class="noindent" >Example 1.14</p></div> 
<div class="tcolorbox-content"></p><!--l. 229--><p class="noindent" >The algebraic operations for complex numbers satisfy the <span 
class="rm-lmsso-12">same properties </span>as the corresponding
operations for real numbers. For example \begin {align*}  z_1(z_2+z_3)&amp;=z_1z_2+z_1z_3\\ z_1 z_2 &amp;= z_2 z_1\\ z^{-3}&amp;=\frac {1}{z^3}  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
                                                                                          
                                                                                          
<h4 class="subsectionHead"><span class="titlemark">1.2   </span> <a 
 id="x1-40001.2"></a>Solving Quadratic Equations</h4>
<!--l. 241--><p class="noindent" >We can extend the definition of \(i\) to take the square root of any negative number.
</p>
<!--l. 248--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-16">    
<div class="tcolorbox-title">
<!--l. 248--><p class="noindent" >Example 1.15</p></div> 
<div class="tcolorbox-content"></p><!--l. 244--><p class="noindent" >If we have \(d &gt;0\), then \(-d &lt;0\), so we have \begin {equation*}  \sqrt {-d}=\sqrt {(-1)\cdot d}= \sqrt {-1} \cdot \sqrt {d} = i\sqrt {d}  \end {equation*}
</p> 
</div> 
</div>                                                                                                                      
<div class="tcolorbox exercise" id="tcolobox-17">    
<div class="tcolorbox-title">
<!--l. 252--><p class="noindent" >Exercise 1.16 </p></div> 
<div class="tcolorbox-content"><!--l. 251--><p class="noindent" >Solve the equation \(z^2+16=0\) </p> 
</div> 
</div>
<!--l. 263--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-18">    
<div class="tcolorbox-title">
<!--l. 263--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 254--><p class="noindent" >We isolate \(z\) by rewriting \begin {align*}  z^2 +16 &amp;= 0\\ z^2 &amp;=-16\\ z&amp; =\pm \sqrt {-16}\\ z&amp; =\pm \sqrt {16}\sqrt {-1}\\ z&amp; =\pm 4\sqrt {-1}\\ z&amp; =\pm 4i.  \end {align*}
</p>
 
</div> 
</div>                                                                                                               
<!--l. 265--><p class="noindent" >With complex numbers, we can revisit our knowledge of solving quadratics using the quadratic
formula.
</p>
<div class="tcolorbox thm" id="tcolobox-19">    
<div class="tcolorbox-title">
<!--l. 278--><p class="noindent" >Theorem 1.17  Quadratic formula</p></div> 
<div class="tcolorbox-content"><!--l. 268--><p class="noindent" >Given the quadratic equation \(az^2+bz+c=0\), with real constants \(a,b,c\). The solutions \(z\) are given by \begin {equation*}  z= \frac {-b\pm \sqrt {b^2-4ac\,}}{2a}\,.  \end {equation*}
The quantity under the square root \(\Delta = b^2 - 4ac\) is called the <span 
class="rm-lmsso-12">discriminant</span>. In particular: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 274--><p class="noindent" >If \(\Delta &gt;0\), the two solutions are <span 
class="rm-lmsso-12">real and distinct</span>.
        </p></li>
        <li class="itemize">
        <!--l. 275--><p class="noindent" >If \(\Delta =0\), the two solutions are <span 
class="rm-lmsso-12">real and overlap</span>.
        </p></li>
        <li class="itemize">
        <!--l. 276--><p class="noindent" >If \(\Delta &lt;0\), the two solutions are <span 
class="rm-lmsso-12">complex and distinct</span>.</p></li></ul>
 
</div> 
</div>
                                                                                          
                                                                                          
<!--l. 280--><p class="noindent" >The first two cases for the discriminant should be familiar. In the case for complex solutions \(z_1, z_2\), we have
the additional property that \(\conj {z_1}=z_2\). That is the two complex solutions form a <span 
class="rm-lmsso-12">complex conjugate</span>
<span 
class="rm-lmsso-12">pair</span>.
</p>
<div class="tcolorbox exercise" id="tcolobox-20">    
<div class="tcolorbox-title">
<!--l. 284--><p class="noindent" >Exercise 1.18 </p></div> 
<div class="tcolorbox-content"><!--l. 283--><p class="noindent" >Solve the equation \(z^2+2z+2=0\) </p> 
</div> 
</div>
<!--l. 294--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-21">    
<div class="tcolorbox-title">
<!--l. 294--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 286--><p class="noindent" >The quadratic formula gives \begin {align*}  z&amp;=\frac {-2\pm \sqrt {2^2-4\times 1\times 2}}{2},\\ &amp;= \frac {-2\pm \sqrt {-4}}{2}\\ &amp;=\frac {-2\pm 2\sqrt {-1}}{2}\\ &amp;=-1\pm \sqrt {-1}=-1\pm i.  \end {align*}
</p><!--l. 293--><p class="noindent" >Thus the solutions are \(z=-1+i\) and \(z=-1-i\). </p> 
</div> 
</div>                                                                          
<div class="tcolorbox exercise" id="tcolobox-22">    
<div class="tcolorbox-title">
<!--l. 299--><p class="noindent" >Exercise 1.19 </p></div> 
<div class="tcolorbox-content"><!--l. 297--><p class="noindent" >Find the 2 solutions \(z_1,z_2\) of the equation \(z^2+14 z +58=0\), and check that \(\conj {z_1} =z_2\). </p> 
</div> 
</div>
<!--l. 308--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-23">    
<div class="tcolorbox-title">
<!--l. 308--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 301--><p class="noindent" >From the quadratic formula \begin {align*}  z&amp;=\frac {-14 \pm \sqrt {(14)^2-4\times 58}}{2}\\ &amp;= \frac {-14 \pm \sqrt {-36}}{2}\\ &amp;= \frac {-14 \pm 6\sqrt {-1}}{2} = -7 \pm 3i  \end {align*}
</p><!--l. 307--><p class="noindent" >The two solutions are \(z_1=-7+3i\) and \(z_2=-7-3i\). We also have \(\conj {z_1}=-7-3i=z_2\). </p> 
</div> 
</div>                                                       
                                                                                          
                                                                                          
<!--l. 312--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">1.3   </span> <a 
 id="x1-50001.3"></a>The Argand Diagram</h4>
<!--l. 314--><p class="noindent" >Operations on complex numbers lend themselves very nicely to visual interpretations. To construct a
visual representation of complex numbers, we start with the real number line.
</p>
<!--l. 356--><p class="noindent" >
</p><!--l. 356--><p class="noindent" ><a 
 id="x1-5001r20"></a><!--l. 317--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.20</span>                                                                          </p><!--l. 317--><p class="noindent" >We can think of real numbers as points along the real number line. </p>
<div class="center" 
>
<!--l. 318--><p class="noindent" >
</p><!--l. 319--><p class="noindent" ><img 
src="F17ZD_main4x.svg" alt="√--
-3-2-1012345678−0π6 22
"  /></p></div>
<!--l. 330--><p class="noindent" >Inequalities in real numbers translates to intervals along the real number line. For example:
</p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 355--><p class="noindent" >The interval \(-1 &lt; x \leq 3\) translate too </p>
      <div class="center" 
>
<!--l. 355--><p class="noindent" >
</p><!--l. 355--><p class="noindent" ><img 
src="F17ZD_main5x.svg" alt="−−−012345678 3 2 1
"  /></p></div>
      </li>
      <li class="itemize">
      <!--l. 355--><p class="noindent" >The interval \(1\leq x &lt; 6\) translates to </p>
      <div class="center" 
>
<!--l. 355--><p class="noindent" >
</p><!--l. 355--><p class="noindent" ><img 
src="F17ZD_main6x.svg" alt="−−−012345678 3 2 1
"  /></p></div>                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 356--><p class="noindent" >      </li></ul>
 
</div> 
</div>                                                                                        
</p><!--l. 358--><p class="noindent" >We can extend this idea to complex numbers. Since two pieces of data is required to describe a complex
number (a real part and an imaginary part). We can use those two data as co-ordinates on the plane.
The complex number \(z = a+ib\) is represented by the point with co-ordinates \((a,b)\) in the plane. Complex numbers
written in \(z=a+ib\) is often called <span 
class="rm-lmsso-12">Cartesian form </span>for its connection with coordinates in the complex
plane.
</p>
<div class="center" 
>
<!--l. 361--><p class="noindent" >
</p><!--l. 362--><p class="noindent" ><img 
src="F17ZD_main7x.svg" alt="RI------123456----1234−2−4em65432143215+3−∖∖ + −lli2eeiff33ttii((zz∖∖rriigghhtt))
"  /></p></div>
<!--l. 376--><p class="noindent" >This is known as an <span 
class="rm-lmsso-12">Argand Diagram </span>or the <span 
class="rm-lmsso-12">complex plane</span>. The Argand diagram provides a simple visual
way of representing many of the key properties of complex numbers.
</p>
<!--l. 394--><p class="noindent" >
</p><!--l. 394--><p class="noindent" ><a 
 id="x1-5002r21"></a><!--l. 379--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.21</span>                                                                          </p><!--l. 379--><p class="noindent" >Let  \(z\) be a complex number. The Argand diagram of  \(\conj {z}\), the complex conjugate of  \(z\), is the point obtained
by reflection in the real axis. </p>
<div class="center" 
>
<!--l. 380--><p class="noindent" >                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 394--><p class="noindent" ></p><!--l. 381--><p class="noindent" ><img 
src="F17ZD_main8x.svg" alt="RIm-6-5-4-3-2-1123456-4-3-2-11234zzwwe∖∖lleefftt((zz∖∖rriigghhtt))
"  /></p></div>
 
</div> 
</div>                                                                                        
</p>
<div class="tcolorbox exercise" id="tcolobox-24">    
<div class="tcolorbox-title">
<!--l. 398--><p class="noindent" >Exercise 1.22 </p></div> 
<div class="tcolorbox-content"><!--l. 397--><p class="noindent" >Plot all complex numbers of the form \(z = a+3i\) for real numbers \(a\) </p> 
</div> 
</div>
<!--l. 415--><p class="noindent" >
</p><!--l. 415--><p class="noindent" ><!--l. 399--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Solution:</span>                                                                          </p><!--l. 401--><p class="noindent" >The imaginary part of  \(z\) is always 3 while the real part can vary. This leads to the following
picture:
</p>
<div class="center" 
>
<!--l. 403--><p class="noindent" >
</p><!--l. 404--><p class="noindent" ><img 
src="F17ZD_main9x.svg" alt="RIm-6-5-4-3-2-1123456-4-3-2-1124ze =∖∖lleeaff +tt((zz3∖∖irriigghhtt))
"  /></p></div>
 
</div> 
</div>                                                                                   
</p>
<div class="tcolorbox exercise" id="tcolobox-25">   <a 
 id="x1-5005x1.3"></a> 
<div class="tcolorbox-title">
<!--l. 419--><p class="noindent" >Exercise 1.23 </p></div> 
<div class="tcolorbox-content"><!--l. 418--><p class="noindent" >Plot all complex numbers \(z = a+ib\) with the property that \(a^2+b^2=9\). </p> 
</div> 
</div>
<!--l. 438--><p class="noindent" >
</p><!--l. 438--><p class="noindent" ><!--l. 421--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Solution:</span>                                                                          </p><!--l. 422--><p class="noindent" >Recall that  \(a^2+b^2=9\) is the equation of a circle with radius  \(3\). Thus a complex number  \(a+ib\) satisfies  \(a^2+b^2=9\) if
it lies on this circle.
</p>
<div class="center" 
>
<!--l. 425--><p class="noindent" >
</p><!--l. 426--><p class="noindent" ><img 
src="F17ZD_main10x.svg" alt=" 2   2
RIm-6-5-4-3-2-1123456-4-3-2-11234ae∖∖+lleebfftt((=zz∖∖9rriigghhtt))
"  /></p></div>
<!--l. 437--><p class="noindent" >Each point on the circle is a complex number satisfying the requirement. </p> 
</div> 
</div>              
</p>
<div class="tcolorbox exercise" id="tcolobox-26">   <a 
 id="x1-5007x1.3"></a> 
<div class="tcolorbox-title">
<!--l. 443--><p class="noindent" >Exercise 1.24 </p></div> 
<div class="tcolorbox-content"><!--l. 441--><p class="noindent" >Plot all complex numbers \(z = a+ib\) with the property that \(z \bar {z}=9\). </p> 
</div> 
</div>
<!--l. 461--><p class="noindent" >
</p><!--l. 461--><p class="noindent" ><!--l. 445--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Solution:</span>                                                                          </p><!--l. 446--><p class="noindent" >This is a retread of the previous question since  \(z \bar {z}= a^2+b^2=9\) is the equation of a circle with radius  \(3\).
</p>
<div class="center" 
>
<!--l. 448--><p class="noindent" >
</p><!--l. 449--><p class="noindent" ><img 
src="F17ZD_main11x.svg" alt="RIm-6-5-4-3-2-1123456-4-3-2-11234ae2∖∖ +lleebff2tt((=zz∖∖9rriigghhtt))
"  /></p></div>                                                                                   
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 461--><p class="noindent" > 
</div> 
</div>                                                                                   
                                                                                          
                                                                                          
</p><!--l. 465--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">1.4   </span> <a 
 id="x1-60001.4"></a>Polar Form</h4>
<!--l. 467--><p class="noindent" >Now that we have a visual representation of complex numbers, we can ask new
questions about them. For example, what is the <span 
class="rm-lmsso-12">size </span>of a complex number. One way to
define size is to take the distance between the number and the origin of the Argand
diagram<span class="footnote-mark"><a 
href="F17ZD_main3.html#fn2x0"><sup class="textsuperscript">2</sup></a></span><a 
 id="x1-6001f2"></a> .
</p>
<div class="center" 
>
<!--l. 468--><p class="noindent" >
</p><!--l. 469--><p class="noindent" ><img 
src="F17ZD_main12x.svg" alt="RImzre =∖∖lleeaff +tt((zzib∖∖rriigghhtt))
"  /></p></div>
<!--l. 476--><p class="noindent" >Using Pythagoras’ Theorem we get \begin {align*}  r^2&amp;=a^2+b^2 &amp;r&amp;=\sqrt {a^2+b^2}  \end {align*}
</p>
<div class="tcolorbox df" id="tcolobox-27">    
<div class="tcolorbox-title">
<!--l. 486--><p class="noindent" >Definition 1.25  Modulus</p></div> 
<div class="tcolorbox-content"><!--l. 482--><p class="noindent" >The <span 
class="rm-lmsso-12">modulus </span>of the complex number \(z =a+ib\) is denoted \(\abs {z} = \abs {a+ib}\) and is defined by \begin {align*}  \abs {z} &amp;= \abs {a+ib} = \sqrt {a^2+b^2}  \end {align*}
</p>
 
</div> 
</div>
<div class="tcolorbox exercise" id="tcolobox-28">    
<div class="tcolorbox-title">
<!--l. 490--><p class="noindent" >Exercise 1.26 </p></div> 
<div class="tcolorbox-content"><!--l. 489--><p class="noindent" >If \(z=3-i\) calculate \(\abs {z}\). </p> 
</div> 
</div>
                                                                                          
                                                                                          
<!--l. 496--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-29">    
<div class="tcolorbox-title">
<!--l. 496--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 492--><p class="noindent" >Since \(z=a+ib\) with \(a=3\) and \(b=-1\), we have \begin {align*}  \abs {z} &amp;=\sqrt {3^2+(-1)^2} =\sqrt {10}  \end {align*}
</p>
 
</div> 
</div>                                                                                                               
<div class="tcolorbox exercise" id="tcolobox-30">    
<div class="tcolorbox-title">
<!--l. 506--><p class="noindent" >Exercise 1.27 </p></div> 
<div class="tcolorbox-content"><!--l. 500--><p class="noindent" >Find the modulus of the following complex numbers
        </p><dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
    1.  </dt><dd 
class="enumerate-enumitem">\(z = i\)
        </dd><dt class="enumerate-enumitem">
    2.  </dt><dd 
class="enumerate-enumitem">\(z = -9\),
        </dd><dt class="enumerate-enumitem">
    3.  </dt><dd 
class="enumerate-enumitem">\(z = \abs {\frac {1}{\sqrt {2}}- \frac {1}{\sqrt {2}}i}\) </dd></dl>
 
</div> 
</div>
<!--l. 514--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-31">    
<div class="tcolorbox-title">
<!--l. 514--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 508--><p class="noindent" >Applying the theorem, we have
        </p><dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
    1.  </dt><dd 
class="enumerate-enumitem">\(\abs {i}=\sqrt {0^2+1^2}=1\)
        </dd><dt class="enumerate-enumitem">
    2.  </dt><dd 
class="enumerate-enumitem">\(\abs {-9}=\sqrt {9^2+0^2}=9\)
        </dd><dt class="enumerate-enumitem">
    3.  </dt><dd 
class="enumerate-enumitem">\(\displaystyle \abs {\frac {1}{\sqrt {2}}- \frac {1}{\sqrt {2}}i}= \sqrt {\frac {1}{2}+\frac {1}{2}}=1.\) </dd></dl>
 
</div> 
</div>                                                                                                               
<div class="tcolorbox exercise" id="tcolobox-32">    
<div class="tcolorbox-title">
<!--l. 518--><p class="noindent" >Exercise 1.28 </p></div> 
<div class="tcolorbox-content"><!--l. 517--><p class="noindent" >In the Argand diagram, draw the set of all complex numbers \(z\) with modulus \(\abs {z} = \sqrt {2}\). </p> 
</div> 
</div>
<!--l. 521--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="tcolorbox solution" id="tcolobox-33">    
<div class="tcolorbox-title">
<!--l. 521--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 520--><p class="noindent" >A complex number \(z = a+ib\) has modulus \(\abs {z} = \sqrt {2}\) if \(\sqrt {a^2+b^2}=\sqrt {2}\) or \(a^2+b^2=2\) (See <a 
href="#x1-5005x1.3">subsection 1.3</a>). </p> 
</div> 
</div>                                
<!--l. 525--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-34">    
<div class="tcolorbox-title">
<!--l. 525--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 524--><p class="noindent" >(Alternate) We know that the modulus of a complex number gives the distance to the
origin. Thus we want all the points at distance \(\sqrt 2\) from the origin. This again leads to the
circle of radius \(\sqrt 2\) (See <a 
href="#x1-5005x1.3">subsection 1.3</a>). </p> 
</div> 
</div>                                                               
<div class="tcolorbox exercise" id="tcolobox-35">   <a 
 id="x1-6013x1.4"></a> 
<div class="tcolorbox-title">
<!--l. 529--><p class="noindent" >Exercise 1.29 </p></div> 
<div class="tcolorbox-content"><!--l. 528--><p class="noindent" >Plot the set of complex numbers \(z\) satisfying \(1 &lt; |z| \leq 2\) in the complex plane. </p> 
</div> 
</div>
<!--l. 546--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-36">    
<div class="tcolorbox-title">
<!--l. 546--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 531--><p class="noindent" >Since the modulus is the distance to the origin the set consists of all those complex
numbers which are at a distance between \(1\) and \(2\) from the origin. This gives the area shown
below.
</p>
<div class="center" 
>
<!--l. 534--><p class="noindent" >
</p><!--l. 535--><p class="noindent" ><img 
src="F17ZD_main13x.svg" alt="RIem ∖∖lleefftt((zz∖∖rriigghhtt))
"  /></p></div>
 
</div> 
</div>                                                                                                               
<!--l. 547--><p class="noindent" ><img 
src="F17ZD_main14x.svg" alt="
"  />Notice the difference between the inner and outer boundaries. A solid line indicates
that the boundary is included and a dashed line indicates that the boundary is not
                                                                                          
                                                                                          
included<span class="footnote-mark"><a 
href="F17ZD_main4.html#fn3x0"><sup class="textsuperscript">3</sup></a></span><a 
 id="x1-6014f3"></a> .
</p><!--l. 549--><p class="noindent" >To uniquely define a complex number with the modulus, we would need another piece of information.
This second piece of information is called the <span 
class="rm-lmsso-12">argument</span>.
</p>
<div class="tcolorbox df" id="tcolobox-37">   <a 
 id="x1-6016x1.4"></a> 
<div class="tcolorbox-title">
<!--l. 554--><p class="noindent" >Definition 1.30  (Principal) Argument</p></div> 
<div class="tcolorbox-content"><!--l. 552--><p class="noindent" >Let \(z\) be a complex number. The principal argument of \(z\), denoted as \(\Arg {z}\), is the angle \(\theta \) in radians
that \(z\) makes with the positive real axis in a counter-clockwise direction, chosen so that \(-\pi &lt;\theta \leq \pi \). </p> 
</div> 
</div>
<!--l. 556--><p class="noindent" >A point \(z\) in the Argand diagram determines an angle \(\theta \) with the positive real axis.
</p>
<div class="center" 
>
<!--l. 558--><p class="noindent" >
</p><!--l. 559--><p class="noindent" ><img 
src="F17ZD_main15x.svg" alt="RIz𝜃em∖∖lleeff t(tz(z∖r∖irgihgtht))
"  /></p></div>
<!--l. 569--><p class="noindent" >The choice of \(\theta \) such that \(-\pi &lt;\theta \le \pi \) means: </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 573--><p class="noindent" >\(0 &lt; \theta \leq \pi \) if \(z\) is <span 
class="rm-lmsso-12">above </span>the real axis
      </p></li>
      <li class="itemize">
      <!--l. 573--><p class="noindent" >\(-\pi &lt; \theta &lt; 0\) if \(z\) is <span 
class="rm-lmsso-12">below </span>the real axis </p></li></ul>
<!--l. 632--><p class="noindent" >
</p><!--l. 632--><p class="noindent" ><a 
 id="x1-6017r31"></a><!--l. 576--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.31</span>                                                                          </p><!--l. 576--><p class="noindent" >The most straight forward method of determining the argument is to draw the diagram.
</p>                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 632--><p class="noindent" ><div class="minipage"><div class="center" 
>
<!--l. 579--><p class="noindent" >
</p><!--l. 580--><p class="noindent" ><img 
src="F17ZD_main16x.svg" alt="RI5em ∖∖lleefftt((zz∖∖rriigghhtt))
"  /></p></div>
<div class="center" 
>
<!--l. 587--><p class="noindent" >
</p><!--l. 588--><p class="noindent" ><img 
src="F17ZD_main17x.svg" alt="RI2emi ∖∖lleefftt((zz∖∖rriigghhtt))
"  /></p></div>                                                                                        
                                                                                          
                                                                                          
</p><!--l. 632--><p class="noindent" ></div><div class="minipage"><div class="center" 
>
<!--l. 599--><p class="noindent" >
</p><!--l. 600--><p class="noindent" ><img 
src="F17ZD_main18x.svg" alt="RI3em ∖∖lleefftt((zz∖∖rriigghhtt))
"  /></p></div>
<div class="center" 
>
<!--l. 608--><p class="noindent" >
</p><!--l. 609--><p class="noindent" ><img 
src="F17ZD_main19x.svg" alt="RI−em4∖∖illeefftt((zz∖∖rriigghhtt))
"  /></p></div></div>
<!--l. 619--><p class="noindent" >Based on the diagrams, we have: \begin {align*}  \Arg {5}&amp;=0 &amp; \Arg {-3}&amp;=\pi \\ \Arg {2i}&amp;=\frac {\pi }{2} &amp; \Arg {-4i}&amp;=-\frac {\pi }{2}  \end {align*}
</p><!--l. 625--><p class="noindent" >A similar approach would show that \begin {align*}  \Arg {1+i} &amp;=\frac {\pi }{4}&amp; \Arg {-1+i}&amp;=\frac {3\pi }{4}\\ \Arg {1-i} &amp;=-\frac {\pi }{4}&amp; \Arg {-1-i}&amp;=-\frac {3\pi }{4}  \end {align*}
</p>
 
</div> 
</div>                                                                                        
</p>
<!--l. 649--><p class="noindent" >
</p><!--l. 649--><p class="noindent" ><a 
 id="x1-6018r32"></a><!--l. 637--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.32</span>                                                                          </p><!--l. 637--><p class="noindent" >We can use our knowledge of coordinate axes to determine: \begin {align*}  \Arg {1} &amp;=0&amp; \Arg {i}&amp;=\frac {\pi }{2}\\ \Arg {-1} &amp;=\pi &amp; \Arg {-i}&amp;=-\frac {\pi }{2}  \end {align*}
</p><!--l. 644--><p class="noindent" >Further, we can use the special triangles to get \begin {align*}  \Arg {\sqrt {3}+i} &amp;=\frac {\pi }{6}&amp; \Arg {1+i\sqrt {3}} &amp;=\frac {\pi }{3}  \end {align*}
</p>                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 649--><p class="noindent" > 
</div> 
</div>                                                                                        
</p><!--l. 651--><p class="noindent" >These are all cases we can obtain the angles by drawing and using our experience with simple triangles.
We can extend this and develop a more systematic method to use for more general complex
numbers
</p>
<!--l. 677--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-38">    
<div class="tcolorbox-title">
<!--l. 677--><p class="noindent" >Example 1.33</p></div> 
<div class="tcolorbox-content"></p><!--l. 654--><p class="noindent" >Suppose \(z=a+ib\) is a complex number which determines an angle \(\theta \) with the real axis.
</p>
<div class="center" 
>
<!--l. 656--><p class="noindent" >
</p><!--l. 657--><p class="noindent" ><img 
src="F17ZD_main20x.svg" alt="RIrbaz𝜃em=∖∖llaeeff+tt((izzb∖∖rriigghhtt))
"  /></p></div>
<!--l. 672--><p class="noindent" >We can use our knowledge of trigonometry to get that \begin {equation*}  \tan (\theta ) = \frac {b}{a}  \end {equation*}
In order words, we can determine the argument \(\theta \) from the Cartesian form. </p> 
</div> 
</div>                         
<!--l. 679--><p class="noindent" ><img 
src="F17ZD_main21x.svg" alt="  "  />In
the case where \(a=0\), we are on the imaginary axis and can resolve the angle using more simple
methods.
</p>
<div class="tcolorbox exercise" id="tcolobox-39">    
<div class="tcolorbox-title">
<!--l. 683--><p class="noindent" >Exercise 1.34 </p></div> 
<div class="tcolorbox-content"><!--l. 682--><p class="noindent" >Find \(\Arg {1+2i}\). </p> 
</div> 
</div>
<!--l. 709--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="tcolorbox solution" id="tcolobox-40">    
<div class="tcolorbox-title">
<!--l. 709--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 686--><p class="noindent" >We start by plotting a diagram
</p>
<div class="center" 
>
<!--l. 688--><p class="noindent" >
</p><!--l. 689--><p class="noindent" ><img 
src="F17ZD_main22x.svg" alt="RIr21z𝜃em=∖∖lel1fef+tt((2zzi∖∖rriigghhtt))
"  /></p></div>
<!--l. 703--><p class="noindent" >Using the above formula we know that \(\tan (\theta )=\frac {2}{1} = 2\). Using a calculator we get \begin {equation*}  \theta =\arctan (2) = 1.107  \end {equation*}
to 3 decimal places (in radians!).
</p>
 
</div> 
</div>                                                                                                               
<!--l. 710--><p class="noindent" ><img 
src="F17ZD_main23x.svg" alt="  "  />Even
though we have \(\tan (\theta ) = \frac {b}{a}\), we have cases where \(\theta \neq \arctan \left (\frac {b}{a}\right )\).
</p>
<!--l. 716--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-41">    
<div class="tcolorbox-title">
<!--l. 716--><p class="noindent" >Example 1.35</p></div> 
<div class="tcolorbox-content"></p><!--l. 714--><p class="noindent" >Suppose we want to find \(\Arg {-1-2i}\) using the same method. We will that \(\frac {b}{a} = \frac {-2}{-1} =2\), which will give \(\theta =\arctan (2) = 1.107\). However this
is clearly not correct based on our previous exercise. That is \(w=-1-2i\) and \(z=1+2i\) are not in the same direction.
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 718--><p class="noindent" >The reason for this is because the inverse tangent function produces values \(-\frac {\pi }{2} &lt; \theta &lt; \frac {\pi }{2}\). In other words, the it
assumes \(a &gt;0\). If we want to obtain the argument for the complex number in the exact opposite direction, we
have to account for it by adding or subtracting \(\pi \)
</p>
<!--l. 742--><p class="noindent" >
</p><!--l. 742--><p class="noindent" ><a 
 id="x1-6022r36"></a><!--l. 721--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.36</span>                                                                          </p><!--l. 721--><p class="noindent" >Consider the numbers  \(z = 1+i\) and  \(w = -1-i\). We know  \(\Arg {z} = \frac {\pi }{4}\) and  \(\Arg {w} = -\frac {3\pi }{4}\) and  \(\tan \left (\frac {\pi }{4}\right ) = \tan \left (\frac {-3\pi }{4}\right )=1\). If we plot a diagram, we see why this is the
case and how we can resolve it.
</p>                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 742--><p class="noindent" ><div class="center" 
>
<!--l. 723--><p class="noindent" >
</p><!--l. 724--><p class="noindent" ><img 
src="F17ZD_main24x.svg" alt="RIzwππ4−em43π∖∖lleefftt((zz∖∖rriigghhtt))
"  /></p></div>
<!--l. 741--><p class="noindent" >Be sure to have your calculator set to <span 
class="rm-lmssbx-10x-x-120">radians </span>mode </p> 
</div> 
</div>                                      
</p>
<div class="tcolorbox thm" id="tcolobox-42">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 806--><p class="noindent" >Summary 1.37 </p></div> 
<div class="tcolorbox-content"><!--l. 745--><p class="noindent" >To find \(\theta =\Arg {a+ib}\),
        </p><dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
    1.  </dt><dd 
class="enumerate-enumitem">Draw a picture showing the angle. Stop if the angle is from a special triangle or on
        the axes.
        </dd><dt class="enumerate-enumitem">
    2.  </dt><dd 
class="enumerate-enumitem">Determine which <span 
class="rm-lmsso-12">quadrant </span>the point is in and use the following:</dd></dl>
<div class="center" 
>
<!--l. 750--><p class="noindent" >
</p><!--l. 751--><p class="noindent" ><img 
src="F17ZD_main25x.svg" alt="                 bbbb
RI𝜃a𝜃a𝜃a𝜃aem=&#x003C;=&#x003E;=&#x003C;=&#x003E;∖∖lla0a0a0a0eer,r,r,r,ffcccctttbtbtbtb((aaaazzn&#x003E;n&#x003E;n&#x003C;n&#x003C;∖∖rr∖0∖0∖0∖0iillllggeeeehhfffftttttt))((((--∖∖∖∖rrrriiiigggghhhhtttt)))) + − ππ
                 aaaa
"  /></p></div>
<!--l. 765--><p class="noindent" >The four <span 
class="rm-lmsso-12">quadrants </span>of the complex plane are usually numbered anti-clockwise.
</p>
<div class="center" 
>
<!--l. 769--><p class="noindent" >
</p><!--l. 770--><p class="noindent" ><img 
src="F17ZD_main26x.svg" alt="RIIIIIemIIV((Izz))
"  /></p></div>
 
</div> 
</div>
                                                                                          
                                                                                          
<!--l. 808--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="tcolorbox exercise" id="tcolobox-43">   <a 
 id="x1-6027x1.4"></a> 
<div class="tcolorbox-title">
<!--l. 811--><p class="noindent" >Exercise 1.38 </p></div> 
<div class="tcolorbox-content"><!--l. 810--><p class="noindent" >Find \(\Arg {-1+2i}\). </p> 
</div> 
</div>
<!--l. 835--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-44">    
<div class="tcolorbox-title">
<!--l. 835--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 813--><p class="noindent" >Starting with the picture:
</p>
<div class="center" 
>
<!--l. 816--><p class="noindent" >
</p><!--l. 817--><p class="noindent" ><img 
src="F17ZD_main27x.svg" alt="RImrz𝜃e =∖∖llee−ff1t(t( +zz∖∖2rriiigghhtt ))
"  /></p></div>
<!--l. 826--><p class="noindent" >We see that \(0&lt;\Arg {-1+2i} &lt;\pi \). Using the above formula we know that \(\tan (\theta )=\frac {2}{-1}=-2\). Using a calculator we get
\begin {equation*}  \arctan (-2)=-1.107  \end {equation*}
to 3 decimal places. Since this is not in the correct range for \(\theta \). Adding \(\pi \) gives \begin {equation*}  \theta = -1.107+\pi = 2.034  \end {equation*}
</p>
 
</div> 
</div>                                                                                                               
<div class="tcolorbox exercise" id="tcolobox-45">    
<div class="tcolorbox-title">
<!--l. 841--><p class="noindent" >Exercise 1.39 </p></div> 
<div class="tcolorbox-content"><!--l. 840--><p class="noindent" >Find \(\Arg {3-4i}\). </p> 
</div> 
</div>
<!--l. 847--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-46">    
<div class="tcolorbox-title">
<!--l. 847--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 843--><p class="noindent" >We see that \(-\frac {\pi }{2}&lt;\Arg {3-4i}&lt;0\), so the inverse tangent value should give us the right argument. This gives:
\begin {equation*}  \Arg {3-4i}=\arctan \left (\frac {-4}{3}\right )= -0.927.  \end {equation*}
</p> 
</div> 
</div>                                                                                                               
                                                                                          
                                                                                          
<!--l. 849--><p class="noindent" >Now that we have we can determine the modulus and argument of a complex number, we can try to
reverse the process.
</p>
<div class="tcolorbox df" id="tcolobox-47">    
<div class="tcolorbox-title">
<!--l. 857--><p class="noindent" >Definition 1.40  Polar form</p></div> 
<div class="tcolorbox-content"><!--l. 852--><p class="noindent" >Let \(z\) be a complex number with modulus \(r\) and principal argument \(\theta \). The polar form of \(z\) is given by \begin {align*}  z &amp;= r\cos (\theta ) + i r\sin (\theta )\\ &amp;= r\left (\cos (\theta ) + i\sin (\theta )\right )  \end {align*}
</p>
 
</div> 
</div>
<!--l. 859--><p class="noindent" >We can verify this by looking at the picture and applying a bit of trigonometry. </p>
<div class="center" 
>
<!--l. 860--><p class="noindent" >
</p><!--l. 861--><p class="noindent" ><img 
src="F17ZD_main28x.svg" alt="RImrbaz𝜃e∖∖lleefft(t(zz∖∖rriigghhtt ))
"  /></p></div>
<!--l. 875--><p class="noindent" >From the picture, we see that the base \(a = r\cos (\theta )\) and height \(b = r\sin (\theta )\). Hence, we have \begin {equation*}  z =r\cos (\theta )+i r\sin (\theta )  \end {equation*}
</p>
<div class="tcolorbox exercise" id="tcolobox-48">    
<div class="tcolorbox-title">
<!--l. 882--><p class="noindent" >Exercise 1.41 </p></div> 
<div class="tcolorbox-content"><!--l. 881--><p class="noindent" >Suppose \(\abs {z}=2\) and \(\Arg {z}=\frac {\pi }{3}\). Write \(z\) in polar form. </p> 
</div> 
</div>
<!--l. 889--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-49">    
<div class="tcolorbox-title">
<!--l. 889--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 884--><p class="noindent" >Set \(r=\abs {z}=2\) and \(\theta =\Arg {z}=\frac {\pi }{3}\). Then \begin {align*}  z&amp;= r\left (\cos (\theta )+i\sin (\theta )\right )\\ &amp;= 2\left (\cos \left (\frac {\pi }{3}\right )+i\sin \left (\frac {\pi }{3}\right )\right )  \end {align*}
</p>
 
</div> 
</div>                                                                                                               
                                                                                          
                                                                                          
<!--l. 890--><p class="noindent" >If we wished to convert it to Cartesian form, we can further simplify \begin {align*}  z&amp;= 2\left (\cos \left (\frac {\pi }{3}\right )+i\sin \left (\frac {\pi }{3}\right )\right )\\ &amp;= 2\left (\frac {1}{2}+i\frac {\sqrt {3}}{2}\right )\\ &amp;= 1+i\sqrt {3}  \end {align*}
</p><!--l. 897--><p class="noindent" >In general, if we want to write a complex number in polar form, we:
      </p><dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
   1. </dt><dd 
class="enumerate-enumitem">Compute the modulus, \(r=\abs {z}\).
      </dd><dt class="enumerate-enumitem">
   2. </dt><dd 
class="enumerate-enumitem">Computer the principal argument, \(\theta =\Arg {z}\).
      </dd><dt class="enumerate-enumitem">
   3. </dt><dd 
class="enumerate-enumitem">Write \(z\) as \begin {equation*}  z= r\left (\cos (\theta )+i\sin (\theta )\right )\\  \end {equation*}
      </dd></dl>
<!--l. 907--><p class="noindent" >We will see in the next subsection that we can also write this as \(z = r\, e^{i\theta }\).
</p>
<div class="tcolorbox exercise" id="tcolobox-50">   <a 
 id="x1-6035x1.4"></a> 
<div class="tcolorbox-title">
<!--l. 911--><p class="noindent" >Exercise 1.42 </p></div> 
<div class="tcolorbox-content"><!--l. 910--><p class="noindent" >Write \(z= 1+i\) in polar form. </p> 
</div> 
</div>
<!--l. 922--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-51">    
<div class="tcolorbox-title">
<!--l. 922--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 913--><p class="noindent" >First we calculate the modulus and principal argument. </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 917--><p class="noindent" >Modulus: \(\abs {z}=\sqrt {1^2+1^2}=\sqrt {2}\).
        </p></li>
        <li class="itemize">
        <!--l. 917--><p class="noindent" >Principal Argument: \(\Arg {z}=\arctan \left (\frac {1}{1}\right ) = \frac {\pi }{4}\). Since \(z\) is in the first quadrant, the argument does not need
        altering. </p></li></ul>
<!--l. 918--><p class="noindent" >Thus the polar form of \(z= 1+i\) is \begin {equation*}  z = \sqrt {2} \left (\cos \left (\frac {\pi }{4}\right )+i\sin \left (\frac {\pi }{4}\right )\right )  \end {equation*}
</p> 
</div> 
</div>                                                                                                               
<div class="tcolorbox exercise" id="tcolobox-52">    
<div class="tcolorbox-title">
<!--l. 926--><p class="noindent" >Exercise 1.43 </p></div> 
<div class="tcolorbox-content"><!--l. 925--><p class="noindent" >Write \(z=-1+2i\) in polar form. </p> 
</div> 
</div>
<!--l. 937--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="tcolorbox solution" id="tcolobox-53">    
<div class="tcolorbox-title">
<!--l. 937--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 928--><p class="noindent" >First we calculate the modulus and principal argument. </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 932--><p class="noindent" >Modulus: \(\abs {z}=\sqrt {(-1)^2+2^2}=\sqrt {5}\).
        </p></li>
        <li class="itemize">
        <!--l. 932--><p class="noindent" >Principal Argument: \(\Arg {z}=\arctan \left (\frac {2}{-1}\right ) = -1.107\). Since \(z\) is in the upper-left quadrant, we need to alter the
        argument (see <a 
href="#x1-6027x1.4">subsection 1.4</a>) to get \(\Arg {z} = 2.034\) </p></li></ul>
<!--l. 933--><p class="noindent" >Thus the polar form of \(z= -1+2i\) is \begin {equation*}  z = \sqrt {5} \left (\cos \left (2.034\right )+i\sin \left (2.034\right )\right )  \end {equation*}
</p> 
</div> 
</div>                                                                                                               
                                                                                          
                                                                                          
<h4 class="subsectionHead"><span class="titlemark">1.5   </span> <a 
 id="x1-70001.5"></a>Exponential Form</h4>
<!--l. 943--><p class="noindent" >The introduction of complex numbers connects two key areas of mathematics: Trigonometry and the
exponential. It is possible to write a Maclaurin series for the exponential \begin {equation*}  e^x = 1+x+\frac {x^2}{2!}+\frac {x^3}{3!} + \cdots = \sum _{n=0}^{\infty } \frac {x^n}{n!}  \end {equation*}
</p><!--l. 948--><p class="noindent" >As it turns out, the exponential can be extended to complex numbers in the analogous way.
</p>
<div class="tcolorbox df" id="tcolobox-54">    
<div class="tcolorbox-title">
<!--l. 955--><p class="noindent" >Definition 1.44  Complex exponential</p></div> 
<div class="tcolorbox-content"><!--l. 951--><p class="noindent" >For a complex number \(z\), the complex exponential is defined to be: \begin {equation*}  e^z = 1+z+\frac {z^2}{2!}+\frac {z^3}{3!} + \cdots = \sum _{n=0}^{\infty } \frac {z^n}{n!}  \end {equation*}
</p> 
</div> 
</div>
<!--l. 957--><p class="noindent" >It is not at all obvious, but it is a fact that the infinite summation results in something finite for all
complex \(z\) so long as \(|z|&lt;\infty \).
</p><!--l. 959--><p class="noindent" >If we consider the complex number \(z=i\theta \) for an angle \(\theta \), we get: \begin {align*}  e^{i\theta } &amp;= 1+\left (i\theta \right )+\frac {\left (i\theta \right )^2}{2!}+\frac {\left (i\theta \right )^3}{3!} + \cdots = \sum _{n=0}^{\infty } \frac {\left (i\theta \right )^n}{n!}  \end {align*}
</p><!--l. 963--><p class="noindent" >By resolving the powers of \(i\), we get \begin {align*}  e^{i\theta } &amp;= 1+\left (i\theta \right )+\frac {\left (i\theta \right )^2}{2!}+\frac {\left (i\theta \right )^3}{3!} + \frac {\left (i\theta \right )^4}{4!} + \frac {\left (i\theta \right )^5}{5!} + \cdots \\ &amp;= 1+i\theta - \frac {\theta ^2}{2!} -\frac {i\theta ^3}{3!} + \frac {\theta ^4}{4!} + \frac {i\theta ^5}{5!} + \cdots \\ &amp;= \left (1- \frac {\theta ^2}{2!} + \frac {\theta ^4}{4!} + \cdots \right ) + i \left (\theta - \frac {\theta ^3}{3!} + \frac {\theta ^5}{5!} + \cdots \right )  \end {align*}
</p><!--l. 969--><p class="noindent" >Recalling the Maclaurin series for \(\sin (x)\) and \(\cos (x)\) , we see that the two summations give: \begin {align*}  e^{i\theta } &amp;= \cos (\theta ) + i \sin (\theta )  \end {align*}
</p><!--l. 974--><p class="noindent" >This gives a fundamental result in complex numbers
</p>
<div class="tcolorbox thm" id="tcolobox-55">    
<div class="tcolorbox-title">
<!--l. 981--><p class="noindent" >Theorem 1.45  Euler’s Formula</p></div> 
<div class="tcolorbox-content"><!--l. 977--><p class="noindent" >For any angle \(\theta \), we have \begin {equation*}  e^{i\theta } = \cos (\theta ) + i \sin (\theta )  \end {equation*}
</p> 
</div> 
</div>
<!--l. 983--><p class="noindent" >In particular, if \(\theta =\pi \), we have the famous equation<span class="footnote-mark"><a 
href="F17ZD_main5.html#fn4x0"><sup class="textsuperscript">4</sup></a></span><a 
 id="x1-7003f4"></a> 
\begin {equation*}  e^{i\pi } +1 = 0  \end {equation*}
</p>
<!--l. 994--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="tcolorbox example" id="tcolobox-56">    
<div class="tcolorbox-title">
<!--l. 994--><p class="noindent" >Example 1.46</p></div> 
<div class="tcolorbox-content"></p><!--l. 989--><p class="noindent" >With Euler’s formula, we have \begin {align*}  e^{i1} &amp;= \cos (1)+i\sin (1).\\ e^{i\frac {\pi }{2}} &amp;= \cos \left (\frac {\pi }{2}\right )+i\sin \left (\frac {\pi }{2}\right ) = 0 + 1i = i.  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 996--><p class="noindent" >Using Euler’s formula we can get a new and very useful representation for complex numbers in polar
form.
</p>
<div class="tcolorbox df" id="tcolobox-57">    
<div class="tcolorbox-title">
<!--l. 1003--><p class="noindent" >Definition 1.47  Exponential form</p></div> 
<div class="tcolorbox-content"><!--l. 999--><p class="noindent" >The exponential form of the complex number \(z\) with modulus \(r\) and argument \(\theta \) is written as
\begin {equation*}  z = re^{i\theta }  \end {equation*}
</p> 
</div> 
</div>
<!--l. 1005--><p class="noindent" >We derive this form by applying Euler’s Formula to the polar form \begin {equation*}  z= r\left (\cos (\theta )+i\sin (\theta )\right ) = re^{i\theta }  \end {equation*}
</p><!--l. 1010--><p class="noindent" >To write a complex number \(z\) in exponential form:
      </p><dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
   1. </dt><dd 
class="enumerate-enumitem">Calculate the modulus \(r = \abs {z}\),
      </dd><dt class="enumerate-enumitem">
   2. </dt><dd 
class="enumerate-enumitem">Calculate the principle argument \(\theta = \Arg {z}\),
      </dd><dt class="enumerate-enumitem">
   3. </dt><dd 
class="enumerate-enumitem">Write \(z=re^{i\theta }\).</dd></dl>
<div class="tcolorbox exercise" id="tcolobox-58">    
<div class="tcolorbox-title">
<!--l. 1020--><p class="noindent" >Exercise 1.48 </p></div> 
<div class="tcolorbox-content"><!--l. 1019--><p class="noindent" >Write \(z = 1+i\) in exponential form. </p> 
</div> 
</div>
<!--l. 1027--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-59">    
<div class="tcolorbox-title">
<!--l. 1027--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 1022--><p class="noindent" >Building on <a 
href="#x1-6035x1.4">subsection 1.4</a>, we know \(\abs {z}= \sqrt {2}\) and \(\Arg {z}=\frac {\pi }{4}\). Hence, the exponential form is \begin {equation*}  z = 1+i= \sqrt 2 e^{i\frac {\pi }{4}}.  \end {equation*}
</p> 
</div> 
</div>                                                                                                               
<div class="tcolorbox exercise" id="tcolobox-60">    
<div class="tcolorbox-title">
<!--l. 1031--><p class="noindent" >Exercise 1.49 </p></div> 
<div class="tcolorbox-content"></p><!--l. 1030--><p class="noindent" >  <!--l. 1030--><p class="noindent" >?  Write \(-i\) and \(-1\) in exponential form. </p> 
</div> 
</div>
                                                                                          
                                                                                          
<!--l. 1033--><p class="noindent" >Now that we have shown how to convert a complex number from Cartesian form \(a+ib\) to exponential form \(r e^{i\theta }\),
we can go in the reverse direction.
</p>
<div class="tcolorbox exercise" id="tcolobox-61">    
<div class="tcolorbox-title">
<!--l. 1037--><p class="noindent" >Exercise 1.50 </p></div> 
<div class="tcolorbox-content"><!--l. 1036--><p class="noindent" >Write the complex number \(z=2e^{i\frac {\pi }{4}}\) in Cartesian form \(a+ib\). </p> 
</div> 
</div>
<!--l. 1047--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-62">    
<div class="tcolorbox-title">
<!--l. 1047--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 1040--><p class="noindent" >Using Euler’s formula, we have \begin {align*}  z &amp;= 2e^{i\frac {\pi }{4}}\\ &amp;=2\left (\cos \left (\frac {\pi }{4}\right )+i\sin \left (\frac {\pi }{4}\right )\right ) \\ &amp;=2\left (\frac {1}{\sqrt {2}}+i\frac {1}{\sqrt {2}}\right )\\ &amp;=\sqrt {2}+i \sqrt {2}  \end {align*}
</p>
 
</div> 
</div>                                                                                                               
<div class="tcolorbox exercise" id="tcolobox-63">    
<div class="tcolorbox-title">
<!--l. 1051--><p class="noindent" >Exercise 1.51 </p></div> 
<div class="tcolorbox-content"><!--l. 1050--><p class="noindent" >Let \(z=5+4i\). Write \(e^z\) in the form \(a+ib\). </p> 
</div> 
</div>
<!--l. 1061--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-64">    
<div class="tcolorbox-title">
<!--l. 1061--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 1054--><p class="noindent" >We have (to 2 decimal point) \begin {align*}  e^{5+4i} &amp;= e^5e^{4i}\\ &amp;= e^5\left (\cos (4)+i\sin (4)\right )\\ &amp;=e^5\cos (4)+ie^5\sin (4)\\ &amp;\approx -97.01-112.32i  \end {align*}
</p>
 
</div> 
</div>                                                                                                               
<!--l. 1063--><p class="noindent" >Recall that multiplication and division of complex numbers can be quite an involved process (See
<a 
href="#x1-3015x1.1">subsection 1.1</a>). However, we can utilise properties of the exponential function (and hence the
exponential form) help simplify this process.
</p><!--l. 1065--><p class="noindent" >Let \(z\) and \(w\) be complex numbers. Then we have: \begin {align*}  e^z\cdot e^w&amp;=e^{z+w} &amp; \frac {e^z}{e^w} &amp;=e^{z-w}  \end {align*}
</p><!--l. 1070--><p class="noindent" >We can express this as:
</p>
<div class="tcolorbox thm" id="tcolobox-65">   <a 
 id="x1-7014x1.5"></a> 
<div class="tcolorbox-title">
<!--l. 1082--><p class="noindent" >Theorem 1.52  Multiplication and division in exponential
                                                                                          
                                                                                          
form</p></div> 
<div class="tcolorbox-content"><!--l. 1073--><p class="noindent" >Let \(z = re^{i\theta }\) and \(w = se^{i\phi }\) be complex numbers in exponential form \begin {align*}  z\cdot w &amp;= (r\cdot s) e^{i(\theta +\phi )} &amp; \frac {z}{w} &amp;= \frac {r}{s} e^{i(\theta -\phi )}.  \end {align*}
</p><!--l. 1077--><p class="noindent" >Converting the right-hand side to <span 
class="rm-lmsso-12">polar form </span>gives: \begin {align*}  z\cdot w &amp;= (r\cdot s) \left (\cos (\theta +\phi ) + i \sin (\theta +\phi )\right ) \\ \frac {z}{w} &amp;= \frac {r}{s} \left (\cos (\theta -\phi ) + i \sin (\theta -\phi )\right ).  \end {align*}
</p>
 
</div> 
</div>
<div class="tcolorbox exercise" id="tcolobox-66">    
<div class="tcolorbox-title">
<!--l. 1086--><p class="noindent" >Exercise 1.53 </p></div> 
<div class="tcolorbox-content"><!--l. 1085--><p class="noindent" >Let \(z=2e^{\frac {i \pi }{4}}\), and \(w=e^{-\frac {i \pi }{4}}\). Find \(z\cdot w\) and \(\frac {z}{w}\). </p> 
</div> 
</div>
<!--l. 1095--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-67">    
<div class="tcolorbox-title">
<!--l. 1095--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 1088--><p class="noindent" >Reading off the complex numbers, we get \(r=2, s=1, \theta =\frac {\pi }{4}, \phi =-\frac {\pi }{4}\). So we get: \begin {align*}  z\cdot w &amp;= (2\cdot 1) \left [\cos \left (\frac {\pi }{4}+\left (-\frac {\pi }{4}\right )\right ) + i \sin \left (\frac {\pi }{4}+\left (-\frac {\pi }{4}\right )\right )\right ]\\ &amp;=2\left [\cos (0)+i \sin (0)\right ]=2\\ \frac {z}{w} &amp;= \frac {2}{1} \left [\cos \left (\frac {\pi }{4}-\left (-\frac {\pi }{4}\right )\right ) + i \sin \left (\frac {\pi }{4}-\left (-\frac {\pi }{4}\right )\right )\right ]\\ &amp;= \frac {2}{1} \left [\cos (\pi /2) + i \sin (\pi /2)\right ] = 2i.  \end {align*}
</p>
 
</div> 
</div>                                                                                                               
<!--l. 1097--><p class="noindent" >Complex exponentials are extremely useful when dealing with multiplication and
division<span class="footnote-mark"><a 
href="F17ZD_main6.html#fn5x0"><sup class="textsuperscript">5</sup></a></span><a 
 id="x1-7016f5"></a> .
However, its usefulness extends beyond simple arithmetic. For the rest of this section, we will take a
(very brief) look into areas where complex exponentials might be useful.
</p>
<h5 class="likesubsubsectionHead"><a 
 id="x1-80001.5"></a>Complex exponentials and trigonometric functions</h5>
<!--l. 1101--><p class="noindent" >Euler’s formula gives us an alternative way to write the sine and cosine functions which is used very
widely in science and engineering. In particular, we have \begin {align*}  e^{i\theta } + e^{-i\theta } &amp;= \left (\cos (\theta ) + i \sin (\theta )\right ) + \left (\cos (\theta ) - i \sin (\theta )\right ) = 2 \cos (\theta )\\ e^{i\theta } - e^{-i\theta } &amp;= \cos (\theta ) + i \sin (\theta ) - \left (\cos (\theta ) - i \sin (\theta )\right ) = 2 i \sin (\theta )  \end {align*}
</p><!--l. 1107--><p class="noindent" >We can rearrange to get the following: </p>
<div class="tcolorbox thm" id="tcolobox-68">    
<div class="tcolorbox-title">
<!--l. 1114--><p class="noindent" >Theorem 1.54  Trigonometric functions in exponential
                                                                                          
                                                                                          
forms</p></div> 
<div class="tcolorbox-content"><!--l. 1109--><p class="noindent" >We can express the trigonometric functions: \begin {align*}  \cos (\theta ) &amp;= \frac {1}{2}\left (e^{i\theta } + e^{-i\theta }\right ),\\ \sin (\theta ) &amp;= \frac {1}{2i}\left (e^{i\theta } - e^{-i\theta }\right ).  \end {align*}
</p>
 
</div> 
</div>
<!--l. 1116--><p class="noindent" >Notice that these relations are very similar to the ones for the hyperbolic trigonometric functions (cosh
and sinh). The introduction of complex numbers illustrates why they share so many similar
identities.
</p>
<div class="tcolorbox exercise" id="tcolobox-69">    
<div class="tcolorbox-title">
<!--l. 1120--><p class="noindent" >Exercise 1.55 </p></div> 
<div class="tcolorbox-content"></p><!--l. 1119--><p class="noindent" >  <!--l. 1119--><p class="noindent" >?  Use complex exponentials to show that \(2\sin (\theta )\cos (\theta )=\sin (2\theta )\). </p> 
</div> 
</div>
<!--l. 1122--><p class="noindent" >
</p>
<h5 class="likesubsubsectionHead"><a 
 id="x1-90001.5"></a>Calculus with complex exponentials</h5>
<!--l. 1124--><p class="noindent" >The complex exponential is also commonly involved in the solution of differential equations,
particularly in cases where solutions oscillate (like in AC electronics, quantum mechanics,
etc.). It satisfies the same rules of differentiation and integration as any other exponential
function<span class="footnote-mark"><a 
href="F17ZD_main7.html#fn6x0"><sup class="textsuperscript">6</sup></a></span><a 
 id="x1-9001f6"></a> .
</p>
<!--l. 1136--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-70">    
<div class="tcolorbox-title">
<!--l. 1136--><p class="noindent" >Example 1.56</p></div> 
<div class="tcolorbox-content"></p><!--l. 1127--><p class="noindent" >For derivatives involving \(i\), we have \begin {align*}  \frac {d}{dx} e^{i \omega x} &amp;= i \omega e^{i \omega x} \\ \frac {d^2}{dx^2} e^{i \omega x} &amp;= (i \omega )^2 e^{i \omega x} = -\omega ^2 e^{i \omega x}  \end {align*}
</p><!--l. 1132--><p class="noindent" >Integration works in a similar way \begin {align*}  \int e^{i \omega x} dx &amp;= \frac {e^{i \omega x}}{i\omega } + C  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<div class="tcolorbox exercise" id="tcolobox-71">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 1140--><p class="noindent" >Exercise 1.57 </p></div> 
<div class="tcolorbox-content"></p><!--l. 1139--><p class="noindent" >  <!--l. 1139--><p class="noindent" >?  Verify that \(y(t) = C e^{-i4t}\) is a solution of \(y'' = -16 y\). </p> 
</div> 
</div>
<h5 class="likesubsubsectionHead"><a 
 id="x1-100001.5"></a>Logarithms of complex numbers</h5>
<!--l. 1144--><p class="noindent" >Now that we have seen the exponential in complex numbers, it is natural to question how logarithm
behave in complex numbers? The short answer is that logarithms are very complicated beasts, with
properties far beyond this introductory course. <img 
src="F17ZD_main29x.svg" alt=" "  />Use with caution!
</p><!--l. 1146--><p class="noindent" >Suppose we “naively” apply logarithms on complex numbers using the rules of real numbers, we have the
following.
</p>
<!--l. 1156--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-72">    
<div class="tcolorbox-title">
<!--l. 1156--><p class="noindent" >Example 1.58</p></div> 
<div class="tcolorbox-content"></p><!--l. 1149--><p class="noindent" >Let \(z = r e^{i\theta }\) be a complex number in exponential form. We can apply the logarithm as \begin {align*}  \Log (z) &amp;= \Log \left (r e^{i\theta }\right )\\ &amp;= \Log \left (r\right ) + \Log \left (e^{i\theta }\right )\\ &amp;= \Log \left (r\right ) + i\theta \\ &amp;= \Log \abs {z} + i\Arg {z}  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<div class="tcolorbox exercise" id="tcolobox-73">    
<div class="tcolorbox-title">
<!--l. 1160--><p class="noindent" >Exercise 1.59 </p></div> 
<div class="tcolorbox-content"><!--l. 1159--><p class="noindent" >Let \(z = 42 e^{i\frac {\pi }{7}}\). Find \(\Log (z)\). </p> 
</div> 
</div>
<!--l. 1163--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-74">    
<div class="tcolorbox-title">
<!--l. 1163--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 1162--><p class="noindent" >Since \(|z| = 42\) and \(\Arg {z} = \frac {\pi }{7}\), we have \(\Log (z) = \Log (42) + i\frac {\pi }{7}\). </p> 
</div> 
</div>                                                                                  
<!--l. 1165--><p class="noindent" ><img 
src="F17ZD_main30x.svg" alt="
"  />Whilst this approach is not wrong, it also does not capture the entire
                                                                                          
                                                                                          
picture<span class="footnote-mark"><a 
href="F17ZD_main8.html#fn7x0"><sup class="textsuperscript">7</sup></a></span><a 
 id="x1-10003f7"></a> .
                                                                                          
                                                                                          
</p>
<h4 class="subsectionHead"><span class="titlemark">1.6   </span> <a 
 id="x1-110001.6"></a>De Moivre’s Theorem</h4>
<!--l. 1171--><p class="noindent" >Through repeated application of <a 
href="#x1-7014x1.5">subsection 1.5</a>, we can extend exponential multiplication to powers of
exponentials. We start with \begin {equation*}  e^{i\theta } \cdot e^{i\theta } = e^{i (\theta +\theta )} = e^{i 2\theta }  \end {equation*}
which can be extended to any integer \(n\) giving \begin {equation*}  \left (e^{i\theta }\right )^n = e^{i n\theta }  \end {equation*}
</p><!--l. 1180--><p class="noindent" >If we convert both sides of the above equation into polar form, we obtain a very important
result.
</p>
<div class="tcolorbox thm" id="tcolobox-75">    
<div class="tcolorbox-title">
<!--l. 1192--><p class="noindent" >Theorem 1.60  De Moivre’s Theorem</p></div> 
<div class="tcolorbox-content"><!--l. 1183--><p class="noindent" >Let \(n\) be an integer. For an angle \(\theta \), we have \begin {equation*}  \left (\cos (\theta )+i\sin (\theta )\right )^n = \cos (n\theta ) +i\sin (n\theta ).  \end {equation*}
For a complex number \(z = r e^{i\theta }\) in exponential form, the following are equivalent \begin {align*}  z^n &amp;= r^n e^{i n\theta } = r^n \left (\cos (n\theta ) + i \sin (n\theta )\right )\\ &amp;= r^n \left (e^{i\theta }\right )^n = r^n \left (\cos (\theta )+i\sin (\theta )\right )^n  \end {align*}
</p>
 
</div> 
</div>
<!--l. 1194--><p class="noindent" >De Moivre’s Theorem is particularly useful when dealing with powers of complex numbers.
</p>
<div class="tcolorbox exercise" id="tcolobox-76">    
<div class="tcolorbox-title">
<!--l. 1198--><p class="noindent" >Exercise 1.61 </p></div> 
<div class="tcolorbox-content"><!--l. 1197--><p class="noindent" >Let \(z=(1+i)\) and compute \(z^8\) </p> 
</div> 
</div>
<!--l. 1212--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-77">    
<div class="tcolorbox-title">
<!--l. 1212--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 1201--><p class="noindent" >We first write \(z\) in expontial form as \begin {equation*}  z = 1+i=\sqrt {2} \left (\cos \left (\frac {\pi }{4}\right )+i\sin \left (\frac {\pi }{4}\right )\right ) = \sqrt {2}e^{i\frac {\pi }{4}}  \end {equation*}
This gives \begin {align*}  z^8 &amp;= (1+i)^8\\ &amp; = \left (\sqrt {2}e^{i\frac {\pi }{4}}\right )^8\\ &amp; = \sqrt {2}^8 e^{i\frac {8\pi }{4}}\\ &amp;= 16 e^{i2\pi }=16  \end {align*}
</p>
 
</div> 
</div>                                                                                                               
<!--l. 1214--><p class="noindent" >We can reach the same solution by multiplying 8 times. But this would be rather tedious and not
recommended.
</p>
<div class="tcolorbox exercise" id="tcolobox-78">    
<div class="tcolorbox-title">
<!--l. 1218--><p class="noindent" >Exercise 1.62 </p></div> 
<div class="tcolorbox-content"><!--l. 1217--><p class="noindent" >Compute \((-1+2i)^{10}\) </p> 
</div> 
</div>
                                                                                          
                                                                                          
<!--l. 1228--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-79">    
<div class="tcolorbox-title">
<!--l. 1228--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 1220--><p class="noindent" >Putting \(z= (-1+2i)\) in exponential form, we find \begin {equation*}  z= (-1+2i)=\sqrt {5}\big (\cos (2.034)+i \sin (2.034)\big ) = \sqrt {5}e^{2.034i}  \end {equation*}
Thus we have \begin {equation*}  z^{10} = (-1+2i)^{10} = \left (\sqrt {5}e^{2.034i}\right )^{10} = 3125 e^{20.34i} = 250.8+ 3115i  \end {equation*}
</p> 
</div> 
</div>                                                                                                               
<!--l. 1230--><p class="noindent" >De Moivre’s theorem is also useful for computing roots of complex numbers.
</p>
<div class="tcolorbox thm" id="tcolobox-80">   <a 
 id="x1-11005x1.6"></a> 
<div class="tcolorbox-title">
<!--l. 1238--><p class="noindent" >Theorem 1.63  Roots of complex numbers</p></div> 
<div class="tcolorbox-content"><!--l. 1233--><p class="noindent" >Let \(z = r\, e^{i\theta }\) be a complex number in exponential form and \(n\) be a positive integer. There are precisely \(n\)
different \(n\)-th roots of \(z\) and they are given by \begin {align*}  z^{\frac {1}{n}} &amp;= r^{\frac {1}{n}} e^{i \frac {\theta + 2\pi k}{n}} &amp;\text {for\;} k=0,1,\ldots ,n-1  \end {align*}
</p><!--l. 1237--><p class="noindent" >where \(r^{\frac {1}{n}}\) is the positive \(n\)-th root of \(r\). </p> 
</div> 
</div>
<!--l. 1255--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-81">    
<div class="tcolorbox-title">
<!--l. 1255--><p class="noindent" >Example 1.64</p></div> 
<div class="tcolorbox-content"></p><!--l. 1241--><p class="noindent" >For <span 
class="rm-lmsso-12">square roots </span>of \(z = re^{i\theta }\) we have \(n=2\). So the <span 
class="rm-lmsso-12">two roots</span> \(w_1, w_2\) are given by \begin {align*}  w_1 &amp;= \sqrt {r} e^{i\frac {\theta + 2\pi 0}{2}} = \sqrt {r} e^{i\frac {\theta }{2}}\\ w_2 &amp;= \sqrt {r} e^{i\frac {\theta + 2\pi 1}{2}} = \sqrt {r} e^{i\frac {\theta + 2\pi }{2}}  \end {align*}
</p><!--l. 1246--><p class="noindent" >and \(\sqrt {r}\) is the positive square root of \(r\).
</p><!--l. 1248--><p class="noindent" >For <span 
class="rm-lmsso-12">cube roots </span>of \(z = re^{i\theta }\) we have \(n=3\). So the <span 
class="rm-lmsso-12">three roots</span> \(w_1, w_2, w_3\) are given by \begin {align*}  w_1 &amp;= \sqrt [3]{r} e^{i\frac {\theta + 2\pi 0}{3}} = \sqrt [3]{r} e^{i\frac {\theta }{3}}\\ w_2 &amp;= \sqrt [3]{r} e^{i\frac {\theta + 2\pi 1}{3}} = \sqrt [3]{r} e^{i\frac {\theta + 2\pi }{3}} w_3 &amp;= \sqrt [3]{r} e^{i\frac {\theta + 2\pi 2}{3}} = \sqrt [3]{r} e^{i\frac {\theta + 4\pi }{3}}  \end {align*}
</p><!--l. 1254--><p class="noindent" >and \(\sqrt [3]{r}\) is the positive cubed root of \(r\). </p> 
</div> 
</div>                                                                         
<div class="tcolorbox exercise" id="tcolobox-82">    
<div class="tcolorbox-title">
<!--l. 1260--><p class="noindent" >Exercise 1.65 </p></div> 
<div class="tcolorbox-content"><!--l. 1259--><p class="noindent" >Let \(z = 9e^{i\frac {\pi }{3}}\). Find the two values of \(\sqrt {z}\) and verify your answer. </p> 
</div> 
</div>
<!--l. 1281--><p class="noindent" >
</p><!--l. 1281--><p class="noindent" ><!--l. 1261--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Solution:</span>                                                                          </p><!--l. 1262--><p class="noindent" >Apply <a 
href="#x1-11005x1.6">subsection 1.6</a> with  \(r=9\),  \(\theta = \frac {\pi }{3}\) and  \(n=2\) gives \begin {align*}  w_1 &amp;= \sqrt {9} e^{i\frac {\frac {\pi }{3} + 2\pi 0}{2}} \\ &amp;= 3 e^{i\frac {\pi }{6}} \\ w_2 &amp;= \sqrt {9} e^{i\frac {\frac {\pi }{3} + 2\pi }{2}} \\ &amp;= 3 e^{i\frac {7\pi }{6}}  \end {align*}
</p><!--l. 1269--><p class="noindent" >We can verify the solutions by squaring the answer \begin {align*}  (w_1)^2 &amp;= \left (3 e^{i\frac {\pi }{6}}\right )^2 \\ &amp;= 9 e^{2i\frac {\pi }{6}} \\ &amp;= 9 e^{i\frac {\pi }{3}}\\ &amp;= z \\ (w_2)^2 &amp;= \left (3 e^{i\frac {7\pi }{6}}\right )^2 \\ &amp;= 9 e^{2i\frac {7\pi }{6}} \\ &amp;= 9 e^{i\frac {14\pi }{6}} \\ &amp;= 9 e^{i\frac {2\pi }{6}}e^{i2\pi } \\ &amp;= z \cdot 1 = z  \end {align*}
</p>                                                                                   
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 1281--><p class="noindent" > 
</div> 
</div>                                                                                   
</p>
<div class="tcolorbox exercise" id="tcolobox-83">    
<div class="tcolorbox-title">
<!--l. 1286--><p class="noindent" >Exercise 1.66 </p></div> 
<div class="tcolorbox-content"></p><!--l. 1285--><p class="noindent" >  <!--l. 1285--><p class="noindent" >?  Find the \(5\)-th roots of \(z = -1 + 2i\). Confirm your answers by plotting it on the complex plane. </p> 
</div> 
</div>
<!--l. 1288--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<!--l. 1375--><p class="noindent" >
</p><!--l. 1375--><p class="noindent" ><a 
 id="x1-11009r67"></a><!--l. 1291--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.67</span>                                                                          </p><!--l. 1291--><p class="noindent" >Plotting the  \(n\)-th roots of a complex number  \(z\) gives some interesting patterns.
</p>
<div class="columns-2">
<!--l. 1295--><p class="noindent" ><img 
src="F17ZD_main31x.svg" alt="RI----1234---123em4321321 ∖∖lleefftt((zz∖∖rriigghhtt))  "  />
<a 
 id="x1-11010r1"></a>
<a 
 id="x1-11011"></a>
</p>
<figcaption class="caption" ><span class="id">Figure 1.1: </span><span  
class="content">Square roots of \(z= 9 e^{i\frac {\pi }{3}}\)
</span></figcaption><!--tex4ht:label?: x1-11010r1 -->
<div class="center" 
>
<!--l. 1314--><p class="noindent" >
</p><!--l. 1315--><p class="noindent" ><img 
src="F17ZD_main32x.svg" alt="RI----1234---123em4321321 ∖∖lleefftt((zz∖∖rriigghhtt))  "  />
<a 
 id="x1-11012r2"></a>
<a 
 id="x1-11013"></a>
</p>
<figcaption class="caption" ><span class="id">Figure 1.2: </span><span  
class="content">Third roots of \(z= \frac {8(-1+i)}{\sqrt {2}}\)
</span></figcaption><!--tex4ht:label?: x1-11012r1 -->
</div>                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 1375--><p class="noindent" ><div class="center" 
>
<!--l. 1334--><p class="noindent" >
</p><!--l. 1335--><p class="noindent" ><img 
src="F17ZD_main33x.svg" alt="RI----1234---123em4321321 ∖∖lleefftt((zz∖∖rriigghhtt))  "  />
<a 
 id="x1-11014r3"></a>
<a 
 id="x1-11015"></a>
</p>
<figcaption class="caption" ><span class="id">Figure 1.3: </span><span  
class="content">\(4\)-th roots of \(z= -1+2i\)
</span></figcaption><!--tex4ht:label?: x1-11014r1 -->
</div>
<div class="center" 
>
<!--l. 1354--><p class="noindent" >
</p><!--l. 1355--><p class="noindent" ><img 
src="F17ZD_main34x.svg" alt="RI----1234---123em4321321 ∖∖lleefftt((zz∖∖rriigghhtt))  "  />
<a 
 id="x1-11016r4"></a>
<a 
 id="x1-11017"></a>
</p>
<figcaption class="caption" ><span class="id">Figure 1.4: </span><span  
class="content">\(5\)-th roots of \(z= -1+2i\)
</span></figcaption><!--tex4ht:label?: x1-11016r1 -->
</div>
</div>
The \(n\)-th roots of \(z = r e^{i\theta }\) are spaced equally in wedges with angle \(\frac {2\pi }{n}\) around a circle of radius \(r^{\frac {1}{n}}\).  
</div> 
</div>        
                                                                                          
                                                                                          
</p><!--l. 1378--><p class="noindent" >As a final application of De Moivre’s theorem, we can use it to derive some useful trigonometric
identities.
</p>
<!--l. 1392--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-84">    
<div class="tcolorbox-title">
<!--l. 1392--><p class="noindent" >Example 1.68</p></div> 
<div class="tcolorbox-content"></p><!--l. 1382--><p class="noindent" >Consider the case of \(n=2\) in De Moivre’s Theorem. We get \begin {align*}  \cos (2\theta )+i\sin (2\theta ) &amp;= (\cos (\theta )+i\sin (\theta ))^2 \\ &amp;= \cos ^2(\theta )-\sin ^2(\theta ) + i 2\sin (\theta )\cos (\theta ) &amp; \text {Expanding}  \end {align*}
</p><!--l. 1387--><p class="noindent" >In particular, if we equate the real and imaginary parts of the equation, we recover a couple of
well-known identities \begin {align*}  \cos (2\theta ) &amp;= \cos ^2(\theta )-\sin ^2(\theta ) \\ \sin (2\theta ) &amp;= 2\sin (\theta )\cos (\theta )  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 1394--><p class="noindent" >The theorem gives us two trigonometric identities at the same time.
</p>
<div class="tcolorbox exercise" id="tcolobox-85">    
<div class="tcolorbox-title">
<!--l. 1398--><p class="noindent" >Exercise 1.69 </p></div> 
<div class="tcolorbox-content"></p><!--l. 1397--><p class="noindent" >  <!--l. 1397--><p class="noindent" >?  Prove the identity \(\sin (3\theta )=3\cos ^2(\theta )\sin (\theta )-\sin ^3(\theta )\) using De Moivre’s Theorem. </p> 
</div> 
</div>
                                                                                          
                                                                                          
<h4 class="subsectionHead"><span class="titlemark">1.7   </span> <a 
 id="x1-120001.7"></a>Complex Arguments in Trigonometric/Hyperbolic Functions</h4>
<!--l. 1403--><p class="noindent" >We have seen that for any real angle \(\theta \), <span 
class="rm-lmsso-12">Euler’s formula </span>gives \begin {equation*}  e^{i\theta } = \cos (\theta ) + i \sin (\theta )  \end {equation*}
which can be used to write \( \sin ( \theta ), \cos ( \theta ) \) as \begin {align*}  \cos (\theta ) &amp;= \frac {1}{2}\left (e^{i\theta } + e^{-i\theta }\right ),\\ \sin (\theta ) &amp;= \frac {1}{2i}\left (e^{i\theta } - e^{-i\theta }\right ).  \end {align*}
</p><!--l. 1413--><p class="noindent" >This can, in turn be used to write \begin {align*}  \cos (z) &amp;= \frac {1}{2}\left (e^{i z} + e^{-i z}\right ),\\ \sin (z) &amp;= \frac {1}{2i}\left (e^{i z} - e^{-iz}\right ).  \end {align*}
</p><!--l. 1418--><p class="noindent" >for <span 
class="rm-lmsso-12">complex </span>arguments \(z\).
</p><!--l. 1420--><p class="noindent" >If we expand the RHS using \(z=x+iy\), we find \[ \cos (z) = \cos (x) \cosh ( y) - i \sin (x) \sinh (y) \] and \[ \sin (z) = \sin (x) \cosh (y) + i \cos (x) \sinh (y) \, . \]
</p><!--l. 1430--><p class="noindent" >This is also true for \(\cosh (z), \sinh (z)\) with complex arguments \(z\), i.e. \begin {align*}  \cosh (z) &amp;= \frac {1}{2}\left (e^{z} + e^{- z}\right ),\\ \sinh (z) &amp;= \frac {1}{2}\left (e^{ z} - e^{-z}\right ).  \end {align*}
</p><!--l. 1435--><p class="noindent" >with <span 
class="rm-lmsso-12">complex </span>arguments \(z=x+iy\).
</p>
<!--l. 1449--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-86">    
<div class="tcolorbox-title">
<!--l. 1449--><p class="noindent" >Example 1.70</p></div> 
<div class="tcolorbox-content"></p><!--l. 1438--><p class="noindent" >Expand \( \cosh (z) \) for \(z=x+iy \).
</p><!--l. 1440--><p class="noindent" >Since \begin {align*}  \cosh (z) &amp;= \frac {1}{2}\left (e^{z} + e^{- z}\right ) = \frac {1}{2}\left (e^{ x+iy} + e^{-x+iy}\right )\\ &amp;= \frac {1}{2}\left (e^x (\cos (y) + i \sin (y)) + e^{-x} (\cos (y) - i \sin (y)) \right )\\ &amp;= \frac {1}{2}\left ((e^x + e^{-x} )\cos (y) + i (e^x - e^{-x} ) \sin (y) \right )\\ &amp;= \cosh (x) \cos (y) + i \sinh (x)\sin (y)  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 1452--><p class="noindent" >The exponential function can be defined for complex arguments \[ \exp (z) = \exp (x+iy) = \exp (x)\exp (iy) = \exp (x) \left ( \cos (y) + i \sin (y)\right ) \] which shows that \[ \text {Re} \exp (z) = \exp (x) \cos (y), \qquad \text {Im} \exp (z) = \exp (x) \sin (y) \, . \] The Logarithm can
also be defined for complex arguments \[ \Log (z) = \Log \abs {z} + i\Arg {z} \] which can be written as \[ \Log (z) = \frac 12\Log (x^2 + y^2) + i \arctan \left ( \frac {y}{x} \right ) \, . \]
</p><!--l. 1469--><p class="noindent" >You might worry that \(\Arg {}\), (i.e. \(\arctan \)), might lead to complications in the definition of \(\Log \) for complex arguments -
it does! Disentangling these is for another course.
                                                                                          
                                                                                          
</p><!--l. 1472--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">1.8   </span> <a 
 id="x1-130001.8"></a>Complex Mappings</h4>
<!--l. 1474--><p class="noindent" >A nice geometrical way to picture complex functions is to think of them as <span 
class="rm-lmsso-12">Complex Mappings</span>. A
complex function \(w=u+iv = f(z)\) takes the point \(z=x+iy\) in the complex plane and <span 
class="rm-lmsso-12">maps </span>it to \(u+iv\).
</p>
<!--l. 1575--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-87">    
<div class="tcolorbox-title">
<!--l. 1575--><p class="noindent" >Example 1.71</p></div> 
<div class="tcolorbox-content"></p><!--l. 1479--><p class="noindent" >Show the effect of considering \(w=iz\) as a complex map that acts on the segment \(1\le |z| \le 2\), \(35^\circ \le \arg z \le 75^\circ \) in the complex \(z\)
plane.
</p><!--l. 1482--><p class="noindent" >If we use polar form, \(i = e^{i \frac {\pi }{2}}\), so it represents a (positive, anticlockwise) rotation by \(\pi /2\) radians.
</p><!--l. 1484--><p class="noindent" >I have sneakily used an alternative notation in the figures for Re\((z)\), namely \(\Re (z)\), and Im\((z)\), namely \(\Im (z)\).
</p><!--l. 1487--><p class="noindent" ><img 
src="F17ZD_main35x.svg" alt="   ∘∘            ∘ ∘
ℜℑz13wℜℑw1152((-p&#x003C;((-&#x003C;5zzl=wwp))a≤ln|zi))a|w≤e|znae|&#x003C;r &#x003C;agr2zg2w≤ 7≤5165
"  />
</p>
 
</div> 
</div>                                                                                             
<!--l. 1577--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<!--l. 1672--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-88">    
<div class="tcolorbox-title">
<!--l. 1672--><p class="noindent" >Example 1.72</p></div> 
<div class="tcolorbox-content"></p><!--l. 1580--><p class="noindent" >Show the effect of considering \(w=z^2\) as a complex map that acts on the segment \(1\le |z| \le 2\), \(35^\circ \le \arg z \le 75^\circ \) in the complex \(z\)
plane.
</p><!--l. 1583--><p class="noindent" >Again, polar form is useful. If \(z = r e^{i \theta }\), then \(w=r^2 e^{2 i \theta }\).
</p><!--l. 1585--><p class="noindent" ><img 
src="F17ZD_main36x.svg" alt="ℜℑz31wℜℑw7150((-p&#x003C;((-&#x003C;zzl∘=wwp∘))a ≤l ≤n|zz))a|we|2naea|r&#x003C;r &#x003C;ggz2w4≤≤ 7515∘0∘  "  />
</p> 
</div> 
</div>                                                                                                                      
<!--l. 1787--><p class="noindent" >
</p><!--l. 1787--><p class="noindent" ><a 
 id="x1-13003r73"></a><!--l. 1680--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.73</span>                                                                          </p><!--l. 1680--><p class="noindent" >We can use a complex function to convert a circle (with certain properties) to an aerofoil shape. Each
dot on the left plot (the \(z\) values) is connected to one on the right (the \(w\) values) by the formula
\begin {equation*}  \label {eqn:comptrans} w(z) = \frac {1}{2}\left (z + \frac {1}{z}\right ).  \end {equation*}
The highlighted circles and diamonds show where their \(z\) values end up on the \(w\) value plot. Note that
the circle in the \(z\) plane is slightly off-centre.
</p>                                                                                        
                                                                                          
                                                                                          
</p><!--l. 1787--><p class="noindent" ><div class="minipage"><div class="center" 
>
<!--l. 1689--><p class="noindent" >
</p><!--l. 1690--><p class="noindent" ><img 
src="F17ZD_main37x.svg" alt="RI-1-1r(em11− ∖∖0ll.ee2ff,tt0((.2zz∖∖)rriigghhtt))
"  /></p></div>
</div><div class="minipage"><div class="center" 
>
<!--l. 1725--><p class="noindent" >
</p><!--l. 1726--><p class="noindent" ><img 
src="F17ZD_main38x.svg" alt="RI-1-1em11 ∖∖lleefftt((ww∖∖rriigghhtt ))
"  /></p></div></div>
<!--l. 1766--><p class="noindent" >\(w(z)\) in this example is a <span 
class="rm-lmsso-12">complex </span>function. The argument \(z\) is a complex number and the function returns a
complex number \(w(z)\).
</p><!--l. 1771--><p class="noindent" >Let us pick apart \( w(z)\) in Cartesian form. \begin {align*}  w(z) &amp;= \frac {1}{2}\left (z + \frac {1}{z}\right ) = \frac {1}{2}\left (x + i y + \frac {1}{x+iy}\right )\\ &amp;= \frac {1}{2}\left (x + i y + \frac {x-iy}{x^2+y^2}\right )\\ &amp;= \frac {1}{2}\left (x + \frac {x}{x^2+y^2} + i \left ( y - \frac {y}{x^2+y^2} \right ) \right )  \end {align*}
</p><!--l. 1778--><p class="noindent" >We can see that the <span 
class="rm-lmsso-12">complex </span>function \( w(z)\) may be written using two <span 
class="rm-lmsso-12">real</span>-valued functions of \(x,y\) \[ w(z) = u(x,y) + i v(x,y) \] where \[ u(x,y) = x + \frac {x}{x^2+y^2}, \qquad v(x,y) = y - \frac {y}{x^2+y^2} \]
This is true in general. </p> 
</div> 
</div>                                                                  
                                                                                          
                                                                                          
</p>
<!--l. 1904--><p class="noindent" >
</p><!--l. 1904--><p class="noindent" ><a 
 id="x1-13004r74"></a><!--l. 1790--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 1.74</span>                                                                          </p><!--l. 1790--><p class="noindent" >Let the initial circle now be centred on the origin with radius  \(R\), i.e. \(|z|=R\) with \(R&gt;0\). What is \(w(z)\)?
</p><!--l. 1793--><p class="noindent" >Parametrize it by \[ z = R e^{i\theta }, \qquad 0\le \theta &lt; 2\pi . \] Then \begin {align*}  w(z) &amp;= \frac 12\left (z+\frac 1z\right ) = \frac 12\left (R e^{i\theta } + \frac {1}{R}e^{-i\theta }\right ) \\ &amp;= \frac 12\Big ((R+\tfrac 1R)\cos \theta \;+\; i\,(R-\tfrac 1R)\sin \theta \Big ).  \end {align*}
</p><!--l. 1804--><p class="noindent" >Writing \(w=u+iv\), we have \[ u=\frac {R+\frac 1R}{2}\cos \theta , \qquad v=\frac {R-\frac 1R}{2}\sin \theta . \] Eliminating \(\theta \) gives \[ \frac {u^2}{\left (\frac {R+\frac 1R}{2}\right )^2} + \frac {v^2}{\left (\frac {|R-\frac 1R|}{2}\right )^2} =1. \] Hence the image of the circle \(|z|=R\) under \(w(z)=\tfrac 12(z+1/z)\) is an ellipse centered at
the origin, with semiaxes \[ a=\frac {R+\frac 1R}{2} \quad \text {(along the real axis)},\qquad b=\frac {|R-\frac 1R|}{2} \quad \text {(along the imaginary axis)}. \] In the special case \(R=1\), the ellipse degenerates to the line segment.
\[ w=\cos \theta \in [-1,1]. \]
</p>
<div class="minipage"><div class="center" 
>
<!--l. 1833--><p class="noindent" >
</p><!--l. 1834--><p class="noindent" ><img 
src="F17ZD_main39x.svg" alt="RI-1-1|em11z|∖∖ =lleeffRtt((zz∖∖rriigghhtt))
"  /></p></div>                                                                                        
                                                                                          
                                                                                          
</p><!--l. 1904--><p class="noindent" ></div><div class="minipage"><div class="center" 
>
<!--l. 1864--><p class="noindent" >
</p><!--l. 1865--><p class="noindent" ><img 
src="F17ZD_main40x.svg" alt="     1
RI-1-1wem11 ∖∖=llee2ff(ttz((ww+∖r∖1ir∕gigzhh)tt ))
"  /></p></div></div>
 
</div> 
</div>                                                                                        
</p>
<div class="tcolorbox exercise" id="tcolobox-89">    
<div class="tcolorbox-title">
<!--l. 1910--><p class="noindent" >Exercise 1.75 </p></div> 
<div class="tcolorbox-content"><!--l. 1907--><p class="noindent" >Consider the <span 
class="rm-lmsso-12">quadratic map</span>, \(f(z) = z^2 +c\) where \(c=a+ib\) (with \(a,b\) real) is a complex constant.
</p><!--l. 1909--><p class="noindent" >What is \(f(z)\) when written in terms of real functions \(g,h\). i.e. \(f(z)= f(x+iy) = g(x,y) + i h(x,y)\)? </p> 
</div> 
</div>
<!--l. 1919--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-90">    
<div class="tcolorbox-title">
<!--l. 1919--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 1913--><p class="noindent" >Substitute \(z=x+iy\) into \(f(z)=z^2 + c\) and gather the real and imaginary parts: </p><div class="eqnarray">\begin {eqnarray*}  f(z) &amp;= f(z+iy) = (x+iy)^2 + a + ib \\ &amp;= (x^2 - y^2 + a) + i (2 xy +b)  \end {eqnarray*}
</div>
<!--l. 1918--><p class="noindent" >hence \(g(x,y) = x^2 - y^2 + a\) and \(h(x,y) = 2 xy +b \) </p> 
</div> 
</div>                                                                                               
<h4 class="subsectionHead"><span class="titlemark">1.9   </span> <a 
 id="x1-140001.9"></a>Iterating complex maps</h4>
<!--l. 1924--><p class="noindent" >One of the most striking uses of complex numbers is in <span 
class="rm-lmsso-12">complex dynamics</span>, where we repeatedly apply a
map \[ z_{n+1}=f(z_n),\qquad n=0,1,2,\dots \] starting from some initial value \(z_0\in \mathbf {C}\). The resulting sequence \(\{z_n\}\) is called the <span 
class="rm-lmsso-12">orbit </span>of \(z_0\). Even very simple
choices of \(f\) can produce complicated behaviour: orbits may converge to a fixed point, fall into a periodic
cycle, or escape to infinity.
                                                                                          
                                                                                          
</p><!--l. 1933--><p class="noindent" >We have just encountered the <span 
class="rm-lmsso-12">quadratic map </span>\[ f_c(z)=z^2+c, \] where \(c\in \mathbf {C}\) is a complex parameter. A useful practical fact is
the <span 
class="rm-lmsso-12">escape radius</span>: if at some step \(|z_n|&gt;2\) (when \( | c | &lt; 2 \)), then the orbit will diverge to infinity (so we do not need to
keep iterating).
</p>
<!--l. 1948--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-91">    
<div class="tcolorbox-title">
<!--l. 1948--><p class="noindent" >Example 1.76</p></div> 
<div class="tcolorbox-content"></p><!--l. 1941--><p class="noindent" >If we consider a few iterates in the complex plane: Take \(c=-0.123+0.745\,i\) and start at \(z_0=0\). The first few iterates
are \[ z_1=c,\qquad z_2\approx -0.6629+0.5617\,i,\qquad z_3\approx 8.9\times 10^{-4}+2.6\times 10^{-4}\,i, \] and then the orbit returns close to \(z_1\) and \(z_2\) again. </p> 
</div> 
</div>                                                    
<!--l. 1962--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-92">    
<div class="tcolorbox-title">
<!--l. 1962--><p class="noindent" >Example 1.77</p></div> 
<div class="tcolorbox-content"></p><!--l. 1952--><p class="noindent" >Julia sets (bounded starting points): Fix \(c\in \mathbf {C}\). The <span 
class="rm-lmsso-12">filled Julia set </span>\(K_c\) is the set of starting values \(z_0\)
whose orbits under \(z_{n+1}=z_n^2+c\) stay bounded. (The <span 
class="rm-lmsso-12">Julia set </span>\(J_c\) is the boundary of \(K_c\).) The picture below is
a coarse “escape-time” plot for \(c=-1\): points \(z_0\) that do <span 
class="rm-lmsso-12">not </span>escape after a fixed number of iterations
(200) are drawn as yellow dots.
</p>
<div class="center" 
>
<!--l. 1959--><p class="noindent" >
</p><!--l. 1960--><p class="noindent" ><img 
src="julia_c_minus1.png" alt="PIC"  
width="230" height="230"  /></p></div>
 
</div> 
</div>                                                                                                                      
<!--l. 1975--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-93">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 1975--><p class="noindent" >Example 1.78</p></div> 
<div class="tcolorbox-content"></p><!--l. 1965--><p class="noindent" >
</p><!--l. 1966--><p class="noindent" >The Mandelbrot set (bounded parameters):
</p><!--l. 1968--><p class="noindent" >Instead of fixing \(c\) and varying \(z_0\), we can fix the starting point \(z_0=0\) and vary the parameter \(c\). The
<span 
class="rm-lmsso-12">Mandelbrot set </span>\(M\) is the set of parameters \(c\) for which the orbit of \(0\) under \(z_{n+1}=z_n^2+c\) remains bounded. The
plot below is again an escape-time picture (very low resolution), now in the \(c\)-plane.
</p>
<div class="center" 
>
<!--l. 1972--><p class="noindent" >
</p><!--l. 1973--><p class="noindent" ><img 
src="mandelbrot.png" alt="PIC"  
width="230" height="230"  /></p></div>
 
</div> 
</div>                                                                                                                      
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
<h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-150002"></a>An aside: Quaternions</h3>
<!--l. 3--><p class="noindent" >Complex numbers extend the real numbers by adjoining a new unit \(i\) with \(i^2=-1\). Quaternions extend the
complex numbers further by adjoining <span 
class="rm-lmsso-12">three </span>imaginary units.
</p><!--l. 5--><p class="noindent" >Quaternions were introduced by the Irish mathematician William Rowan Hamilton in 1843 as a way to
extend complex numbers to describe rotations in three dimensions. After years of trying (and failing) to
build a consistent “three–dimensional complex arithmetic”, Hamilton realised that the key
was to move to four components and to accept that multiplication need not commute. On
16th October 1843, while walking in Dublin, he famously carved the fundamental relations
\begin {equation*}  \mathbf {i}^2=\mathbf {j}^2=\mathbf {k}^2=\mathbf {i}\mathbf {j}\mathbf {k}=-1.  \end {equation*}
into the stone of Brougham Bridge. </p>
<div class="center" 
>
<!--l. 10--><p class="noindent" >
</p><!--l. 11--><p class="noindent" ><img 
src="Broom_Bridge.jpg" alt="PIC"  
width="236" height="236"  /></p></div>
<!--l. 14--><p class="noindent" >Quaternions quickly attracted attention in 19th-century mathematical physics and geometry, but they
were later overshadowed in many applications by the vector calculus of Gibbs and Heaviside. In the late
20th century they saw a major revival in engineering and computing—especially in robotics, aerospace,
and computer graphics—because unit quaternions provide a numerically stable and efficient way to
represent 3D rotations without the singularities that can occur with Euler angles, which appear when
using vectors.
</p><!--l. 16--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">2.1   </span> <a 
 id="x1-160002.1"></a>Definitions and Arithmetic</h4>
<div class="tcolorbox df" id="tcolobox-94">    
<div class="tcolorbox-title">
<!--l. 28--><p class="noindent" >Definition 2.1  Quaternion</p></div> 
<div class="tcolorbox-content"><!--l. 19--><p class="noindent" >A <span 
class="rm-lmsso-12">quaternion </span>is an expression of the form \begin {equation*}  q = a + b\,\mathbf {i} + c\,\mathbf {j} + d\,\mathbf {k}  \end {equation*}
where \(a,b,c,d\in \R \) and the symbols \(\mathbf {i},\mathbf {j},\mathbf {k}\) satisfy \begin {equation*}  \mathbf {i}^2=\mathbf {j}^2=\mathbf {k}^2=\mathbf {i}\mathbf {j}\mathbf {k}=-1.  \end {equation*}
The set of all quaternions is denoted \(\mathbb H\). </p> 
</div> 
</div>
<!--l. 30--><p class="noindent" >From the defining relations one can derive the multiplication rules </p>
<div class="center" 
>
<!--l. 31--><p class="noindent" >
</p>
                                                                                          
                                                                                          
<div class="tabular"> <table id="TBL-2" class="tabular" 
 
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1" /></colgroup><colgroup id="TBL-2-2g"><col 
id="TBL-2-2" /><col 
id="TBL-2-3" /><col 
id="TBL-2-4" /></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-1"  
class="td11"> \(\cdot \) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-2"  
class="td11"> \(\mathbf {i}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-3"  
class="td11"> \(\mathbf {j}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-4"  
class="td11"> \(\mathbf {k}\) </td></tr><tr class="hline" style="border-top:1px solid #000"><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-1"  
class="td11">  \(\mathbf {i}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-2"  
class="td11">  \(-1\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-3"  
class="td11">  \(\mathbf {k}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-4"  
class="td11">  \(-\mathbf {j}\)</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-1"  
class="td11"> \(\mathbf {j}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-2"  
class="td11"> \(-\mathbf {k}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-3"  
class="td11"> \(-1\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-4"  
class="td11"> \(\mathbf {i}\) </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-1"  
class="td11"> \(\mathbf {k}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-2"  
class="td11"> \(\mathbf {j}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-3"  
class="td11"> \(-\mathbf {i}\) </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-4"  
class="td11"> \(-1\) </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-2-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-1"  
class="td11">   </td>
</tr></table></div></div>
<!--l. 42--><p class="noindent" ><img 
src="F17ZD_main41x.svg" alt="
"  />Quaternion multiplication is <span 
class="rm-lmsso-12">not commutative</span>. For example, \begin {equation*}  \mathbf {i}\mathbf {j}=\mathbf {k}\qquad \text {but}\qquad \mathbf {j}\mathbf {i}=-\mathbf {k}.  \end {equation*}
</p>
<div class="tcolorbox df" id="tcolobox-95">    
<div class="tcolorbox-title">
<!--l. 53--><p class="noindent" >Definition 2.2  Scalar and vector parts</p></div> 
<div class="tcolorbox-content"><!--l. 48--><p class="noindent" >Write a quaternion as \begin {equation*}  q = a + \vec v,\qquad \text {where}\quad \vec v=b\,\mathbf {i}+c\,\mathbf {j}+d\,\mathbf {k}.  \end {equation*}
We call \(a\) the <span 
class="rm-lmsso-12">scalar part </span>of \(q\) and \(\vec v\) the <span 
class="rm-lmsso-12">vector part</span>. </p> 
</div> 
</div>
<!--l. 55--><p class="noindent" >Addition and subtraction are componentwise (exactly as for complex numbers): \begin {equation*}  (a,\,b,\,c,\,d)\pm (e,\,f,\,g,\,h)=(a\pm e,\,b\pm f,\,c\pm g,\,d\pm h).  \end {equation*}
Multiplication is defined using distributivity together with the table above.
</p>
<!--l. 73--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-96">    
<div class="tcolorbox-title">
<!--l. 73--><p class="noindent" >Example 2.3</p></div> 
<div class="tcolorbox-content"></p><!--l. 62--><p class="noindent" >Compute \((1+\mathbf {i})(1+\mathbf {j})\). \begin {align*}  (1+\mathbf {i})(1+\mathbf {j})&amp;=1+\mathbf {j}+\mathbf {i}+\mathbf {i}\mathbf {j}\\ &amp;=1+\mathbf {i}+\mathbf {j}+\mathbf {k}.  \end {align*}
</p><!--l. 67--><p class="noindent" >If we swap the factors, \begin {align*}  (1+\mathbf {j})(1+\mathbf {i})&amp;=1+\mathbf {i}+\mathbf {j}+\mathbf {j}\mathbf {i}\\ &amp;=1+\mathbf {i}+\mathbf {j}-\mathbf {k},  \end {align*}
</p><!--l. 72--><p class="noindent" >so the order matters. </p> 
</div> 
</div>                                                                                          
<!--l. 75--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">2.2   </span> <a 
 id="x1-170002.2"></a>Conjugate, Norm and Inverse</h4>
<!--l. 77--><p class="noindent" >Quaternions have an analogue of complex conjugation.
</p>
<div class="tcolorbox df" id="tcolobox-97">    
<div class="tcolorbox-title">
<!--l. 84--><p class="noindent" >Definition 2.4  Quaternion conjugate</p></div> 
<div class="tcolorbox-content"><!--l. 80--><p class="noindent" >For \(q=a+b\mathbf {i}+c\mathbf {j}+d\mathbf {k}\), the <span 
class="rm-lmsso-12">conjugate </span>is \begin {equation*}  \conj {q}=a-b\mathbf {i}-c\mathbf {j}-d\mathbf {k}.  \end {equation*}
</p> 
</div> 
</div>
<div class="tcolorbox df" id="tcolobox-98">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 91--><p class="noindent" >Definition 2.5  Norm</p></div> 
<div class="tcolorbox-content"><!--l. 87--><p class="noindent" >The <span 
class="rm-lmsso-12">norm </span>of \(q\) is defined by \begin {equation*}  \lVert q\rVert = \sqrt {q\conj {q}}=\sqrt {a^2+b^2+c^2+d^2}.  \end {equation*}
</p> 
</div> 
</div>
<!--l. 99--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-99">    
<div class="tcolorbox-title">
<!--l. 99--><p class="noindent" >Example 2.6</p></div> 
<div class="tcolorbox-content"></p><!--l. 94--><p class="noindent" >Let \(q=2-\mathbf {i}+2\mathbf {j}+\mathbf {k}\). Then \begin {align*}  \conj {q}&amp;=2+\mathbf {i}-2\mathbf {j}-\mathbf {k},\\ \lVert q\rVert &amp;=\sqrt {2^2+(-1)^2+2^2+1^2}=\sqrt {10}.  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 101--><p class="noindent" >Just like complex numbers, the norm is useful for division.
</p>
<div class="tcolorbox thm" id="tcolobox-100">    
<div class="tcolorbox-title">
<!--l. 108--><p class="noindent" >Theorem 2.7  Inverse of a nonzero quaternion</p></div> 
<div class="tcolorbox-content"><!--l. 104--><p class="noindent" >If \(q\neq 0\), then \(q\) has a multiplicative inverse \begin {equation*}  q^{-1}=\frac {\conj {q}}{\lVert q\rVert ^2}.  \end {equation*}
</p> 
</div> 
</div>
<!--l. 117--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-101">    
<div class="tcolorbox-title">
<!--l. 117--><p class="noindent" >Example 2.8</p></div> 
<div class="tcolorbox-content"></p><!--l. 111--><p class="noindent" >Find the inverse of \(q=1+\mathbf {i}+\mathbf {j}+\mathbf {k}\). \begin {align*}  \conj {q}&amp;=1-\mathbf {i}-\mathbf {j}-\mathbf {k},\\ \lVert q\rVert ^2&amp;=1^2+1^2+1^2+1^2=4,\\ q^{-1}&amp;=\frac {1}{4}\,(1-\mathbf {i}-\mathbf {j}-\mathbf {k}).  \end {align*}
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 119--><p class="noindent" ><img 
src="F17ZD_main42x.svg" alt="
"  />Because multiplication is not commutative, one must distinguish between left and right division in more
advanced settings. In this course we will only divide by placing the inverse on the <span 
class="rm-lmsso-12">right</span>:
\(\,p/q:=p\,q^{-1}\).
</p><!--l. 121--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">2.3   </span> <a 
 id="x1-180002.3"></a>Unit Quaternions and 3D Rotations</h4>
<!--l. 123--><p class="noindent" >A key application of quaternions is the description of rotations in \(\R ^3\).
</p>
<div class="tcolorbox df" id="tcolobox-102">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 131--><p class="noindent" >Definition 2.9  Pure imaginary quaternion</p></div> 
<div class="tcolorbox-content"><!--l. 126--><p class="noindent" >A quaternion with zero scalar part, \begin {equation*}  \vec v=b\mathbf {i}+c\mathbf {j}+d\mathbf {k},  \end {equation*}
is called <span 
class="rm-lmsso-12">pure imaginary</span>. We identify it with the vector \((b,c,d)\in \R ^3\). </p> 
</div> 
</div>
<div class="tcolorbox df" id="tcolobox-103">    
<div class="tcolorbox-title">
<!--l. 135--><p class="noindent" >Definition 2.10  Unit quaternion</p></div> 
<div class="tcolorbox-content"><!--l. 134--><p class="noindent" >A quaternion \(q\) with \(\lVert q\rVert =1\) is called a <span 
class="rm-lmsso-12">unit quaternion</span>. </p> 
</div> 
</div>
<!--l. 137--><p class="noindent" >Any rotation by angle \(\theta \) about a unit axis \(\hat n\in \R ^3\) can be encoded by the unit quaternion \begin {equation*}  q = \cos \frac {\theta }{2} + \sin \frac {\theta }{2}\,(n_1\mathbf {i}+n_2\mathbf {j}+n_3\mathbf {k}),\qquad \hat n=(n_1,n_2,n_3),\ \lVert \hat n\rVert =1.  \end {equation*}
Given a vector \(\vec v\in \R ^3\) (viewed as a pure imaginary quaternion), the rotated vector is \begin {equation*}  \vec v\,' = q\,\vec v\,q^{-1}.  \end {equation*}
</p>
<!--l. 162--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-104">    
<div class="tcolorbox-title">
<!--l. 162--><p class="noindent" >Example 2.11</p></div> 
<div class="tcolorbox-content"></p><!--l. 147--><p class="noindent" >Rotate \((1,0,0)\) by \(90^\circ \) about the \(z\)–axis.
</p><!--l. 149--><p class="noindent" >The axis is \(\hat n=(0,0,1)\) and \(\theta =\pi /2\), so \begin {equation*}  q=\cos \frac {\pi }{4}+\sin \frac {\pi }{4}\,\mathbf {k}=\frac {\sqrt 2}{2}+\frac {\sqrt 2}{2}\,\mathbf {k},\qquad q^{-1}=\conj {q}=\frac {\sqrt 2}{2}-\frac {\sqrt 2}{2}\,\mathbf {k}.  \end {equation*}
Let \(\vec v=\mathbf {i}\) (since \((1,0,0)\) corresponds to \(\mathbf {i}\)). Then \begin {align*}  \vec v\,' &amp;= q\,\mathbf {i}\,q^{-1} =\left (\tfrac {\sqrt 2}{2}+\tfrac {\sqrt 2}{2}\mathbf {k}\right )\mathbf {i}\left (\tfrac {\sqrt 2}{2}-\tfrac {\sqrt 2}{2}\mathbf {k}\right )\\ &amp;=\left (\tfrac {\sqrt 2}{2}\mathbf {i}+\tfrac {\sqrt 2}{2}\mathbf {k}\mathbf {i}\right )\left (\tfrac {\sqrt 2}{2}-\tfrac {\sqrt 2}{2}\mathbf {k}\right ) =\left (\tfrac {\sqrt 2}{2}\mathbf {i}+\tfrac {\sqrt 2}{2}\mathbf {j}\right )\left (\tfrac {\sqrt 2}{2}-\tfrac {\sqrt 2}{2}\mathbf {k}\right )\\ &amp;= \mathbf {j}.  \end {align*}
</p><!--l. 161--><p class="noindent" >Thus \((1,0,0)\) rotates to \((0,1,0)\), as expected. </p> 
</div> 
</div>                                                                              
                                                                                          
                                                                                          
<div class="tcolorbox exercise" id="tcolobox-105">    
<div class="tcolorbox-title">
<!--l. 168--><p class="noindent" >Exercise 2.12 </p></div> 
<div class="tcolorbox-content"><!--l. 167--><p class="noindent" >Use the multiplication table to compute \(\mathbf {i}\mathbf {k}\) and \(\mathbf {k}\mathbf {i}\). What do you notice? </p> 
</div> 
</div>
<!--l. 171--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-106">    
<div class="tcolorbox-title">
<!--l. 171--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 170--><p class="noindent" >From the table, \(\mathbf {i}\mathbf {k}=-\mathbf {j}\) and \(\mathbf {k}\mathbf {i}=\mathbf {j}\). They differ by a minus sign, illustrating non-commutativity. </p> 
</div> 
</div>       
<div class="tcolorbox exercise" id="tcolobox-107">    
<div class="tcolorbox-title">
<!--l. 175--><p class="noindent" >Exercise 2.13 </p></div> 
<div class="tcolorbox-content"><!--l. 174--><p class="noindent" >Let \(q=3-2\mathbf {i}+\mathbf {j}\). Compute \(\conj {q}\), \(\lVert q\rVert \), and \(q^{-1}\). </p> 
</div> 
</div>
<!--l. 185--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-108">    
<div class="tcolorbox-title">
<!--l. 185--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 177--><p class="noindent" >We have \(\conj {q}=3+2\mathbf {i}-\mathbf {j}\) and \begin {equation*}  \lVert q\rVert =\sqrt {3^2+(-2)^2+1^2+0^2}=\sqrt {14}.  \end {equation*}
Hence \begin {equation*}  q^{-1}=\frac {\conj {q}}{\lVert q\rVert ^2}=\frac {1}{14}\,(3+2\mathbf {i}-\mathbf {j}).  \end {equation*}
</p> 
</div> 
</div>                                                                                                               
<div class="tcolorbox exercise" id="tcolobox-109">    
<div class="tcolorbox-title">
<!--l. 189--><p class="noindent" >Exercise 2.14 </p></div> 
<div class="tcolorbox-content"><!--l. 188--><p class="noindent" >Let \(\hat n=(1,0,0)\) and \(\theta =\pi \). Write down the unit quaternion \(q\) describing rotation by \(\pi \) about the \(x\)–axis, and
compute \(q\,\mathbf {j}\,q^{-1}\). </p> 
</div> 
</div>
<!--l. 197--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-110">    
<div class="tcolorbox-title">
<!--l. 197--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 191--><p class="noindent" >Here \(q=\cos (\pi /2)+\sin (\pi /2)\,\mathbf {i}=\mathbf {i}\) and \(q^{-1}=\conj {q}=-\mathbf {i}\). Then \begin {equation*}  q\,\mathbf {j}\,q^{-1}=\mathbf {i}\,\mathbf {j}\,(-\mathbf {i})=-(\mathbf {i}\mathbf {j})\mathbf {i}=-\mathbf {k}\mathbf {i}=-\mathbf {j}.  \end {equation*}
So the \(y\)–axis is sent to \(-y\), as expected for a \(180^\circ \) rotation about the \(x\)–axis. </p> 
</div> 
</div>                         
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
<h3 class="sectionHead"><span class="titlemark">3   </span> <a 
 id="x1-190003"></a>Vectors, Spans and Bases in \(\FR ^2\), \(\FR ^3\) and \(\FR ^n\).</h3>
<!--l. 3--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">3.1   </span> <a 
 id="x1-200003.1"></a>Vectors in \(\FR ^2\)</h4>
<!--l. 4--><p class="noindent" >We briefly recall some facts about vectors in \(\FR ^2\). Intuitively, a <span 
class="rm-lmsso-12">vector </span>of \(\FR ^2\) can be thought as an arrow
encoding a length and a direction. A vector however does not have an origin, so one may draw many
arrows in the plane corresponding to the same vector:
</p><!--l. 21--><p class="noindent" >\begin {equation*}  \begin {picture}(70,70) \put (0.0,8.0){\line (1,0){70}} \put (8.0,0.0){\line (0,1){70}} \put (8.0,8.0){\vector (2,1){50}} \put (58.0,15.0){\vector (2,1){50}} \put (22.0,40.0){\vector (2,1){50}} \put (33.0,10.0){\line (0,-1){4}} \put (58.0,10.0){\line (0,-1){4}} \put (6.0,33.0){\line (1,0){4}} \put (6.0,58.0){\line (1,0){4}} \end {picture}  \end {equation*}
</p><!--l. 23--><p class="noindent" >One may identify the set of all vectors in \(\FR ^2\) with the set of points of the plane \(\FR ^2\), by associating to each
point of the plane the vector from the origin to that point. Under this identification, the origin
corresponds to a vector called the <span 
class="rm-lmsso-12">null vector </span>and denoted \(\nv \).
</p>
<div class="tcolorbox df" id="tcolobox-111">    
<div class="tcolorbox-title">
<!--l. 49--><p class="noindent" >Definition 3.1  </p></div> 
<div class="tcolorbox-content"><!--l. 26--><p class="noindent" >Each point and thus each vector \(\vv \in \FR ^2\) can be denoted by a pair of real numbers (thus the notation \(\FR ^2\)):
the horizontal displacement and the vertical displacement. Such a vector is denoted by the \(2 \times 1\) matrix
\begin {equation*}  \vv =\vectt {v_1}{v_2}~.  \end {equation*}

</p><!--l. 45--><p class="noindent" >\begin {equation*}  \begin {picture}(70,70) \put (0.0,8.0){\line (1,0){70}} \put (8.0,0.0){\line (0,1){70}} \put (8.0,8.0){\vector (2,1){50}} \put (33.0,10.0){\line (0,-1){4}} \put (58.0,10.0){\line (0,-1){4}} \put (6.0,33.0){\line (1,0){4}} \put (6.0,58.0){\line (1,0){4}} \put (0.0,33.0){\makebox (0,0)[c]{$v_2$}} \put (58.0,0.0){\makebox (0,0)[c]{$v_1$}} \end {picture}  \end {equation*}
</p><!--l. 48--><p class="noindent" >It is a very useful convention to write vectors vertically, i.e. as \(2 \times 1\) matrices. For typesetting reasons
however, we often write that vector as \(\vv =(v_1,v_2)^T\), i.e. as a \(1\times 2\) matrix instead. </p> 
</div> 
</div>
<!--l. 51--><p class="noindent" >We have the following rules for adding two vectors \(\vv =(v_1,v_2)^T\) and \(\wv =(w_1,w_2)^T\) and for multiplying a vector by a number \(\lambda \in \FR \)
(called a <span 
class="rm-lmsso-12">scalar</span>): \begin {equation*}  \vv +\wv =\vectt {v_1}{v_2}+\vectt {w_1}{w_2}:=\vectt {v_1+w_1}{v_2+w_2}\eand \lambda \vv :=\vectt {\lambda v_1}{\lambda v_2}~.  \end {equation*}
\begin {equation*}  \begin {picture}(200,150)(0,50) \put (0.0,100.0){\line (1,0){200}} \put (100.0,50.0){\line (0,1){150}} \put (100.0,100.0){\vector (1,3){30}} \put (100.0,100.0){\vector (3,-1){45}} \put (100.0,100.0){\vector (-3,1){90}} \put (130.0,190.0){\vector (3,-1){45}} \put (100.0,100.0){\vector (1,1){75}} \put (95.0,92.0){\makebox (0,0)[c]{$\nv $}} \put (120.0,180.0){\makebox (0,0)[c]{$\vv _1$}} \put (185.0,160.0){\makebox (0,0)[c]{$\vv _1+\vv _2$}} \put (140.0,80.0){\makebox (0,0)[c]{$\vv _2$}} \put (30.0,130.0){\makebox (0,0)[c]{$-2\vv _2$}} \end {picture}  \end {equation*}
</p><!--l. 72--><p class="noindent" >With our convention, the null vector is the vector with coordinates \((0,0)^T\). Note that if we multiply a vector by
\(0\), we obtain the null vector: \(0\vv =\nv \) for all \(\vv \in \FR ^2\).
</p><!--l. 160--><p class="noindent" >
</p>
                                                                                          
                                                                                          
<h4 class="subsectionHead"><span class="titlemark">3.2   </span> <a 
 id="x1-210003.2"></a>Vectors in \(\FR ^3\)</h4>
<div class="tcolorbox df" id="tcolobox-112">    
<div class="tcolorbox-title">
<!--l. 167--><p class="noindent" >Definition 3.2  </p></div> 
<div class="tcolorbox-content"><!--l. 162--><p class="noindent" >Most of the notions introduced on \(\FR ^2\) straightforwardly extend to \(\FR ^3\). A point in the Euclidean
dimensions encodes a vector \(\vv \in \FR ^3\), i.e. an arrow from the origin \(\nv \) to that point. We describe the
vector \(\vv \) by a \(3\times 1\) matrix
</p>
<div class="math-display" >
<img 
src="F17ZD_main43x.svg" alt="           v1
⃗v = ∖lef t( v2 ∖right),
           v3
" class="math-display"  /></div>
<!--l. 166--><p class="noindent" >or sometimes as a \(1 \times 3\) matrix \(\vv =(v_1,v_2,v_3)^T\) for typesetting reasons. </p> 
</div> 
</div>
<!--l. 170--><p class="noindent" >We can add vectors and multiply them by scalars as before: \begin {equation*}  \vv +\wv =\vecttt {v_1}{v_2}{v_3}+\vecttt {w_1}{w_2}{w_3}=\vecttt {v_1+w_1}{v_2+w_2}{v_3+w_3}\eand \lambda \vv =\vecttt {\lambda v_1}{\lambda v_2}{\lambda v_3}~.  \end {equation*}
</p><!--l. 185--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">3.3   </span> <a 
 id="x1-220003.3"></a>Vectors in \(\FR ^n\)</h4>
<!--l. 186--><p class="noindent" >The generalisation to \(\FR ^n\) should now be clear.
</p>
<div class="tcolorbox df" id="tcolobox-113">    
<div class="tcolorbox-title">
<!--l. 194--><p class="noindent" >Definition 3.3  </p></div> 
<div class="tcolorbox-content"><!--l. 189--><p class="noindent" >We define: \begin {equation*}  \FR ^n=\left \{\vecttt {x_1}{\vdots }{x_n}~\Big |~x_i\in \FR ~,~~~i=1,\ldots ,n\right \}~.  \end {equation*}
An element of \(\FR ^n\) is called a <span 
class="rm-lmsso-12">vector </span>(of \(\FR ^n\)), and is identified with an \(n\times 1\) matrix. </p> 
</div> 
</div>
<div class="tcolorbox df" id="tcolobox-114">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 202--><p class="noindent" >Definition 3.4  </p></div> 
<div class="tcolorbox-content"><!--l. 197--><p class="noindent" >For vectors \(\xv , \yv \in \FR ^n\) and a scalar \(\lambda \in \FR \), we define the following operations on vectors: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 199--><p class="noindent" >addition: \(\vecttt {x_1}{\vdots }{x_n}+\vecttt {y_1}{\vdots }{y_n}:=\vecttt {x_1+y_1}{\vdots }{x_n+y_n}\),
        </p></li>
        <li class="itemize">
        <!--l. 200--><p class="noindent" >multiplication by a scalar: \(\lambda \vecttt {x_1}{\vdots }{x_n}:=\vecttt {\lambda x_1}{\vdots }{\lambda x_n}.\)</p></li></ul>
 
</div> 
</div>
<!--l. 204--><p class="noindent" >From a given family of vectors, one can construct new vectors using additions and multiplication
scalars.
</p>
<div class="tcolorbox df" id="tcolobox-115">    
<div class="tcolorbox-title">
<!--l. 209--><p class="noindent" >Definition 3.5  </p></div> 
<div class="tcolorbox-content"><!--l. 207--><p class="noindent" >Let \(\vv _1, \ldots , \vv _k\in \FR ^n\). A <span 
class="rm-lmsso-12">linear combination </span>of \(\vv _1, \ldots , \vv _k\) is a vector of the form
</p>
<div class="math-display" >
<img 
src="F17ZD_main44x.svg" alt="λ1⃗v1 + ...+ λk⃗vk     for some λ1,...,λk ∈ ℝ.
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 214--><p class="noindent" >
</p><!--l. 214--><p class="noindent" ><a 
 id="x1-22004r6"></a><!--l. 212--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 3.6</span>                                                                           </p><!--l. 212--><p class="noindent" >The vector  \(\vectt {1}{3}\in \FR ^2\) is a linear combination of  \(\vectt {1}{2}\) and  \(\vectt {2}{3}\), since
</p>
<div class="math-display" >
<img 
src="F17ZD_main45x.svg" alt="∖left( 1 ∖right) = 3∖left( 1 ∖right) − ∖left( 2 ∖right).
       3                   2                  3
" class="math-display"  /></div>
 
</div> 
</div>                                                                                        
</p>
<div class="tcolorbox exercise" id="tcolobox-116">    
<div class="tcolorbox-title">
<!--l. 228--><p class="noindent" >Exercise 3.7 </p></div> 
<div class="tcolorbox-content"><!--l. 217--><p class="noindent" >Let \[ \vv =\begin {pmatrix}1\\-1\end {pmatrix}, \qquad \wv =\begin {pmatrix}-1\\3\end {pmatrix}. \]
        </p><dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
   a)  </dt><dd 
class="enumerate-enumitem">Compute \(\vv +\wv \), \(\vv -\wv \) and \(3\vv -2\wv \).
        </dd><dt class="enumerate-enumitem">
   b)  </dt><dd 
class="enumerate-enumitem">Find all \(\lambda \in \FR \) such that \(\vv +\lambda \wv =\nv \) (if any).
        </dd><dt class="enumerate-enumitem">
    c)  </dt><dd 
class="enumerate-enumitem">Are \(\vv \) and \(\wv \) scalar multiples of one another?</dd></dl>
 
</div> 
</div>
<!--l. 258--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-117">    
<div class="tcolorbox-title">
<!--l. 258--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content">        </p><dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
   a)  </dt><dd 
class="enumerate-enumitem">\[ \vv +\wv =\begin {pmatrix}1-1\\-1+3\end {pmatrix}=\begin {pmatrix}0\\2\end {pmatrix},\qquad \vv -\wv =\begin {pmatrix}1+1\\-1-3\end {pmatrix}=\begin {pmatrix}2\\-4\end {pmatrix}. \] Also \[ 3\vv -2\wv =3\begin {pmatrix}1\\-1\end {pmatrix}-2\begin {pmatrix}-1\\3\end {pmatrix} =\begin {pmatrix}3\\-3\end {pmatrix}-\begin {pmatrix}-2\\6\end {pmatrix} =\begin {pmatrix}5\\-9\end {pmatrix}. \]
        </dd><dt class="enumerate-enumitem">
   b)  </dt><dd 
class="enumerate-enumitem">Solve \(\vv +\lambda \wv =\nv \): \[ \begin {pmatrix}1\\-1\end {pmatrix}+\lambda \begin {pmatrix}-1\\3\end {pmatrix} =\begin {pmatrix}0\\0\end {pmatrix} \quad \Longleftrightarrow \quad \begin {cases} 1-\lambda =0,\\ -1+3\lambda =0. \end {cases} \] The first gives \(\lambda =1\) while the second gives \(\lambda =\tfrac 13\), which is impossible. Hence there is
        <span 
class="rm-lmsso-12">no</span> \(\lambda \) with \(\vv +\lambda \wv =\nv \).
        </dd><dt class="enumerate-enumitem">
    c)  </dt><dd 
class="enumerate-enumitem">If \(\vv =\mu \wv \), then \(1=-\mu \) and \(-1=3\mu \), giving \(\mu =-1\) and \(\mu =-\tfrac 13\), a contradiction. So they are not scalar multiples. This
        is just rephrasing (b) with \(\mu = - \lambda \).</dd></dl>
 
</div> 
</div>                                                                                                               
<!--l. 264--><p class="noindent" >Writing a vector as a linear combination of other vectors can be thought of as ‘decomposing’ that
vector. Given a family of vectors \(\vv _1, \ldots , \vv _k\) of \(\FR ^n\), we will study the following questions: </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 266--><p class="noindent" >Can every vector of \(\FR ^n\) be written as a linear combination of \(\vv _1, \ldots , \vv _k\)?
      </p></li>
      <li class="itemize">
      <!--l. 267--><p class="noindent" >If not, <span 
class="rm-lmsso-12">which </span>vectors of \(\FR ^n\) can be written as a linear combination of \(\vv _1, \ldots , \vv _k\)?
      </p></li>
      <li class="itemize">
                                                                                          
                                                                                          
      <!--l. 268--><p class="noindent" ><span 
class="rm-lmsso-12">In how many ways </span>can a vector of \(\FR ^n\) be written as a linear combination of \(\vv _1, \ldots , \vv _k\)?</p></li></ul>
<!--l. 271--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-230003.3"></a><span 
class="rm-lmssbx-10x-x-120">The standard basis of</span> \(\FR ^n\)<span 
class="rm-lmssbx-10x-x-120">.</span></span>
There is already a standard family of vectors for which these questions have a simple answer.
</p>
<div class="tcolorbox df" id="tcolobox-118">    
<div class="tcolorbox-title">
<!--l. 279--><p class="noindent" >Definition 3.8  </p></div> 
<div class="tcolorbox-content"><!--l. 276--><p class="noindent" >We introduce the following vectors of \(\FR ^n\):
</p>
<div class="math-display" >
<img 
src="F17ZD_main46x.svg" alt="             1                     0                          0
             0                     1                          ...
⃗e1 :=  ∖left( .. ∖right),⃗e2 := ∖left( 0 ∖right),...,⃗en := ∖left(  ∖right).
             .                     ..                          0
             0                     .                          1
" class="math-display"  /></div>
<!--l. 278--><p class="noindent" >This family of vectors is generally called the <span 
class="rm-lmsso-12">standard basis </span>of \(\FR ^n\). </p> 
</div> 
</div>
<div class="tcolorbox thm" id="tcolobox-119">    
<div class="tcolorbox-title">
<!--l. 288--><p class="noindent" >Theorem 3.9  </p></div> 
<div class="tcolorbox-content"><!--l. 286--><p class="noindent" >Every vector \(\xv = (x_1, \ldots , x_n)^T\) of \(\FR ^n\) can be written in a unique way as a linear combination of \(\ev _1, \ldots , \ev _n\), namely:
</p>
<div class="math-display" >
<img 
src="F17ZD_main47x.svg" alt="⃗x = x1 ⃗e1 + ...+ xn⃗en.
" class="math-display"  /></div>
 
</div> 
</div>
                                                                                          
                                                                                          
<!--l. 294--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-120">    
<div class="tcolorbox-title">
<!--l. 294--><p class="noindent" >Example 3.10</p></div> 
<div class="tcolorbox-content"></p><!--l. 291--><p class="noindent" >The vector \(\vecttt {1}{3}{5}\in \FR ^3\) can be written as a linear combination of the standard basis vectors for \(\FR ^3\), \(\ev _1, \ev _2, \ev _3\) since
</p>
<div class="math-display" >
<img 
src="F17ZD_main48x.svg" alt="       1                                 1                  0                  0
∖lef t( 3 ∖right ) = ⃗e1+3 ⃗e2+5 ⃗e3 = ∖left( 0 ∖right)+3 ∖left( 1 ∖right)+5 ∖left( 0 ∖right).
       5                                 0                  0                  1
" class="math-display"  /></div>
 
</div> 
</div>                                                                                                    
<!--l. 296--><p class="noindent" >Note that it is also possible to use a <span 
class="rm-lmsso-12">non</span>-standard basis to describe the vectors, as in the following
example.
</p>
<!--l. 307--><p class="noindent" >
</p><!--l. 307--><p class="noindent" ><a 
 id="x1-23004r11"></a><!--l. 299--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 3.11</span>                                                                          </p><!--l. 299--><p class="noindent" >The vector  \(\vecttt {1}{3}{5}\in \FR ^3\) from the previous example can also be written as a linear combination of the
non-standard basis vectors for \(\FR ^3\), \(\ev _1, \vec {f}_2, \vec {f}_3\) where
</p>
<div class="math-display" >
<img 
src="F17ZD_main49x.svg" alt="            0                      0
⃗f2 = ∖left( 1 ∖right), ⃗f3 = ∖left( 1  ∖right )
            1                      − 1
" class="math-display"  /></div>
<!--l. 304--><p class="noindent" >since                                                                                   
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 307--><p class="noindent" ></p>
<div class="math-display" >
<img 
src="F17ZD_main50x.svg" alt="       1                                 1                 0                  0
∖left( 3 ∖right) = ⃗e1+4 ⃗f2− 1⃗f3 = ∖left( 0 ∖right)+4 ∖lef t(1  ∖right )− ∖lef t( 1  ∖right).
       5                                 0                 1                 − 1
" class="math-display"  /></div>
 
</div> 
</div>                                                                                        
</p>
<div class="tcolorbox exercise" id="tcolobox-121">    
<div class="tcolorbox-title">
<!--l. 314--><p class="noindent" >Exercise 3.12 </p></div> 
<div class="tcolorbox-content"></p><!--l. 310--><p class="noindent" >  <!--l. 310--><p class="noindent" >?   Can I use \(\vecttt {1}{2}{3}\) along with \(\vecttt {1}{0}{0}\) and \(\vecttt {0}{1}{0}\) as basis for \(\FR ^3\)?
</p> 
</div> 
</div>
<!--l. 318--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">3.4   </span> <a 
 id="x1-240003.4"></a>Systems of linear equations in vector form.</h4>
<!--l. 325--><p class="noindent" >Consider a system of linear equations whose associated matrix is an \(m\times n\) matrix of the form:
</p>
<div class="math-display" >
<img 
src="F17ZD_main51x.svg" alt="             a11  a12  ...  a1n
             a21  a22  ...  a2n                           |   |
A  :=  ∖left(  .    .          .  ∖right) = ∖left( col1(A ) |⋅⋅⋅|coln(A ) ∖right ).
              ..    ..          ..
             am1  am2  ...  amn
" class="math-display"  /></div>
<!--l. 333--><p class="noindent" >For \(\xv = \vectttt {x_1}{x_2}{\vdots }{x_n} \) a vector of \(\FR ^n\), we define :
</p>
<div class="math-display" >
<img 
src="F17ZD_main52x.svg" alt="                                      n
A ⃗x := x1col1(A ) + ...+ xncoln(A ) ∈ ℝ .
" class="math-display"  /></div>
<!--l. 335--><p class="noindent" >In other words, for \(1 \leq i \leq m\), the \(i\)-th component of \(A\xv \) is \((A\xv )_i = \sum _{1 \leq k \leq n} a_{ik}x_k.\)
</p><!--l. 341--><p class="noindent" >Consider the following system of linear equations: \begin {equation*}  \begin {aligned} \left \{ \begin {array}{ccc} a_{11}x_1+a_{12}x_2+\ldots +a_{1n}x_n&amp; = &amp; b_1\\ a_{21}x_1+a_{22}x_2+\ldots +a_{2n}x_n&amp; = &amp; b_2\\ \vdots &amp; &amp; \vdots \\ a_{m1}x_1+a_{m2}x_2+\ldots +a_{mn}x_n&amp; = &amp; b_m \end {array}\right . \end {aligned}  \end {equation*}
</p><!--l. 367--><p class="noindent" >Let \(A\) be the matrix associated to this system, and let \(\yv = \vecttt {y_1}{\vdots }{y_n}\in \FR ^n.\) A vector \(\xv =\vecttt {x_1}{\vdots }{x_n}\) is solution of that system if and only we
have:
</p>
<div class="math-display" >
<img 
src="F17ZD_main53x.svg" alt="A ⃗x = ⃗y.
" class="math-display"  /></div>
<!--l. 370--><p class="noindent" >Solving that system is thus equivalent to the following problem: </p>
<div class="center" 
>
<!--l. 371--><p class="noindent" >
</p><!--l. 372--><p class="noindent" ><span 
class="rm-lmsso-12">Is</span> \(\yv \in \FR ^n\) <span 
class="rm-lmsso-12">a linear combination of</span> \(\mathrm {col}_1(A), \ldots , \mathrm {col}_n(A)\)<span 
class="rm-lmsso-12">?</span></p></div>
                                                                                          
                                                                                          
<!--l. 377--><p class="noindent" >Thus, a \(m \times n\) matrix is not ‘just a bunch of numbers’, but we can use it to associate to a vector \(\xv \in \FR ^n\) a new
vector \(A\xv \in \FR ^m\). In other words, we can associate to any \(m \times n \) matrix a map from \(\FR ^n\) to \(\FR ^m\).
</p><!--l. 379--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">3.5   </span> <a 
 id="x1-250003.5"></a>Spans of vectors and spanning families in \(\FR ^n\)</h4>
<!--l. 381--><p class="noindent" >Given a family \(\vv _1, \ldots , \vv _k\) of vectors of \(\FR ^n\), we start by considering the questions: What vectors of \(\FR ^n\) can be obtained as
a linear combination of \(\vv _1, \ldots , \vv _k\)? We first introduce some definition:
</p>
<div class="tcolorbox df" id="tcolobox-122">    
<div class="tcolorbox-title">
<!--l. 386--><p class="noindent" >Definition 3.13  </p></div> 
<div class="tcolorbox-content"><!--l. 383--><p class="noindent" >The  <span 
class="rm-lmsso-12">span </span>of a family \(\vv _1, \ldots , \vv _k\) of vectors of \(\FR ^n\) is the set span\((\vv _1, \ldots , \vv _k)\) of vectors of \(\FR ^n\) that can be written as a
linear combination of \(\vv _1, \ldots , \vv _k\). In other words,
</p>
<div class="math-display" >
<img 
src="F17ZD_main54x.svg" alt="                    k
span (⃗v ,...,⃗v ) = {∑  λ ⃗v  | λ ∈ ℝ for every i}.
      1      k     i=1 i i   i
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 389--><p class="noindent" >Checking whether a given vector is a linear combination of a family of vectors is checked by solving a
system of linear equations. Here is an example:
</p>
<!--l. 425--><p class="noindent" >
</p><!--l. 425--><p class="noindent" ><a 
 id="x1-25002r14"></a><!--l. 391--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 3.14</span>                                                                          </p><!--l. 391--><p class="noindent" >Let us determine whether the vector  \((3,-4,2)^T\) is a linear combination of  \((1,0,2)^T\) and  \((1,1,3)^T\). We have to solve the equation
\begin {equation*}  \lambda \vecttt {1}{0}{2}+\kappa \vecttt {1}{1}{3}= \vecttt {3}{-4}{2}  \end {equation*}
with variables \(\lambda , \kappa \in \FR \). Using coordinates, we express this as a system of linear equations, and perform
Gaussian elimination: \begin {equation*} \relax \expandafter \ifx \csname cur:th\endcsname \relax \expandafter \:label \else \expandafter \l:bel \fi {rem} \begin {aligned} \left (\begin {array}{cc|c} 1 &amp; 1 &amp; 3 \\ 0 &amp; 1 &amp; -4 \\ 2 &amp; 3 &amp; 2 \end {array}\right ) \elt {R_3\leftrightarrow R_3-2R_1} \left (\begin {array}{cc|c} 1 &amp; 1 &amp;3 \\ 0 &amp; 1 &amp; -4 \\ 0 &amp; 1 &amp; -4 \end {array}\right ) \elt {R_3\rightarrow R_3-R_2} \left (\begin {array}{cc|c} 1 &amp; 1 &amp;3 \\ 0 &amp; 1 &amp; -4 \\ 0 &amp; 0 &amp; 0 \end {array}\right ) \end {aligned}  \end {equation*}
From this echelon form, we see that the system has exactly one solution (all variables are pivot
variables and no inconsistent line). We solve the resulting system by substitution, which yields \(\kappa = -4\) and \(\lambda = 3 - \kappa = 7\).
Thus, \begin {equation*}  7 \vecttt {1}{0}{2} -4 \vecttt {1}{1}{3}= \vecttt {3}{-4}{2}.  \end {equation*}                                                                                  
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 425--><p class="noindent" ></p> 
</div> 
</div>                                                                                        
</p>
<div class="tcolorbox exercise" id="tcolobox-123">    
<div class="tcolorbox-title">
<!--l. 434--><p class="noindent" >Exercise 3.15  Linear combination test in \(\FR ^3\)</p></div> 
<div class="tcolorbox-content"><!--l. 428--><p class="noindent" >Determine whether the vector \(\vecttt {2}{5}{1}\) is a linear combination of \(\vecttt {1}{1}{0}\) and \(\vecttt {0}{2}{1}\). If it is, find scalars \(\lambda ,\kappa \in \FR \) such
that \[ \lambda \vecttt {1}{1}{0}+\kappa \vecttt {0}{2}{1}=\vecttt {2}{5}{1}. \] </p> 
</div> 
</div>
<!--l. 482--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-124">    
<div class="tcolorbox-title">
<!--l. 482--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 437--><p class="noindent" >We must solve \[ \lambda \vecttt {1}{1}{0}+\kappa \vecttt {0}{2}{1}=\vecttt {2}{5}{1}. \] In coordinates this becomes the linear system \[ \begin {cases} \lambda \;=\; 2,\\ \lambda +2\kappa \;=\; 5,\\ \kappa \;=\; 1. \end {cases} \] which is inconsistent.<br 
class="newline" />
</p><!--l. 451--><p class="noindent" >Equivalently, we row-reduce the augmented matrix: \[ \begin {aligned} \left (\begin {array}{cc|c} 1 &amp; 0 &amp; 2\\ 1 &amp; 2 &amp; 5\\ 0 &amp; 1 &amp; 1 \end {array}\right ) \elt {R_2\rightarrow R_2-R_1} \left (\begin {array}{cc|c} 1 &amp; 0 &amp; 2\\ 0 &amp; 2 &amp; 3\\ 0 &amp; 1 &amp; 1 \end {array}\right ) \elt {R_2\leftrightarrow R_3} \left (\begin {array}{cc|c} 1 &amp; 0 &amp; 2\\ 0 &amp; 1 &amp; 1\\ 0 &amp; 2 &amp; 3 \end {array}\right ) \elt {R_3\rightarrow R_3-2R_2} \left (\begin {array}{cc|c} 1 &amp; 0 &amp; 2\\ 0 &amp; 1 &amp; 1\\ 0 &amp; 0 &amp; 1 \end {array}\right ). \end {aligned} \] The last row represents \(0\lambda +0\kappa =1\), which is
impossible. Hence the system has no solution, so \(\vecttt {2}{5}{1}\) is <span 
class="rm-lmsso-12">not </span>a linear combination of \(\vecttt {1}{1}{0}\) and \(\vecttt {0}{2}{1}\). </p> 
</div> 
</div>  
<!--l. 487--><p class="noindent" >Since checking whether a given vector \(\vv \) is a linear combination of vectors \(\vv _1, \ldots , \vv _k\) is checked by
solving a system of linear equations, it follows that there is either no way to write \(\vv \) as a linear
combination \(\vv _1, \ldots , \vv _k\) (precisely when \(\vv \) is not in the span of \(\vv _1, \ldots , \vv _k\)), exactly one way or infinitely many
ways.
</p>
<div class="tcolorbox thm" id="tcolobox-125">    
<div class="tcolorbox-title">
<!--l. 501--><p class="noindent" >Theorem 3.16  </p></div> 
<div class="tcolorbox-content"><!--l. 492--><p class="noindent" >Let \(\uv _1, \ldots , \uv _k\) be a family of vectors of \(\FR ^n\) and consider \(V \coloneqq \mathrm {span}(\uv _1, \ldots , \uv _k)\). We have the following: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 494--><p class="noindent" >\(\nv \in V\),
        </p></li>
        <li class="itemize">
        <!--l. 495--><p class="noindent" >For every \(\xv , \yv \in V\), we also have \(\xv + \yv \in V\).
        </p></li>
        <li class="itemize">
        <!--l. 496--><p class="noindent" >For every \(\xv \in V\) and \(\lambda \in \FR \), we also have \(\lambda \xv \in V\).</p></li></ul>
<!--l. 500--><p class="noindent" >More generally, any linear combination of vectors of \(V\) is again in \(V\). </p> 
</div> 
</div>
<div class="tcolorbox df" id="tcolobox-126">    
<div class="tcolorbox-title">
<!--l. 505--><p class="noindent" >Definition 3.17  </p></div> 
<div class="tcolorbox-content"><!--l. 504--><p class="noindent" >A subset of \(\FR ^n\) containing the null vector and stable under linear combinations is called a
<span 
class="rm-lmsso-12">subspace </span>of \(\FR ^n\). </p> 
</div> 
</div>
<!--l. 509--><p class="noindent" >The algebraic properties of spans of vectors mentioned above have a geometric counterpart: Spans of
vectors, seen as subsets of \(\FR ^n\), have a very simple shape: line or plane through the origin in \(\FR ^3\)
                                                                                          
                                                                                          
for instance. More complicated shapes, such as spheres, hyperboloids, etc. can never be
spans.
</p><!--l. 512--><p class="noindent" >As an illustration, we now list the various possibilities for the span of two vectors of \(\FR ^2\). In particular, we
see that such spans are geometrically very simple.
</p>
<div class="tcolorbox thm" id="tcolobox-127">    
<div class="tcolorbox-title">
<!--l. 523--><p class="noindent" >Theorem 3.18
</p>
</div> 
<div class="tcolorbox-content"><!--l. 516--><p class="noindent" >The span of two vectors of \(\FR ^2\) is either: </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 518--><p class="noindent" >
      </p></li>
      <li class="itemize">
      <!--l. 519--><p class="noindent" >\(\{\nv \}\),
      </p></li>
      <li class="itemize">
      <!--l. 520--><p class="noindent" >a straight line through the origin,
      </p></li>
      <li class="itemize">
      <!--l. 521--><p class="noindent" >all of \(\FR ^2\).</p></li></ul>
 
</div> 
</div>
<div class="proof">
<!--l. 524--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>Consider two arbitrary vectors \(\vv = (v_1, v_2)^T, \wv = (w_1, w_2)^T\). If \(\vv =\wv =\nv \), the span of \(\vv \) and \(\wv \) is just \(\{\nv \}\). If they are collinear and for
instance \(\vv \) is non-vanishing, then the span of \(\vv \) and \(\wv \) is the straight line parallel to \(\vv \) going through
the origin.
</p><!--l. 526--><p class="noindent" >If they are not collinear, then we now show that \(\vv \) and \(\wv \) span \(\FR ^2\). Let \(\uv =(a, b)^T\) be an arbitrary vector of \(\FR ^2\). We want to
write \(\uv \) as a linear combination of \(\vv \) and \(\wv \). In other words, we want to find numbers \(\lambda , \mu \in \FR \) such that \(\lambda \vv + \mu \wv = \uv \). By taking
coordinates, this yields the following equations: \begin {equation*}  \begin {cases} \lambda v_1+\mu w_1 &amp;=u_1\\ \lambda v_2+\mu w_2 &amp;=u_2 \end {cases}  \end {equation*}
Here, the variables are \(\lambda \) and \(\mu \), and the coefficients \(u_1, u_2, v_1, v_2, w_1, w_2\) are constants. We thus have a system of linear
equations. We perform the following row operations:
</p><!--l. 543--><p class="noindent" >\begin {equation*} \relax \expandafter \ifx \csname cur:th\endcsname \relax \expandafter \:label \else \expandafter \l:bel \fi {rem} \begin {aligned} \melt {R_1\rightarrow w_2R_1-w_1R_2\\ R_2 \rightarrow v_1R_2 - v_2R_1} \begin {cases} (v_1w_2-v_2w_1)\lambda &amp;= u_1w_2-u_2w_1 \\ (v_2w_1-v_1w_2)\mu &amp;= u_2v_1-u_1v_2 \end {cases} \end {aligned}  \end {equation*}
Since \(\vv \) and \(\wv \) are not collinear, we have \(v_1w_2-v_2w_1 \neq 0\), so we can find solutions \(\lambda \) and \(\mu \) in terms of the other constants.
Thus, there is a solution to our system of equations, so \(\uv \) is a linear combination of \(\vv \) and
                                                                                          
                                                                                          
\(\wv \).                                                                                                                      □
</p>
</div>
<!--l. 551--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">3.6   </span> <a 
 id="x1-260003.6"></a>Spanning families.</h4>
<div class="tcolorbox df" id="tcolobox-128">    
<div class="tcolorbox-title">
<!--l. 555--><p class="noindent" >Definition 3.19  </p></div> 
<div class="tcolorbox-content"><!--l. 554--><p class="noindent" >We say that a family \(\vv _1, \ldots , \vv _k\) of vectors of \(\FR ^n\) <span 
class="rm-lmsso-12">spans</span> \(\FR ^n\) (or is a <span 
class="rm-lmsso-12">spanning family </span>of \(\FR ^n\), or that \(\FR ^n\) is <span 
class="rm-lmsso-12">spanned</span>
<span 
class="rm-lmsso-12">by</span> \(\vv _1, \ldots , \vv _k\)), if \(span (\vv _1, \ldots , \vv _k) = \FR ^n\), that is, if every vector of \(\FR ^n\) can be written as a linear combination of \(\vv _1, \ldots , \vv _k\). </p> 
</div> 
</div>
<div class="tcolorbox thm" id="tcolobox-129">    
<div class="tcolorbox-title">
<!--l. 559--><p class="noindent" >Theorem 3.20  </p></div> 
<div class="tcolorbox-content"><!--l. 558--><p class="noindent" >Let \(\vv _1, \ldots , \vv _k\) be a spanning family of \(\FR ^n\). Then \(k \geq n\). In other words, a spanning family of \(\FR ^n\) contains at least
\(n\) vectors. </p> 
</div> 
</div>
<div class="proof">
<!--l. 561--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>Given a vector \(\bv = (b_1, \ldots , b_n)^T\) of \(\FR ^n\), we have to consider the equation \begin {equation*}  \lambda _1\vv _1 + \cdots + \lambda _k\vv _k = \bv ,  \end {equation*}
where \(\lambda _1, \ldots , \lambda _k\) are variables. Taking coordinates gives us a system of \(n\) equations (one for each coordinate
of \(\FR ^n\)) with \(k\) variables. We can now perform Gaussian elimination to get a system in echelon
form. We will now show that each row contains a pivot, which will be enough to conclude
that \(k \geq n\): Since each pivot must be strictly to the right of the pivots of the previous rows, this
means that there must be at least as many columns as rows in the associated matrix, hence
\(k \geq n\).
</p><!--l. 567--><p class="noindent" >If there was a row without a pivot, then the last row of the augmented matrix the echelon form would be
of the form \begin {equation*}  \begin {aligned} \left (\begin {array}{ccc|c} 0 &amp; \cdots &amp; 0 &amp; \beta \end {array}\right ), \end {aligned}  \end {equation*}
where \(\beta \) is a non-trivial linear combination of \(b_1, \ldots , b_n\). Note that if \(\beta \neq 0\), then we have a forbidden row, and the
system has no solution. We can now choose specific values of \(b_1, \ldots , b_n\) such that \(\beta \neq 0\). This corresponds to an
equation of the form \begin {equation*}  \lambda _1\vv _1 + \cdots + \lambda _k\vv _k = \vecttt {b_1}{\vdots }{b_n}  \end {equation*}
which also has no solution. Thus, the vector \(\bv =(b_1, \ldots , b_n)^T\) is not a linear combination of \(\vv _1, \ldots , \vv _n\), hence these vectors do not
form a spanning family.                                                                                          □
                                                                                          
                                                                                          
</p>
</div>
<!--l. 609--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">3.7   </span> <a 
 id="x1-270003.7"></a>Linearly independent families in \(\FR ^n\)</h4>
<!--l. 626--><p class="noindent" >We now study family of vectors \(\vv _1, \ldots , \vv _k \in \FR ^n \) for which there is a unique way to write a given in the span as a linear
combination of \(\vv _1, \ldots , \vv _k\).
</p>
<div class="tcolorbox df" id="tcolobox-130">    
<div class="tcolorbox-title">
<!--l. 634--><p class="noindent" >Definition 3.21  </p></div> 
<div class="tcolorbox-content"><!--l. 629--><p class="noindent" >A family of vectors of \(\FR ^n\) is <span 
class="rm-lmsso-12">linearly dependent </span>if one of the vectors is a linear combination
of the other vectors. Equivalently, a family of vectors \(\vv _1, \ldots , \vv _k\) is linearly dependent if there exist
scalars \(\lambda _1, \lambda _k\in \FR \) not all zero such that
</p>
<div class="math-display" >
<img 
src="F17ZD_main55x.svg" alt="λ1⃗v1 + ...+ λk⃗vk = ⃗0.
" class="math-display"  /></div>
<!--l. 632--><p class="noindent" >A family of vectors \(\vv _1, \ldots , \vv _k\) is <span 
class="rm-lmsso-12">linearly independent </span>if for every \(\lambda _1, \lambda _k\in \FR \), we have
</p>
<div class="math-display" >
<img 
src="F17ZD_main56x.svg" alt="λ1⃗v1 + ...+ λk⃗vk = ⃗0   =⇒   λ1 =  ⋅⋅⋅ = λk = 0.
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 643--><p class="noindent" >A family of vectors \(\vv _1, \ldots , \vv _k\) is linearly independent if and only if there is a unique way to write any vector of the
                                                                                          
                                                                                          
span as a linear combination of \(\vv _1, \ldots , \vv _k\).
</p><!--l. 675--><p class="noindent" >Determining whether vectors are linearly independent amounts to solving a system of linear equations,
which we do using Gaussian elimination.
</p>
<div class="tcolorbox thm" id="tcolobox-131">    
<div class="tcolorbox-title">
<!--l. 683--><p class="noindent" >Theorem 3.22  </p></div> 
<div class="tcolorbox-content"><!--l. 678--><p class="noindent" >To determine whether vectors \(\vv _1,\ldots ,\vv _k\) are linearly independent, one uses Gaussian elimination to find all
solutions to the system \begin {equation*}  c_1\vv _1+c_2\vv _2+\ldots +c_k\vv _k=\nv ~.  \end {equation*}
If all variables \(c_1,\ldots , c_k\) are pivot variables, this homogeneous system of linear equations has only the
trivial solution \(c_1=c_2=\ldots =c_k=0\). In this case, \(\vv _1,\ldots \vv _k\) are linearly independent. Otherwise, nontrivial solutions exist and
the vectors are linearly dependent. </p> 
</div> 
</div>
<!--l. 686--><p class="noindent" >Let us consider some examples:
</p>
<!--l. 736--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-132">    
<div class="tcolorbox-title">
<!--l. 736--><p class="noindent" >Example 3.23</p></div> 
<div class="tcolorbox-content"></p><!--l. 688--><p class="noindent" >We want to determine whether the vectors \((2,1,1)^T\), \((1,2,1)^T\), and \((1,1,2)^T\) are linearly independent. We need to study the
equation \begin {equation*}  \lambda \vecttt {2}{1}{1}+\kappa \vecttt {1}{2}{1}+\mu \vecttt {1}{1}{2}= \vecttt {0}{0}{0}  \end {equation*}
with variables \(\lambda , \kappa , \mu \in \FR \). Taking coordinates, this leads to the system of linear equations: \begin {equation*}  \begin {cases} 2\lambda + \kappa + \mu = 0\\ \lambda + 2\kappa + \mu = 0\\ \lambda + \kappa + 2\mu = 0 \end {cases}  \end {equation*}
We use Gaussian elimination to solve this system. We get \begin {equation*} \relax \expandafter \ifx \csname cur:th\endcsname \relax \expandafter \:label \else \expandafter \l:bel \fi {rem} \begin {aligned} \left (\begin {array}{ccc|c} 2 &amp; 1 &amp; 1 &amp; 0 \\ 1 &amp; 2 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 2 &amp; 0 \end {array}\right ) \elt {R_1\leftrightarrow R_2} \left (\begin {array}{ccc|c} 1 &amp; 2 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 1 &amp; 0 \\ 1 &amp; 1 &amp; 2 &amp; 0 \end {array}\right ) \end {aligned}  \end {equation*}
\begin {equation*}  \begin {aligned} \melt {R_2\rightarrow R_2-2R_1\\R_3\rightarrow -R_3+R_1} \left (\begin {array}{ccc|c} 1 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; -3 &amp; -1 &amp; 0 \\ 0 &amp; 1 &amp; -1 &amp; 0 \end {array}\right ) \elt {R_3\leftrightarrow 3R_3+R_1} \left (\begin {array}{ccc|c} 1 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; -3 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; -4 &amp; 0 \end {array}\right ) \end {aligned}  \end {equation*}
Thus, all the variables are pivot variables and the system is consistent as it is homogeneous, so the
system has exactly one solution. Since the system has at least one solution, namely \(\lambda = \kappa =\mu =0\), this
must be the only solution. As a consequence, the vectors \((2,1,1)^T\), \((1,2,1)^T\), and \((1,1,2)^T\) are linearly independent.
</p> 
</div> 
</div>                                                                                                                      
<div class="tcolorbox exercise" id="tcolobox-133">    
<div class="tcolorbox-title">
<!--l. 741--><p class="noindent" >Exercise 3.24  Linearly dependent vectors</p></div> 
<div class="tcolorbox-content"><!--l. 740--><p class="noindent" >Determine whether the vectors \((1,2,3,-1)^T\), \((2,1,3,1)^T\) and \((4,5,9,-1)^T\) are linearly dependent. </p> 
</div> 
</div>
<!--l. 801--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-134">    
<div class="tcolorbox-title">
<!--l. 801--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 744--><p class="noindent" >We must find all solutions \((c_1,c_2,c_3)\) of the homogeneous system: \begin {equation*}  c_1\left (\begin {array}{c} 1 \\ 2 \\ 3 \\ -1 \end {array}\right )+ c_2\left (\begin {array}{c} 2 \\ 1 \\ 3 \\ 1 \end {array}\right )+ c_3\left (\begin {array}{c} 4 \\ 5 \\ 9 \\ -1 \end {array}\right )= \left (\begin {array}{c} 0 \\ 0 \\ 0 \\ 0 \end {array}\right )~.  \end {equation*}
We reduce the associated system using Gaussian elimination: \begin {equation*}  \left \{ \begin {array}{ccccl} c_1&amp;+2c_2&amp;+4c_3&amp;=&amp;0\\ 2c_1&amp;+c_2&amp;+5c_3&amp;=&amp;0\\ 3c_1&amp;+3c_2&amp;+9c_3&amp;=&amp;0\\ -c_1&amp;+c_2&amp;-c_3&amp;=&amp;0\\ \end {array}\right .~~\rightsquigarrow ~~ \left (\begin {array}{ccc|c}1 &amp; 2 &amp; 4 &amp; 0 \\ 2 &amp; 1 &amp; 5 &amp; 0 \\ 3 &amp; 3 &amp; 9 &amp; 0 \\ -1 &amp; 1 &amp; -1 &amp; 0\end {array}\right )  \end {equation*}
\begin {equation*}  \melt {R_2\rightarrow R_2-2R_1\\R_3\rightarrow R_3-3R_1\\R_4\rightarrow R_4+R_1} \left (\begin {array}{ccc|c}1 &amp; 2 &amp; 4 &amp; 0 \\ 0 &amp; -3 &amp; -3 &amp; 0 \\ 0 &amp; -3 &amp; -3 &amp; 0 \\ 0 &amp; 3 &amp; 3 &amp; 0\end {array}\right ) \melt {R_3\rightarrow R_3-R_2\\R_4\rightarrow R_4+R_3\\R_2\rightarrow -\frac {1}{3}R_2} \left (\begin {array}{ccc|c}1 &amp; 2 &amp; 4 &amp; 0 \\ 0 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0\end {array}\right )~.  \end {equation*}
Not all the variables are pivot variables (\(c_3\) is a free variable) and therefore the system has
infinitely many solutions, which implies that the vectors are linearly dependent. More precisely, if
we rewrite it as a system of linear equations, we get \begin {equation*}  \left \{\begin {array}{rcl} c_1+2c_2+4c_3&amp;=&amp;0\\ c_2+c_3&amp;=&amp;0\\ \end {array}\right .~~~ \begin {array}{l} \rightarrow c_1=-2c_2-4c_3=-2\alpha \\ \rightarrow c_3=\alpha \Rightarrow c_2=-\alpha \\ \end {array}~~~.  \end {equation*}
For instance, by setting \(\alpha =1\) we obtain \begin {equation*}  -2\left (\begin {array}{c} 1 \\ 2 \\ 3 \\ -1 \end {array}\right )- \left (\begin {array}{c} 2 \\ 1 \\ 3 \\ 1 \end {array}\right )+ \left (\begin {array}{c} 4 \\ 5 \\ 9 \\ -1 \end {array}\right )= \left (\begin {array}{c} 0 \\ 0 \\ 0 \\ 0 \end {array}\right )~.  \end {equation*}
</p> 
</div> 
</div>                                                                                                               
                                                                                          
                                                                                          
<!--l. 805--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">3.8   </span> <a 
 id="x1-280003.8"></a>Bases of \(\FR ^n\).</h4>
<!--l. 806--><p class="noindent" >We now study families of vectors such that <span 
class="rm-lmsso-12">every </span>vector of \(\FR ^n\) can be written <span 
class="rm-lmsso-12">in a unique way </span>as a linear
combination of \(\vv _1, \ldots , \vv _k\). We have already seen one example of such a family, namely the standard basis of \(\FR ^n\):
\begin {equation*}  \ev _1 = \vectttt {1}{0}{\vdots }{0}, \ev _2 = \vectttt {0}{1}{0}{\vdots }, \ldots , \ev _n = \vectttt {0}{\vdots }{0}{1}.  \end {equation*}
This family of vectors is generally referred to as the <span 
class="rm-lmsso-12">standard basis </span>of \(\FR ^n\). This notion can be generalised as
follows:
</p>
<div class="tcolorbox df" id="tcolobox-135">    
<div class="tcolorbox-title">
<!--l. 820--><p class="noindent" >Definition 3.25  </p></div> 
<div class="tcolorbox-content"><!--l. 819--><p class="noindent" >A <span 
class="rm-lmsso-12">basis </span>of \(\FR ^n\) is a family of vectors \(\vv _1, \ldots , \vv _k\) such that every vector of \(\FR ^n\) can be written in a unique way
as a linear combination of \(\vv _1, \ldots , \vv _k\). In other words, a family of vectors is a basis if and only if it is
both spanning (existence of a linear combination) and linearly independent (uniqueness of
a linear combination). </p> 
</div> 
</div>
<div class="tcolorbox thm" id="tcolobox-136">    
<div class="tcolorbox-title">
<!--l. 826--><p class="noindent" >Theorem 3.26  A</p></div> 
<div class="tcolorbox-content"><!--l. 825--><p class="noindent" >basis of \(\FR ^n\) contains exactly \(n\) vectors.                                                                    □</p> 
</div> 
</div>
<!--l. 830--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">3.9   </span> <a 
 id="x1-290003.9"></a>Characterisation of bases of \(\FR ^n\)</h4>
<div class="tcolorbox thm" id="tcolobox-137">    
<div class="tcolorbox-title">
<!--l. 839--><p class="noindent" >Theorem 3.27  </p></div> 
<div class="tcolorbox-content"><!--l. 833--><p class="noindent" >Let \(\uv _1, \ldots , \uv _n\) be a family of <span 
class="rm-lmssbx-10x-x-120">exactly</span> \(n\) vectors of \(\FR ^n\). The following are equivalent: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 835--><p class="noindent" >\(\uv _1, \ldots , \uv _n\) is a basis of \(\FR ^n\).
        </p></li>
        <li class="itemize">
        <!--l. 836--><p class="noindent" >\(\uv _1, \ldots , \uv _n\) spans \(\FR ^n\).
        </p></li>
        <li class="itemize">
        <!--l. 837--><p class="noindent" >\(\uv _1, \ldots , \uv _n\) is linearly independent.</p></li></ul>
 
</div> 
</div>
                                                                                          
                                                                                          
<!--l. 842--><p class="noindent" >Note that \((i) \Rightarrow (ii), (iii)\). Let us show that \((ii) \Rightarrow (i)\). We want to show that \(\uv _1, \ldots , \uv _n\) is linearly independent. To that end, we try to
solve the homogeneous system of equations \(\lambda _1 \uv _1 + \ldots + \lambda _n\uv _n = \nv \) by Gaussian elimination. Since we have \(n\) equations with \(n\)
variables, either every variable is a pivot variable, or there exists a free variable. In the latter case, we can
find a vector \(\uv \) of \(\FR ^n\) such that reducing the system of equations \(\lambda _1\vv _1 + \ldots + \lambda _n \vv _n = \uv \) would give a bottom row of the
form
</p>
<div class="math-display" >
<img 
src="F17ZD_main57x.svg" alt="                 |
∖left( 0  ...  0 |1 ∖right).
" class="math-display"  /></div>
<!--l. 846--><p class="noindent" >This would imply that the system associated to the equation \(\lambda _1\uv _1 + \ldots + \lambda _n \uv _n = \uv \) is inconsistent, contradicting the fact that
the family spans \(\FR ^n\). Thus, every variable is a pivot variable, and we know the system has exactly one
solution: \((\lambda _1, \ldots , \lambda _n)=(0, \ldots , 0)\).
</p><!--l. 848--><p class="noindent" >Let us now show that \((iii) \Rightarrow (i)\), using a similar strategy. We want to show that \(\uv _1, \ldots , \uv _n\) spans \(\FR ^n\). To that end, we try to
solve the system of equations \(\lambda _1 \uv _1 + \ldots + \lambda _n\uv _n = \uv \), for some vector \(\uv \) of \(\FR ^n\), by Gaussian elimination. Since we have \(n\) equations
with \(n\) variables, either every variable is a pivot variable, or there exists a free variable. In the latter case,
the system of equations \(\lambda _1\vv _1 + \ldots + \lambda _n \vv _n = \nv \) would have infinitely many solutions, contradicting the fact that the family is
linearly independent. Thus, every variable is a pivot variable, which implies that the system of equations \(\lambda _1\uv _1 + \ldots + \lambda _n \uv _n = \uv \)
admits at least one solution.
</p><!--l. 851--><p class="noindent" >The implication \((iii) \Rightarrow (i)\) gives us a possible method to show that a given set of vectors forms a basis of \(\FR ^n\).
However, there is often a faster way to show that a given family of vectors forms a basis of
\(\FR ^n\):
</p>
<div class="tcolorbox thm" id="tcolobox-138">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 864--><p class="noindent" >Theorem 3.28  </p></div> 
<div class="tcolorbox-content"><!--l. 858--><p class="noindent" >Let \(\vv _1, \ldots , \vv _n \) be a family of \(n\) vectors of \(\FR ^n\), and let
</p>
<div class="math-display" >
<img 
src="F17ZD_main58x.svg" alt="               |   |   |
A := ∖left( ⃗v1 |⃗v2 |⋅⋅⋅|⃗vn ∖right )
" class="math-display"  /></div>
<!--l. 862--><p class="noindent" >be the associated matrix. Then:
</p>
<div class="math-display" >
<img 
src="F17ZD_main59x.svg" alt="⃗v ,...,⃗v is a basis of ℝn ⇐ ⇒  det(A ) ⁄= 0.
  1      n
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 866--><p class="noindent" >This works because the determinant is calculating a <span 
class="rm-lmsso-12">volume </span>(or area in \(2\) dimensions) defined by the
vectors.
</p><!--l. 868--><p class="noindent" >In \(\FR ^2\) the determinant gives the area of the parallelogram defined by \(\av , \bv \). If \(\bv \) is collinear with \(\av \) the area is zero.
</p>
<div class="center" 
>
<!--l. 869--><p class="noindent" >
</p><!--l. 870--><p class="noindent" ><img 
src="F17ZD_main60x.svg" alt="⃗
⃗ab𝜃h
"  /></p></div>
                                                                                          
                                                                                          
<!--l. 882--><p class="noindent" >In \(\FR ^3\), on the other hand, three vectors \(\av , \bv , \cv \) generically define a parallelepiped, as shown below. If one of the
vectors is linearly dependent on the other two in will be coplanar with them and the parallelepiped will
collapse to have zero volume. It is harder to picture in higher dimensions, but the idea is the
same.
</p>
<div class="center" 
>
<!--l. 884--><p class="noindent" >
</p><!--l. 885--><p class="noindent" ><img 
src="F17ZD_main61x.svg" alt="⃗a⃗b⃗chϕ
"  /></p></div>
<!--l. 958--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-139">    
<div class="tcolorbox-title">
<!--l. 958--><p class="noindent" >Example 3.29</p></div> 
<div class="tcolorbox-content"></p><!--l. 909--><p class="noindent" > Using a determinant to test for a basis of \(\FR ^3\):
</p><!--l. 913--><p class="noindent" >Let \[ \vv _1=\begin {pmatrix}1\\0\\2\end {pmatrix},\qquad \vv _2=\begin {pmatrix}0\\1\\-1\end {pmatrix},\qquad \vv _3=\begin {pmatrix}3\\2\\1\end {pmatrix}. \] Form the associated matrix with these vectors as columns: \[ A:=\left (\begin {array}{c|c|c} \vv _1 &amp; \vv _2 &amp; \vv _3 \end {array}\right ) = \begin {pmatrix} 1 &amp; 0 &amp; 3\\ 0 &amp; 1 &amp; 2\\ 2 &amp; -1 &amp; 1 \end {pmatrix}. \] Compute its determinant
(expand along the first row, for example): \[ \det (A) = 1\cdot \det \begin {pmatrix}1&amp;2\\-1&amp;1\end {pmatrix} -0\cdot (\cdots ) +3\cdot \det \begin {pmatrix}0&amp;1\\2&amp;-1\end {pmatrix}. \] Hence \[ \det (A) = \bigl (1\cdot 1 - 2\cdot (-1)\bigr ) +3\bigl (0\cdot (-1)-1\cdot 2\bigr ) = (1+2)+3( -2) = 3-6 = -3\neq 0. \] Therefore, by the theorem, \[ \vv _1,\vv _2,\vv _3 \text { is a basis of } \FR ^3. \] </p> 
</div> 
</div>                  
<!--l. 960--><p class="noindent" >We can also use the determinant test when the vectors do <span 
class="rm-lmsso-12">not </span>form a basis:
</p>
<!--l. 1012--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-140">    
<div class="tcolorbox-title">
<!--l. 1012--><p class="noindent" >Example 3.30</p></div> 
<div class="tcolorbox-content"></p><!--l. 962--><p class="noindent" > Using a determinant to test for a basis of \(\FR ^3\) (an example where the vectors don’t form a basis).
</p><!--l. 965--><p class="noindent" >Let \[ \vv _1=\begin {pmatrix}1\\0\\2\end {pmatrix},\qquad \vv _2=\begin {pmatrix}0\\1\\-1\end {pmatrix},\qquad \vv _3=\begin {pmatrix}1\\1\\1\end {pmatrix}. \] (Notice that \(\vv _3=\vv _1+\vv _2\).)
</p><!--l. 973--><p class="noindent" >Form the associated matrix with these vectors as columns: \[ A:=\left (\begin {array}{c|c|c} \vv _1 &amp; \vv _2 &amp; \vv _3 \end {array}\right ) = \begin {pmatrix} 1 &amp; 0 &amp; 1\\ 0 &amp; 1 &amp; 1\\ 2 &amp; -1 &amp; 1 \end {pmatrix}. \] Compute its determinant (expand
along the first row, for example): \[ \det (A) = 1\cdot \det \begin {pmatrix}1&amp;1\\-1&amp;1\end {pmatrix} -0\cdot (\cdots ) +1\cdot \det \begin {pmatrix}0&amp;1\\2&amp;-1\end {pmatrix}. \] Hence \[ \det (A) = \bigl (1\cdot 1 - 1\cdot (-1)\bigr ) +\bigl (0\cdot (-1)-1\cdot 2\bigr ) = (1+1)+(-2) = 0. \] Therefore, by the theorem, \[ \vv _1,\vv _2,\vv _3 \text { do not form a basis of } \FR ^3. \] (In fact they are linearly
dependent since \(\vv _3=\vv _1+\vv _2\).)
</p>
 
</div> 
</div>                                                                                                                      
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
<h3 class="sectionHead"><span class="titlemark">4   </span> <a 
 id="x1-300004"></a>Vector Spaces in General</h3>
<!--l. 4--><p class="noindent" >In the previous chapter, we introduced the space \(\FR ^n\) of <span 
class="rm-lmsso-12">vectors</span>. The key operations for vectors in \(\FR ^n\) are
<span 
class="rm-lmsso-12">adding </span>two vectors and <span 
class="rm-lmsso-12">multiplying </span>a vector by a scalar. In this section, we will develop the
general abstract framework allowing us to treat various examples of vector spaces in a uniform
way.
</p><!--l. 7--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">4.1   </span> <a 
 id="x1-310004.1"></a>Examples and definition</h4>
<!--l. 14--><p class="noindent" >To be able to treat various cases with a single mathematical framework, we now introduce the notion of
an <span 
class="rm-lmsso-12">abstract vector space</span>. This is the notion that makes rigorous this idea of ‘spaces of objects where one
can add vectors and multiply them by a scalar’.
</p>
<div class="tcolorbox df" id="tcolobox-141">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 35--><p class="noindent" >Definition 4.1  </p></div> 
<div class="tcolorbox-content"><!--l. 20--><p class="noindent" >A <span 
class="rm-lmsso-12">real vector space </span>is a set \(V\) (whose elements are called <span 
class="rm-lmsso-12">vectors</span>) endowed with two operations:
</p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 22--><p class="noindent" >an <span 
class="rm-lmsso-12">addition </span>denoted \(+\) that associates to two vectors \(\uv ,\vv \in V\) a vector \(\uv + \vv \in V\),
        </p></li>
        <li class="itemize">
        <!--l. 23--><p class="noindent" >a <span 
class="rm-lmsso-12">multiplication by a scalar </span>denoted \(\cdot \) that associates to a vector \(\vv \in V\) and a scalar \(\lambda \in \FR \) a vector
        \(\lambda \cdot \vv \in V\) (or simply \(\lambda \vv \)).</p></li></ul>
<!--l. 25--><p class="noindent" >We further require that the following <span 
class="rm-lmsso-12">vector space axioms </span>are satisfied: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 27--><p class="noindent" ><span 
class="rm-lmsso-12">null vector: </span>There is an element \(\nv =\nv _V\in V\), the <span 
class="rm-lmsso-12">zero </span>or <span 
class="rm-lmsso-12">null vector</span>, such that \(\vv +\nv =\vv \) for all \(\vv \in V\).
        </p></li>
        <li class="itemize">
        <!--l. 28--><p class="noindent" ><span 
class="rm-lmsso-12">opposite: </span>For all \(\vv \in V\), there is an element \(-\vv \in V\) such that \(\vv +(-\vv )=\nv \).
        </p></li>
        <li class="itemize">
        <!--l. 29--><p class="noindent" ><span 
class="rm-lmsso-12">commutativity of the addition:</span> \(\vv +\wv =\wv +\vv \) for all \(\vv ,\wv \in V\),
        </p></li>
        <li class="itemize">
        <!--l. 30--><p class="noindent" ><span 
class="rm-lmsso-12">associativity of the addition:</span> \((\vv +\wv )+\uv =\vv +(\wv +\uv )\) for all \(\vv ,\wv ,\uv \in V\).
        </p></li>
        <li class="itemize">
        <!--l. 31--><p class="noindent" ><span 
class="rm-lmsso-12">distribituvity of the scalar multiplication:</span> \(a(\vv +\wv )=a\vv +a\wv \) and \((a+b)\vv =a\vv +b\vv \), \(a,b\in \FR \), \(\vv ,\wv \in V\),
        </p></li>
        <li class="itemize">
        <!--l. 32--><p class="noindent" ><span 
class="rm-lmsso-12">associativity of the scalar multiplication:</span> \(a(b\vv )=(ab)\vv \), \(a,b\in \FR \), \(\vv \in V\),
        </p></li>
        <li class="itemize">
        <!--l. 33--><p class="noindent" ><span 
class="rm-lmsso-12">compatibility:</span> \(1\cdot \vv =\vv \).</p></li></ul>
 
</div> 
</div>
<!--l. 37--><p class="noindent" >This list of axioms (1)-(7) may seem long and technical. However, you should convince yourself that
they encode the usual properties that one expects from addition and multiplication by a number for
vectors, functions, etc. Moreover, the strength of this abstract framework is that, once we know
that something is a vector space, we can treat objects of that space (functions, sequences,
or more exotic objects) as if they were vectors, and use our geometric intuition to solve
problems.
</p>
                                                                                          
                                                                                          
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 41--><p class="noindent" >The space \(\FR ^n\) is a vector space for the addition and multiplication by a scalar introduced in the
      previous chapter. The null vector \((0, \ldots , 0)^T\) satisfies axiom (1). The opposite of a vector \((v_1, \ldots , v_n)^T\) is defined as
      \((-v_1, \ldots , -v_n)^T\) to satisfy axiom (2). One then checks that axioms (3)-(7) hold. Indeed, one has to check
      the equations coordinate by coordinate, and each of these equations then boils down to a
      standard rule about addition and multiplication of real numbers (commutativity of addition,
      distributivity of the multiplication, etc.)
      </p></li>
      <li class="itemize">
      <!--l. 44--><p class="noindent" >The space \(\FR ^\FN \) is a vector space for the addition and multiplication by a scalar defined above.
      Here, a vector is a sequence
      </p>
<div class="math-display" >
<img 
src="F17ZD_main62x.svg" alt="       u0
∖left( u1 ∖right),
        ..
        .
" class="math-display"  /></div>
      <!--l. 45--><p class="noindent" >the zero vector is the <span 
class="rm-lmsso-12">zero sequence</span>
      </p>
<div class="math-display" >
<img 
src="F17ZD_main63x.svg" alt="       0
∖lef t(0 ∖right ),
        ..
        .
" class="math-display"  /></div>
      <!--l. 46--><p class="noindent" >and the opposite of a vector is given by
                                                                                          
                                                                                          
      </p>
<div class="math-display" >
<img 
src="F17ZD_main64x.svg" alt="         u0                    − u0
− ∖left( u1 ∖right) :==  ∖left( − u1 ∖right).
         ..                       ..
         .                       .
" class="math-display"  /></div>
      <!--l. 47--><p class="noindent" >Here again, one then checks that axioms (3)-(7) hold, by checking each equation pointwise.
</p>
      </li></ul>
<!--l. 52--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-320004.1"></a><span 
class="rm-lmssbx-10x-x-120">Complex vector spaces.</span></span>
We have just defined <span 
class="rm-lmsso-12">real </span>vector spaces, that is, vector spaces where the set of scalars is \(\FR \). These will be
the almost sole focus of this course. However, one can analogously define <span 
class="rm-lmsso-12">complex vectors spaces </span>by
setting the set of scalars to be \(\FC \) and having the same list of axioms. Examples of complex vectors spaces
are: </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 54--><p class="noindent" >The space \(\FC ^n\) of vectors of the form \((z_1, \ldots , z_n)^T\) with \(z_i \in \FC \) for every \(i\).
      </p></li>
      <li class="itemize">
      <!--l. 55--><p class="noindent" >The space \(\CF (\FC )\) of functions from \(\FC \) to \(\FC \).
      </p></li>
      <li class="itemize">
      <!--l. 56--><p class="noindent" >The space \(\CP (\FC )\) of polynomial functions with complex coefficients.
      </p></li>
      <li class="itemize">
      <!--l. 57--><p class="noindent" >The space \(\FC ^\FN \) of complex sequences \((z_0, z_1, \ldots )\) with \(z_i \in \FC \) for every \(i \geq 0\).</p></li></ul>
                                                                                          
                                                                                          
<!--l. 65--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">4.2   </span> <a 
 id="x1-330004.2"></a>Linear combinations in abstract vector spaces</h4>
<!--l. 67--><p class="noindent" >We now generalise to abstract vector spaces the notions we introduced in \(\FR ^n\).
</p>
<div class="tcolorbox df" id="tcolobox-142">    
<div class="tcolorbox-title">
<!--l. 76--><p class="noindent" >Definition 4.2  </p></div> 
<div class="tcolorbox-content"><!--l. 71--><p class="noindent" >A vector \(\vv \in V\) is called a <span 
class="rm-lmsso-12">linear combination </span>of the vectors \(\uv _1,\ldots ,\uv _k\in V\), if it can be written as \begin {equation*}  \vv =a_1\uv _1+a_2\uv _2+\ldots +a_k\uv _k~,~~~a_1,\ldots ,a_k\in \FR ~.  \end {equation*}
This is an extension of our definition of linear combination of vectors in \(\FR ^n\). </p> 
</div> 
</div>
<!--l. 80--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-143">    
<div class="tcolorbox-title">
<!--l. 80--><p class="noindent" >Example 4.3</p></div> 
<div class="tcolorbox-content"></p><!--l. 78--><p class="noindent" >Consider the vector space \(\CP _2(\FR )\) of polynomials of degree at most 2. Linear combinations of the two
functions (=vectors) defined \(P_1(x) = x\) and \(P_2(x)= x^2\) are polynomial functions of the form
</p>
<div class="math-display" >
<img 
src="F17ZD_main65x.svg" alt="         2
a1x + a2x ,  for a1, a2 ∈ ℝ.
" class="math-display"  /></div>
<!--l. 79--><p class="noindent" >For instance, the polynomial function defined \(P(x) = 5x^2 - 2x\) is a linear combination of \(x\) and \(x^2\), since we have \(P= 5P_2 - 2P_1\).
</p> 
</div> 
</div>                                                                                                                      
<div class="tcolorbox df" id="tcolobox-144">    
<div class="tcolorbox-title">
<!--l. 89--><p class="noindent" >Definition 4.4  </p></div> 
<div class="tcolorbox-content"><!--l. 84--><p class="noindent" >The <span 
class="rm-lmsso-12">span </span>of a family of vectors \(\uv _1,\ldots ,\uv _k\), usually denoted by \(span(\uv _1,\) \(\ldots ,\uv _k)\), is the set of all possible linear combinations
of \(\uv _1,\ldots ,\uv _k\): \begin {equation*}  span(\uv _1,\ldots ,\uv _k) :=\{a_1\uv _1+\ldots +a_k\uv _k|a_1,\ldots ,a_k\in \FR \}~.  \end {equation*}
We also say that \(span(\uv _1,\ldots ,\uv _k)\) is spanned by \(\uv _1,\ldots ,\uv _k\) or that these vectors span \(span(\uv _1,\ldots ,\uv _k)\). </p> 
</div> 
</div>
<!--l. 98--><p class="noindent" >
</p><!--l. 98--><p class="noindent" ><a 
 id="x1-33004r5"></a><!--l. 92--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 4.5</span>                                                                           </p><!--l. 92--><p class="noindent" ></p>
      <ul class="itemize1">
      <li class="itemize">                                                                                  
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 98--><p class="noindent" >      <!--l. 93--><p class="noindent" >The vectors \((1,0,0)^T\), \((0,1,0)^T\), \((0,0,1)^T\) span \(\FR ^3\).
      </p></li>
      <li class="itemize">
      <!--l. 94--><p class="noindent" >The vector space of polynomials up to degree \(n\), \(\CP _n(\FR )\), is spanned by the polynomials \(1,x,x^2,\) \(\ldots ,x^n\), as every
      polynomial function in \(\CP _n(\FR )\) can be written as \(a_0 \times 1 + \ldots + a_nx^n\).
      </p></li>
      <li class="itemize">
      <!--l. 95--><p class="noindent" >The polynomial function \(2x^2 +x -1\) belongs to span\((x^2+x, x+1)\) since \(2x^2+x-1 = 2(x^2 +x) - (x+1)\).</p></li></ul>
 
</div> 
</div>                                                                                        
</p>
<div class="tcolorbox df" id="tcolobox-145">    
<div class="tcolorbox-title">
<!--l. 108--><p class="noindent" >Definition 4.6  </p></div> 
<div class="tcolorbox-content"><!--l. 104--><p class="noindent" >We say that vectors \(\vv _1,\ldots ,\vv _k\) are <span 
class="rm-lmsso-12">linearly independent </span>if
</p>
<div class="math-display" >
<img 
src="F17ZD_main66x.svg" alt="c1⃗v1 + ...+ ck⃗vk = ⃗0   =⇒   c1 = ...=  ck = 0.
" class="math-display"  /></div>
<!--l. 106--><p class="noindent" >Otherwise, we say that the vectors are <span 
class="rm-lmsso-12">linearly dependent</span>. This is an extension of our
definition of linear independence for \(\FR ^n\). </p> 
</div> 
</div>
<!--l. 110--><p class="noindent" >As in the case of \(\FR ^n\), linear dependence as a simple interpretation: a family of vectors \(\uv _1,\ldots ,\uv _k\) is linearly dependent,
if and only if one (i.e. at least one) of the \(\uv _i\)s can be expressed as a linear combination of the
others.
</p>
<!--l. 206--><p class="noindent" >
</p><!--l. 206--><p class="noindent" ><a 
 id="x1-33006r7"></a><!--l. 197--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 4.7</span>                                                                           </p><!--l. 197--><p class="noindent" >Let us show that the functions  \(f_1(x):=\cos (x), f_2(x):=\cos (2x), f_3(x):=\cos (3x)\) are linearly independent vectors of  \(\CF (\FR )\).
</p><!--l. 200--><p class="noindent" >Let \(a, b, c\in \FR \) be scalars such that \(af_1 + bf_2 + cf_3\) is the zero vector of \(\CF (\FR )\), that is, the zero function. This is equivalent
to:                                                                                     
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 206--><p class="noindent" ></p>
<div class="math-display" >
<img 
src="F17ZD_main67x.svg" alt="a cos(x) + bcos(2x) + ccos(3x) = 0   for every  x ∈ ℝ.
" class="math-display"  /></div>
<!--l. 202--><p class="noindent" >By evaluating at \(x= \pi /2\), we get \(b=0\), hence \( a\cos (x) +c\cos (3x)=0 \) for every \(x \in \FR \). By evaluating this equation at \(x = \pi /6\), we get \(a\times \sqrt {3}/2 = 0\), hence \(a=0\).
Finally, we have \(c\cos (3x)=0\) for every \(x \in \FR \), and evaluating at \(x=0\) yields \(c=0\).
</p><!--l. 204--><p class="noindent" >We thus have \(a=b=c=0\), and it follows that \(f_1, f_2, f_3\) are linearly independent.
</p>
 
</div> 
</div>                                                                                        
</p><!--l. 293--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">4.3   </span> <a 
 id="x1-340004.3"></a>Basis and dimension</h4>
<div class="tcolorbox df" id="tcolobox-146">    
<div class="tcolorbox-title">
<!--l. 304--><p class="noindent" >Definition 4.8  </p></div> 
<div class="tcolorbox-content"><!--l. 299--><p class="noindent" >A family of vectors \(\uv _1,\ldots ,\uv _n\) is called a <span 
class="rm-lmsso-12">basis </span>of \(V\), if it is a linearly independent set of vectors that
span \(V\).
</p>
 
</div> 
</div>
<!--l. 314--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-147">    
<div class="tcolorbox-title">
<!--l. 314--><p class="noindent" >Example 4.9</p></div> 
<div class="tcolorbox-content"></p><!--l. 309--><p class="noindent" > We have already seen the standard basis \(\ev _1 = (1, 0, \ldots , 0)^T\), \(\ldots \) ,\( \ev _n = (0, \ldots , 0, 1)^T\) of \(\FR ^n\).
</p><!--l. 312--><p class="noindent" >Note that a real vector space has infinitely many bases. For instance, \((1, 1)^T\) and \((1, -1)^T\) is also a basis of \(\FR ^2\)
Why?.
</p>
 
</div> 
</div>                                                                                                                      
<!--l. 316--><p class="noindent" >Here is an important example: </p>
                                                                                          
                                                                                          
<div class="tcolorbox thm" id="tcolobox-148">    
<div class="tcolorbox-title">
<!--l. 320--><p class="noindent" >Theorem 4.10  </p></div> 
<div class="tcolorbox-content"><!--l. 319--><p class="noindent" >A basis for the vector space of polynomials of degree at most \(n\) is \(1,x,x^2,\ldots ,x^n\). </p> 
</div> 
</div>
<div class="proof">
<!--l. 323--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>The family clearly spans \(\CP _n(\FR )\), as every polynomial function of \(\CP _n(\FR )\) can be written as a linear
combination \(a_0 + a_1x + \ldots + a_n x^n\) for some scalars \(a_0, \ldots , a_n \in \FR \). Let us show that this family is free. Suppose that we have a linear
combination \(a_0 + a_1x + \ldots + a_n x^n\) that is the zero vector, that is, \(a_0 + a_1x + \ldots + a_n x^n= 0\) for every \(x\in \FR \). By evaluating at \(x=0\), we get \(a_0 = 0\). Now since
the polynomial function \(a_0 + a_1x + \ldots + a_n x^n\) is the zero function, so its derivative, so we get \(a_1 + 2a_2x + \ldots + na_nx^{n-1} = 0\) for every \(x \in \FR \). Evaluating
again at \(x=0\), we get \(a_1 = 0\). By repeating the same procedure (differentiating and evaluating at \(x=0\)), we prove
successively that \(a_0 = a_1 = \ldots = a_n = 0\). Thus, the family of polynomial functions \(1, x, \ldots , x^n\) is linearly independent, so it is a
basis of \(\CP _n(\FR )\).                                                                                                            □
</p>
</div>
<!--l. 329--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">4.4   </span> <a 
 id="x1-350004.4"></a>Dimension of a vector space.</h4>
<!--l. 330--><p class="noindent" >In order to define the dimension of a vector space, we need the following important result:
</p>
<div class="tcolorbox thm" id="tcolobox-149">    
<div class="tcolorbox-title">
<!--l. 334--><p class="noindent" >Theorem 4.11  </p></div> 
<div class="tcolorbox-content"><!--l. 333--><p class="noindent" >Any two bases for a vector space \(V\) contain the same number of vectors. </p> 
</div> 
</div>
<!--l. 336--><p class="noindent" >Before proving it, we need the following result:
</p>
<div class="tcolorbox thm" id="tcolobox-150">    
<div class="tcolorbox-title">
<!--l. 340--><p class="noindent" >Theorem 4.12  </p></div> 
<div class="tcolorbox-content"><!--l. 339--><p class="noindent" >If a family of vectors \(\vv _1,\vv _2,\ldots ,\vv _n\) is a basis of a vector space \(V\), then every family of vectors of \(V\) containing
more than \(n\) vectors is linearly dependent. </p> 
</div> 
</div>
                                                                                          
                                                                                          
<div class="proof">
<!--l. 342--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>Let \(\wv _1,\wv _2,\ldots ,\wv _m\) be a family of \(m&gt;n\) vectors of \(V\). We will show that there exist \(c_1,c_2,\ldots ,c_m\) not all zero such that
\begin {equation} \relax \expandafter \ifx \csname cur:th\endcsname \relax \expandafter \:label \else \expandafter \l:bel \fi {eq:SLE1} c_1\wv _1+c_2\wv _2+\ldots +c_m\wv _m=\nv ~.  \end {equation}
Since \(\vv _1,\vv _2,\ldots ,\vv _n\) spans \(V\), each \(\wv _i\) can be expressed as a linear combination of the \(\vv _i\)’s: \begin {equation*}  \begin {aligned} \wv _1&amp;=a_{11}\vv _1+a_{12}\vv _2+\ldots +a_{1n}\vv _n~,\\ \wv _2&amp;=a_{21}\vv _1+a_{22}\vv _2+\ldots +a_{2n}\vv _n~,\\ \vdots &amp; \hspace {2cm}\vdots \\ \wv _m&amp;=a_{m1}\vv _1+a_{m2}\vv _2+\ldots +a_{mn}\vv _n~. \end {aligned}  \end {equation*}
Plugging this into (<span 
class="rm-lmssbx-10x-x-120">??</span>), we have \begin {equation*}  c_1(a_{11}\vv _1+\ldots +a_{1n}\vv _n)+\ldots +c_m(a_{m1}\vv _1+\ldots +a_{mn}\vv _n)=\nv ~.  \end {equation*}
To have all the coefficients of the \(\vv _1,\ldots ,\vv _n\) vanish, note that it is sufficient to find \(c_1,\ldots ,c_m\) such that \begin {equation*}  \begin {aligned} a_{11}c_1+a_{21}c_2+\ldots +a_{m1}c_m &amp;=0~,\\ a_{12}c_1+a_{22}c_2+\ldots +a_{m2}c_m &amp;=0~,\\ \vdots \hspace {2cm}&amp;\vdots \\ a_{1n}c_1+a_{2n}c_2+\ldots +a_{mn}c_m &amp;=0~. \end {aligned}  \end {equation*}
This is a homogeneous system of linear equations with more unknowns (\(m\)) than equations (\(n\)) and thus
there is a solution with not all of the \(c_i\) being zero. It follows that (<span 
class="rm-lmssbx-10x-x-120">??</span>) has a solution besides the trivial
solution and so \(S'\) is linearly dependent.                                                                        □
</p>
</div>
<div class="proof">
<!--l. 372--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof of Proposition </span><a 
href="#x1-35001r4.4"><span 
class="rm-lmsso-12">4.4</span><!--tex4ht:ref: th:2.5.4 --></a><span 
class="rm-lmsso-12">.</span> </span> Let \(B=(\vv _1,\vv _2,\ldots ,\vv _n)\) and \(B'=(\vv '_1,\vv '_2,\ldots ,\vv '_m)\) be bases for \(V\). From the above lemma, we conclude that since
\(B\) is a basis and \(B'\) is linearly independent, \(m\leq n\). Equally, since \(B'\) is a basis and \(B\) is linearly independent, \(n\leq m\).
Altogether, we have \(m=n\).                                                                                            □
</p>
</div>
<!--l. 378--><p class="noindent" >We are now able to define properly the dimension of a vector space:
</p>
<div class="tcolorbox thm" id="tcolobox-151">    
<div class="tcolorbox-title">
<!--l. 383--><p class="noindent" >Theorem 4.13  </p></div> 
<div class="tcolorbox-content"><!--l. 382--><p class="noindent" >Let \(V\) be a vector space. We say that \(V\) is <span 
class="rm-lmsso-12">finite-dimensional  </span>if it has a finite basis, and
<span 
class="rm-lmsso-12">infinite-dimensional </span>otherwise. We define the <span 
class="rm-lmsso-12">dimension </span>of \(V\) to be the number of vectors in
any basis of \(V\). </p> 
</div> 
</div>
<!--l. 398--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-152">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 398--><p class="noindent" >Example 4.14</p></div> 
<div class="tcolorbox-content"></p><!--l. 388--><p class="noindent" >We have constructed bases of several vector spaces. We have the following dimensions:
\begin {equation*}  \begin {tabular}{r|c|c|c|c} Space &amp; $\FR ^2$ &amp; $\FR ^3$ &amp; $\FR ^n$ &amp; $\CP _n(\FR )$ \\ \hline Dimension &amp; 2 &amp; 3 &amp; $n$ &amp; $n+1$ \end {tabular}  \end {equation*}
</p> 
</div> 
</div>                                                                                                                      
<!--l. 400--><p class="noindent" >The following result is useful in finding a lower bound for the dimension of a vector space:
</p>
<div class="tcolorbox thm" id="tcolobox-153">    
<div class="tcolorbox-title">
<!--l. 404--><p class="noindent" >Theorem 4.15  </p></div> 
<div class="tcolorbox-content"><!--l. 403--><p class="noindent" >Let \(V\) be a vector space, and let \(\vv _1, \ldots , \vv _k\) be a linearly independent family. Then we have:
</p>
<div class="math-display" >
<img 
src="F17ZD_main68x.svg" alt="dim V ≥  k.
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 414--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">4.5   </span> <a 
 id="x1-360004.5"></a>Vector subspaces</h4>
<!--l. 416--><p class="noindent" >The previous examples are the main sources of vectors spaces for this course. However, we are often not
interested in the space of all vectors, or of all functions. Instead, we are often interested in particular
subsets of elements that satisfy some equation: system of linear equations, differential equations,
etc.
</p><!--l. 418--><p class="noindent" >In this section, we introduce the notion of <span 
class="rm-lmsso-12">vector subspace </span>as the natural notion of subset of a vector
space that is compatible with the operations of addition and multiplication by a scalar. We will see that
the set of solutions of various equations naturally form a vector subspace of the associated vector
space.
</p>
<div class="tcolorbox df" id="tcolobox-154">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 429--><p class="noindent" >Definition 4.16  </p></div> 
<div class="tcolorbox-content"><!--l. 421--><p class="noindent" >Let \(V\) be a vector space and let \(W\) be a subset of \(V\). We say that \(W\) is a <span 
class="rm-lmsso-12">vector subspace </span>(or simply
a subspace) of \(V\) if the following holds:
</p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 424--><p class="noindent" >\(\nv \in W\),
        </p></li>
        <li class="itemize">
        <!--l. 425--><p class="noindent" >for all \(\wv _1,\wv _2\in W\), we have \(\wv _1+\wv _2\in W\) and
        </p></li>
        <li class="itemize">
        <!--l. 426--><p class="noindent" >for all \(\lambda \in \FR \) and \(\wv \in W\), we have \(\lambda \wv \in W\).</p></li></ul>
 
</div> 
</div>
<!--l. 433--><p class="noindent" >A vector subspace \(W\) of \(V\) is itself a vector space, when endowed with the addition and scalar
multiplication coming from \(V\). (This is actually an equivalence: a subset \(W\) is a subspace if and
only if it \(W\) is a vector space when endowed with the addition and scalar multiplication from
\(V\).)
</p><!--l. 435--><p class="noindent" >As a consequence, we can talk of the dimension or of bases of a given subspace.
</p>
<!--l. 450--><p class="noindent" >
</p><!--l. 450--><p class="noindent" ><a 
 id="x1-36002r17"></a><!--l. 439--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 4.17</span>                                                                          </p><!--l. 439--><p class="noindent" ></p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 441--><p class="noindent" >In a vector space \(V\), the whole space \(V\) and the trivial subset \(\{\nv \}\) are always vector subspaces.
      </p></li>
      <li class="itemize">
      <!--l. 442--><p class="noindent" >Consider the subset \(W=\{(x,y)^T\in \FR ^2:x+y=0\}\) of \(\FR ^2\). That is, \(W\) consists of all the vectors of the form \((x, -x)^T\) for \(x \in \FR \). The null vector \((0,0)^T\) is
      clearly in \(W\). If we add two vectors or multiply a vector by a scalar in \(W\), we end up back in \(W\):
      \begin {equation*}  \vectt {x_1}{-x_1}+\vectt {x_2}{-x_2}=\vectt {x_1+x_2}{-(x_1+x_2)}~~~\mbox {and}~~~\lambda \vectt {x_1}{-x_1}=\vectt {\lambda x_1}{-\lambda x_1}~.  \end {equation*}
      Thus, \(W\) us a subspace of \(\FR ^2\). Note that every vector of \(W\) is of the form
      </p>
<div class="math-display" >                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 450--><p class="noindent" ><img 
src="F17ZD_main69x.svg" alt="∖left(  x  ∖right) = x∖left(  1  ∖right)  for some  x ∈ ℝ,
       − x                   − 1
" class="math-display"  /></div>
      <!--l. 448--><p class="noindent" >so \(W\) has dimension \(1\) and a basis of \(W\) is given by the vector \((1, -1)^T.\)</p></li></ul>
 
</div> 
</div>                                                                                        
</p>
<!--l. 452--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-370004.5"></a><span 
class="rm-lmssbx-10x-x-120">Remark.</span></span>
If a subspace \(W\) contains a vector \(\wv \), it also contains its opposite \(-\wv \), as \(W\) is stable under multiplication by a
scalar and \(-\wv = (-1).\wv \).
</p>
<!--l. 462--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-155">    
<div class="tcolorbox-title">
<!--l. 462--><p class="noindent" >Example 4.18</p></div> 
<div class="tcolorbox-content"></p><!--l. 460--><p class="noindent" > The subset \(W'=\{(x,y)^T | x+y=1\}\) is not a vector subspace of \(\FR ^2\), as \(\nv =(0,0)^T\) is not an element of \(W'\). Another reason is that the
sum of two elements \(w_1,w_2\in W'\) is not always in \(W'\). For instance, \((1,0)^T\) and \((0,1)^T\) belong to \(W'\), but their sum \((1,1)^T\) does not.
</p> 
</div> 
</div>                                                                                                                      
<!--l. 488--><p class="noindent" >
</p><!--l. 488--><p class="noindent" ><a 
 id="x1-37002r19"></a><!--l. 476--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 4.19</span>                                                                          </p><!--l. 476--><p class="noindent" ></p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 477--><p class="noindent" >Let \(V=\FR ^2\). Then \(W=\{(x,y)^T | x=3y\}\) is a vector subspace, since vectors of \(W\) are of the form
      </p>
<div class="math-display" >
<img 
src="F17ZD_main70x.svg" alt="       3y                    3
∖lef t(  y ∖right ) = y∖left( 1 ∖right)  for some y ∈ ℝ,
" class="math-display"  /></div>                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 488--><p class="noindent" >      <!--l. 479--><p class="noindent" >and we have: \begin {equation*}  \vectt {3y_1}{y_1}+\vectt {3y_2}{y_2}=\vectt {3(y_1+y_2)}{y_1+y_2}~~~\mbox {and}~~~\lambda \vectt {3y}{y}=\vectt {3\lambda y}{\lambda y}~,~~~\lambda \in \FR ~.  \end {equation*}
      Thus, \(W\) is a subspace of \(\FR ^2\) of dimension \(1\), and a basis of it is given by the vector \((3, 1)^T\).
      </p><!--l. 485--><p class="noindent" >In general, lines through the origin of \(\FR ^2\) form vector subspaces of \(\FR ^2\).<br 
class="newline" />
      </p></li>
      <li class="itemize">
      <!--l. 486--><p class="noindent" >Lines in \(\FR ^2\) that do not pass through the origin do not contain \(\nv \) and thus are not vector subspaces
      of \(\FR ^2\), cf. example in <a 
href="#x1-37001r4.5">4.5<!--tex4ht:ref: ex:2.2.2 --></a>.</p></li></ul>
 
</div> 
</div>                                                                                        
</p>
<!--l. 492--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-380004.5"></a><span 
class="rm-lmssbx-10x-x-120">Subspaces of</span> \(\FR ^2\)<span 
class="rm-lmssbx-10x-x-120">.</span></span>
We have seen already that we can completely describe the subspaces of \(\FR ^2\): </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 494--><p class="noindent" >the trivial subspace \(\{\nv \}\),
      </p></li>
      <li class="itemize">
      <!--l. 495--><p class="noindent" >lines through the origin,
      </p></li>
      <li class="itemize">
      <!--l. 496--><p class="noindent" >the whole space \(\FR ^2\).</p></li></ul>
<!--l. 498--><p class="noindent" >There is a similar picture in \(\FR ^3\), where subspaces can be lines through the origin, planes through the origin,
etc. Geometrically, being stable under addition and scalar multiplication makes vector subspaces ‘look
flat’.
</p>
<!--l. 504--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-390004.5"></a><span 
class="rm-lmssbx-10x-x-120">Solutions of systems of linear differential equations as subspaces.</span></span>
We saw in a previous example how the set of solutions of certain differential equations may be described
as a span, and hence is a subspace of \(\CF (\FR )\). Even without an explicit description of the solutions, it is possible
to show that the set of solutions forms a subspace.
                                                                                          
                                                                                          
</p><!--l. 507--><p class="noindent" >Consider the following differential equation (linearised simple pendulum):
</p>
<div class="math-display" >
<img 
src="F17ZD_main71x.svg" alt="y′′ + ω2y = 0.
" class="math-display"  /></div>
<!--l. 509--><p class="noindent" >Then the set \(W\) of solutions of this equation is a subspace of \(\CF (\FR )\).
</p>
<div class="proof">
<!--l. 513--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>Let \(y_1, y_2\in W\) and \(\lambda \in \FR \). Then: \begin {equation*}  \begin {aligned} &amp;(y_1 + y_2)'' + \omega ^2 (y_1 + y_2) =(y_1'' + \omega ^2 y_1) + (y_2'' + \omega ^2 y_2) = 0 + 0 = 0,\mbox { hence } y_1+y_2\in W\\ &amp;(\lambda y_1)'' + \omega ^2 (\lambda y_1) = \lambda (y_1'' + \omega ^2 y_1) = \lambda 0 = 0,\mbox { hence } \lambda y_1\in W~. \end {aligned}  \end {equation*}
We conclude that \(W\) is a vector subspace of \(\CF (\FR )\).                                                                 □
</p>
</div>
<!--l. 608--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-400004.5"></a><span 
class="rm-lmssbx-10x-x-120">Dimension of subspaces.</span></span>
Since a subspace of vector space is itself a vector space, it also has a dimension. It is natural to wonder
whether the dimension behaves well with respect to subspaces: For instance, is the dimension of a
subspace at most the dimension of the original vector space? While this intuitively obvious, it requires a
proof. And indeed, things go extremely well:
</p>
<div class="tcolorbox thm" id="tcolobox-156">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 618--><p class="noindent" >Theorem 4.20  </p></div> 
<div class="tcolorbox-content"><!--l. 615--><p class="noindent" >Let \(V\) be a finite-dimensional vector space, and let \(W\) be a subspace. Then
</p>
<div class="math-display" >
<img 
src="F17ZD_main72x.svg" alt="dim W  ≤  dim V.
" class="math-display"  /></div>
<!--l. 616--><p class="noindent" >Moreover, we have
</p>
<div class="math-display" >
<img 
src="F17ZD_main73x.svg" alt="dim  W  = dim V   ⇐ ⇒   W  =  V.
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 623--><p class="noindent" >This result can provide a simple way to show the equality between two subspaces: Instead
of showing both inclusions, it is only necessary to show one inclusion and the equality of
dimensions, something that is generally easier to handle. We will see applications in the next
chapter.
</p><!--l. 625--><p class="noindent" >The previous theorem relies on the following results:
</p><!--l. 627--><p class="noindent" >Let \(V\) be an \(n\)-dimensional vector space and let \(\vv _1,\ldots ,\vv _k\) be a linearly independent family of vectors. Then \(\vv _1,\ldots ,\vv _k\) can be
extended to a basis \(\vv _1, \ldots , \vv _n\) of \(V\).                                                                                         □
</p><!--l. 634--><p class="noindent" >Suppose that a family \(\vv _1,\ldots ,\vv _k\) of vectors spans a (finite-dimensional) vector space \(V\). Then there exists a
subfamily that is a basis for \(V\).                                                                                   □
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p>
<h3 class="sectionHead"><span class="titlemark">5   </span> <a 
 id="x1-410005"></a>The solution space of a homogeneous system of linear equations.</h3>
<!--l. 3--><p class="noindent" >In F17ZB we looked at solving \(A \xv = \bv \), now we are going to look in more detail at solving the <span 
class="rm-lmsso-12">homogeneous</span>
system of equations \[ A \xv = \nv \] This leads on naturally to a discussion of <span 
class="rm-lmsso-12">rank </span>for matrices (we have already seen
one definition of this in F17ZB).
</p>
<div class="tcolorbox thm" id="tcolobox-157">    
<div class="tcolorbox-title">
<!--l. 13--><p class="noindent" >Theorem 5.1  </p></div> 
<div class="tcolorbox-content"><!--l. 12--><p class="noindent" >The set of solutions of a <span 
class="rm-lmsso-12">homogeneous </span>system of linear equations in \(n\) unknowns is a vector
subspace of \(\FR ^n\) whose dimension is the number of free variables, after reducing the system to
its echelon form. </p> 
</div> 
</div>
<div class="proof">
<!--l. 18--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>Let \(W\) be the set of solutions. By definition, we have \(W= \{\xv \in \FR ^n ~|~ A\xv = \nv \}\) for some matrix \(A\). In particular, the null vector
belongs to \(W\). Let \(\xv _1,\xv _2\in W\) and \(\lambda \in \FR \). We have: \begin {equation*}  \begin {aligned} &amp;A(\xv _1+\xv _2)=A\xv _1+A\xv _2=\nv +\nv =\nv ,\mbox { hence } \xv _1+\xv _2\in W\\ &amp;A(\lambda \xv _1)=\lambda A\xv _1=\lambda \nv =\nv ,\mbox { hence } \lambda \xv _1\in W~. \end {aligned}  \end {equation*}
Thus, \(W\) is a vector subspace of \(\FR ^n\). We solve the system \(A\xv = \nv \) using Gaussian elimination. As usual, we can
express solutions in terms of the \(k \leq n\) free variables \(\alpha _1, \ldots , \alpha _k\) of the system. In particular, we write the general
solution as a linear combination of the form \(\alpha _1 \vv _1 +\ldots + \alpha _k\vv _k \) for some vectors \(\vv _1, \ldots , \vv _k\) of \(\FR ^n\). In particular, we see that
the family \(\vv _1, \ldots , \vv _k\) spans the subspace of solutions. To show that this family is also free, notice
that if a free variable \(\alpha _i\) corresponds to the variable \(x_j\), then the \(j\)-th component of the vector
\(\alpha _1 \vv _1 +\ldots + \alpha _k\vv _k \) is exactly \(\alpha _i\). In particular, if \(\alpha _1 \vv _1 +\ldots + \alpha _k\vv _k = \nv \), then all the components must be zero, and it follows that
\(\alpha _1 = \ldots = \alpha _k = 0\).                                                                                                                      □
</p>
</div>
<!--l. 30--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-420005"></a><span 
class="rm-lmssbx-10x-x-120">Remark.</span></span>
Note that \(\{x\in \FR ^n:A\xv =\bv \}\) with \(\bv \neq \nv \) is <span 
class="rm-lmsso-12">not </span>a vector subspace. Indeed, this subset does not contain the null vector as \(A\nv = \nv \neq \bv \). This
is analogous to lines in \(\FR ^2\) not running through the origin.
</p><!--l. 33--><p class="noindent" >To find a basis of the space of solutions of a homogeneous system of linear equations, we can apply the
following algorithm: </p>
      <ul class="itemize1">
      <li class="itemize">
                                                                                          
                                                                                          
      <!--l. 35--><p class="noindent" >Solve the system using Gaussian elimination.
      </p></li>
      <li class="itemize">
      <!--l. 36--><p class="noindent" >Express solutions in terms of the free variables \(\alpha _1, \ldots , \alpha _k\).
      </p></li>
      <li class="itemize">
      <!--l. 37--><p class="noindent" >Decompose the general solution as a linear combination of the form \(\alpha _1 \vv _1 + \ldots + \alpha _k\vv _k.\)
      </p></li>
      <li class="itemize">
      <!--l. 38--><p class="noindent" >A basis of the space of solutions is \(\vv _1, \ldots , \vv _k\).</p></li></ul>
<!--l. 90--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-158">    
<div class="tcolorbox-title">
<!--l. 90--><p class="noindent" >Example 5.2</p></div> 
<div class="tcolorbox-content"></p><!--l. 43--><p class="noindent" >Consider the following system of linear equations: \begin {equation*} \relax \expandafter \ifx \csname cur:th\endcsname \relax \expandafter \:label \else \expandafter \l:bel \fi {eq:SLE2} \begin {aligned} x_1-x_2+2x_3+x_4&amp;=0~,\\ 2x_1+x_2-x_3+x_4&amp;=0~,\\ 4x_1-x_2+3x_3+3x_4&amp;=0~,\\ x_1+2x_2-3x_3&amp;=0~. \end {aligned}  \end {equation*}
We would like to determine a basis of the subspace \(W\) of solutions. We first perform Gaussian
elimination to reduce the system: \begin {equation*}  \begin {aligned} \left (\begin {array}{cccc|c} 1 &amp; -1 &amp; 2 &amp; 1 &amp; 0\\ 2 &amp; 1 &amp; -1 &amp; 1 &amp; 0\\ 4 &amp; -1 &amp; 3 &amp; 3 &amp; 0\\ 1 &amp; 2 &amp; -3 &amp; 0 &amp; 0 \end {array}\right )~~~ \melt {R_2\rightarrow R_2-2R_1\\R_3\rightarrow R_3-4R_1\\R_4\rightarrow R_4-R_1}~~~ \left (\begin {array}{cccc|c} 1 &amp; -1 &amp; 2 &amp; 1 &amp; 0\\ 0 &amp; 3 &amp; -5 &amp; -1 &amp; 0\\ 0 &amp; 3 &amp; -5 &amp; -1 &amp; 0\\ 0 &amp; 3 &amp; -5 &amp; -1 &amp; 0 \end {array}\right )\\ \melt {R_3\rightarrow R_3-R_2\\R_4\rightarrow R_4-R_2}~~~ \left (\begin {array}{cccc|c} 1 &amp; -1 &amp; 2 &amp; 1 &amp; 0\\ 0 &amp; 3 &amp; -5 &amp; -1 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \end {array}\right ) \end {aligned}  \end {equation*}
There are two free variables, \(x_3\) and \(x_4\), and the general solution is thus of the form \begin {equation*}  x_3=\alpha ~,~~~x_4=\beta ~,~~~x_2=\frac {1}{3}(5\alpha +\beta )~,~~~x_1=x_2-2x_3-x_4=-\frac {1}{3}\alpha -\frac {2}{3}\beta ~.  \end {equation*}
Any solution can be rewritten as \begin {equation*}  \xv =\vectttt {x_1}{x_2}{x_3}{x_4}=\frac {1}{3}\vectttt {-\alpha -2\beta }{5\alpha +\beta }{3\alpha }{3\beta }=\frac {\alpha }{3}\vectttt {-1}{5}{3}{0}+\frac {\beta }{3}\vectttt {-2}{1}{0}{3}~,  \end {equation*}
and we find that \(\frac {1}{3}(-1,5,3,0)^T, \frac {1}{3}(-2,1,0,3)^T\) is a basis of the solution space of the system. </p> 
</div> 
</div>                                     
<!--l. 94--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-430005"></a><span 
class="rm-lmssbx-10x-x-120">The span of a family of vectors.</span></span>
Another important type of subspace is given by spans of families of vectors. The following generalises a
result we have seen in \(\FR ^n\): </p>
<div class="tcolorbox thm" id="tcolobox-159">    
<div class="tcolorbox-title">
<!--l. 100--><p class="noindent" >Theorem 5.3  </p></div> 
<div class="tcolorbox-content"><!--l. 99--><p class="noindent" >Let \(V\) be a vector space and let \(\uv _1,\ldots ,\uv _k\) be vectors of \(V\). Then \(\mathrm {span}(\uv _1,\ldots ,\uv _k)\) is a vector subspace of \(V\). </p> 
</div> 
</div>
<div class="proof">
<!--l. 103--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>The span contains the null vector since \(\nv = 0\uv _1 + \ldots + 0\uv _k\). Let \(a_1\uv _1 + \ldots + a_k\uv _k, b_1\uv _1+\ldots + b_k \uv _k\) be vectors in \(span(\uv _1,\ldots ,\uv _k)\), and let \(\lambda \in \FR \). Then we have
                                                                                          
                                                                                          
</p>
<div class="math-display" >
<img 
src="F17ZD_main74x.svg" alt="(a1⃗u1 + ...+ ak⃗uk) + (b1⃗u1 + ...+ bk⃗u) = (a1 + b1)⃗u1 + ...+  (ak + bk)⃗uk ∈ span (⃗u1,...,⃗uk),
" class="math-display"  /></div>
<!--l. 106--><p class="noindent" >
</p>
<div class="math-display" >
<img 
src="F17ZD_main75x.svg" alt="λ (a1 ⃗u1 + ...+ ak⃗uk) = (λa1)⃗u1 + ...+  (λak )⃗uk ∈ span (⃗u1, ...,⃗uk).
" class="math-display"  /></div>
<!--l. 107--><p class="noindent" >Thus, \(span(\uv _1,\ldots ,\uv _k)\) is a subspace of \(V\).                                                                                        □
</p>
</div>
<!--l. 114--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-160">    
<div class="tcolorbox-title">
<!--l. 114--><p class="noindent" >Example 5.4</p></div> 
<div class="tcolorbox-content"></p><!--l. 111--><p class="noindent" ></p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 112--><p class="noindent" >A solution of the differential equation \(y'' + \omega ^2 y = 0\) is of the form \(a\cos (\omega x) + b\sin (\omega x)\) with \(a, b \in \FR \). Thus, the set of solutions of
        the differential equation \(y'' + \omega ^2 y = 0\) is the span of the functions \(x \mapsto \cos (\omega x)\) and \(x \mapsto \sin (\omega x)\). In particular, we recover the
        fact that it is a vector subspace of \(\CF (\FR )\).</p></li></ul>
 
</div> 
</div>                                                                                                                      
                                                                                          
                                                                                          
<!--l. 116--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-440005"></a><span 
class="rm-lmssbx-10x-x-120">Remark.</span></span>
A vector subspace can sometimes be spanned by many different sets of vectors: for instance, both the
pairs of vectors \(\big ((1,0)^T,(0,1)^T\big )\) and \(\big ((1,0)^T,(1,1)^T\big )\) span \(\FR ^2\). Indeed, given a vector \((x,y)^T\in \FR ^2\), we have \begin {equation*}  \vectt {x}{y}=x\vectt {1}{0}+y\vectt {0}{1}~~\mbox { and also }~~~\vectt {x}{y}=(x-y)\vectt {1}{0}+y\vectt {1}{1}~.  \end {equation*}
</p>
<div class="tcolorbox df" id="tcolobox-161">    
<div class="tcolorbox-title">
<!--l. 124--><p class="noindent" >Definition 5.5  </p></div> 
<div class="tcolorbox-content"><!--l. 123--><p class="noindent" >The <span 
class="rm-lmsso-12">rank </span>of a family of vectors \(\vv _1, \ldots , \vv _k\) is the dimension of the subspace they span. </p> 
</div> 
</div>
<!--l. 126--><p class="noindent" >For this type of subspaces, there is also a simple way to determine their dimension and find a basis.
However, the proof of this result requires tools that will be introduced in the next chapter, so we
postpone its proof for now.
</p><!--l. 148--><p class="noindent" >Finding the dimension and a basis of the span of a family of vectors of \(\FR ^n\). We can find a basis for \(\mathrm {span}(\vv _1,\ldots ,\vv _k)\) as
follows: </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 151--><p class="noindent" >Write down a matrix whose \(i\)th column is \(\vv _i\).
      </p></li>
      <li class="itemize">
      <!--l. 152--><p class="noindent" >Perform elementary row operations to bring the matrix into row echelon form.
      </p></li>
      <li class="itemize">
      <!--l. 153--><p class="noindent" >The rank of \((\vv _1,\ldots ,\vv _k)\) is the number of pivot variables in the row echelon form. If we denote by \(j_1, \ldots , j_k\) the
      columns of the row echelon form that contain a pivot, then a basis for \(\mathrm {span}(\vv _1,\ldots ,\vv _k)\) is given by \(\vv _{j_1}, \ldots , \vv _{j_k}\).</p></li></ul>
<!--l. 199--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-162">    
<div class="tcolorbox-title">
<!--l. 199--><p class="noindent" >Example 5.6</p></div> 
<div class="tcolorbox-content"></p><!--l. 162--><p class="noindent" > We wish to find a basis for the subspace of \(\FR ^3\) spanned by \[ v_1=\vecttt {1}{0}{1},\qquad v_2=\vecttt {2}{1}{3},\qquad v_3=\vecttt {3}{1}{4}. \] We apply the previous algorithm
(place the vectors as columns and row-reduce): \[ \begin {aligned} \left (\begin {array}{ccc} 1 &amp; 2 &amp; 3\\ 0 &amp; 1 &amp; 1\\ 1 &amp; 3 &amp; 4 \end {array}\right ) \melt {R_3\rightarrow R_3-R_1} \left (\begin {array}{ccc} 1 &amp; 2 &amp; 3\\ 0 &amp; 1 &amp; 1\\ 0 &amp; 1 &amp; 1 \end {array}\right ) \melt {R_3\rightarrow R_3-R_2} \left (\begin {array}{ccc} 1 &amp; 2 &amp; 3\\ 0 &amp; 1 &amp; 1\\ 0 &amp; 0 &amp; 0 \end {array}\right ). \end {aligned} \]  There are two pivot columns (columns \(1\) and
\(2\)), so the rank of this family of vectors is \(2\) and the vectors span a two-dimensional subspace of \(\FR ^3\)
(a plane through the origin). Hence a basis for the span is given by \[ \vecttt {1}{0}{1},\ \vecttt {2}{1}{3}. \] Moreover, since the third
column is non-pivot, \(v_3\) is a linear combination of \(v_1\) and \(v_2\); indeed the row-reduction shows \(v_3=v_1+v_2\). </p> 
</div> 
</div>       
<!--l. 201--><p class="noindent" >Since finding a span is a useful skill, let’s consider a further example <span 
class="wasy-10x-x-120">☺</span>
</p>
<!--l. 238--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="tcolorbox example" id="tcolobox-163">    
<div class="tcolorbox-title">
<!--l. 238--><p class="noindent" >Example 5.7</p></div> 
<div class="tcolorbox-content"></p><!--l. 203--><p class="noindent" > We wish to find a basis for the subspace of \(\FR ^4\) spanned by \( (1,2,3, 0)^T, (2,1,2,1)^T\) and \((1,5,7,-1)^T, (0,0,1,2)^T\). We apply the previous algorithm:
\begin {equation*}  \begin {aligned} \left (\begin {array}{cccc} 1 &amp; 2 &amp; 1 &amp; 0 \\ 2 &amp; 1 &amp; 5 &amp; 0 \\ 3 &amp; 2 &amp; 7 &amp; 1\\ 0 &amp; 1 &amp; -1 &amp; 2 \end {array}\right )\melt {R_2\rightarrow R_2-2R_1\\R_3\rightarrow R_3-3R_1} \left (\begin {array}{cccc} 1 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; -3 &amp; 3 &amp; 0 \\ 0 &amp; -4 &amp; 4 &amp; 1\\ 0 &amp; 1 &amp; -1 &amp; 2 \end {array}\right ) \melt {R_3\rightarrow 3R_3-4R_2 \\ R_4 \rightarrow 3R_4 + R_2} \\ \left (\begin {array}{cccc} 1 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; -3&amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3\\ 0 &amp; 0 &amp; 0 &amp; 2 \end {array}\right ) \elt {R_4 \rightarrow -3R_4+2R_3 } \left (\begin {array}{cccc} 1 &amp; 2 &amp; 1 &amp; 0 \\ 0 &amp; -3 &amp; 3 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 3\\ 0 &amp; 0 &amp; 0 &amp; 0 \end {array}\right ). \end {aligned}  \end {equation*}
There are three pivot variables, corresponding to columns \(1, 2\), and \(4\), so the rank of this family of
vectors is \(3\) and a basis for its span is given by \( (1,2,3, 0)^T, (2,1,2,1)^T, (0,0,1,2)^T\). </p> 
</div> 
</div>                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
<h3 class="sectionHead"><span class="titlemark">6   </span> <a 
 id="x1-450006"></a>Matrices</h3>
<!--l. 4--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">6.1   </span> <a 
 id="x1-460006.1"></a>Image and rank of a matrix.</h4>
<div class="tcolorbox df" id="tcolobox-164">    
<div class="tcolorbox-title">
<!--l. 21--><p class="noindent" >Definition 6.1  </p></div> 
<div class="tcolorbox-content"><!--l. 8--><p class="noindent" >Given an \(m \times n\) matrix written in column form as \begin {equation*}  A=\left (\begin {array}{c|c|c} \cv _1 &amp; \cdots &amp; \cv _n \end {array}\right )~,  \end {equation*}
the <span 
class="rm-lmsso-12">image </span>of \(A\), denoted Im\((A)\), is the subspace of \(\FR ^m\) spanned by \(\cv _1, \ldots , \cv _n\). Its dimension is called the <span 
class="rm-lmsso-12">rank </span>of \(A\)
and denoted \(\rk (A)\) or rank\((A)\). </p> 
</div> 
</div>
<!--l. 39--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-165">    
<div class="tcolorbox-title">
<!--l. 39--><p class="noindent" >Example 6.2</p></div> 
<div class="tcolorbox-content"></p><!--l. 23--><p class="noindent" >Consider the following matrix: \begin {equation*}  A=\left (\begin {array}{ccc} 1 &amp; 2 &amp; 3\\ 4 &amp; 5 &amp; 6 \end {array} \right ) \hspace {0.5cm}\begin {array}{ll} \mbox {row space:}~span\{(1,2,3),(4,5,6)\}~,~~~&amp;2=\rk _r(A)\leq 2~,\\ \mbox {column space:}~span\{(1,4)^T,(2,5)^T,(3,6)^T\}~,~~~&amp;2=\rk _c(A)\leq 3~.\\ \end {array}  \end {equation*}
</p><!--l. 36--><p class="noindent" >The column space of \(A\) is \(span((1,4)^T,(2,5)^T,(3,6)^T) \) and a basis of it (obtained by Gaussian elimination) is \((1,4)^T,(2,5)^T\).
</p><!--l. 38--><p class="noindent" >In this example, we see that \(\rk _r(A) = \rk _c(A) = 2.\) </p> 
</div> 
</div>                                                                                 
<div class="tcolorbox thm" id="tcolobox-166">    
<div class="tcolorbox-title">
<!--l. 43--><p class="noindent" >Theorem 6.3  </p></div> 
<div class="tcolorbox-content"><!--l. 42--><p class="noindent" >The inhomogeneous system of linear equations \(A\xv =\bv \) is consistent if and only if \(\bv \) is in the column
space of \(A\). </p> 
</div> 
</div>
<div class="proof">
<!--l. 45--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>\begin {equation*}  \begin {aligned} \{A\xv : \xv \in \FR ^n\}&amp;=\left \{\left (\begin {array}{c} a_{11}x_1+\ldots +a_{1n}x_n\\ \vdots \\ a_{m1}x_1+\ldots +a_{mn}x_n \end {array}\right ): x_1,...,x_n\in \FR \right \}\\ &amp;=\left \{x_1\left (\begin {array}{c} a_{11}\\ \vdots \\ a_{m1} \end {array}\right )+\ldots +x_n\left (\begin {array}{c} a_{1n}\\ \vdots \\ a_{mn} \end {array}\right ): x_1,...,x_n\in \FR \right \}\\ &amp;=\span \left \{\left (\begin {array}{c} a_{11}\\ \vdots \\ a_{m1} \end {array}\right ),\ldots ,\left (\begin {array}{c} a_{1n}\\ \vdots \\ a_{mn} \end {array}\right )\right \} \end {aligned}  \end {equation*}
The last expression is the column space of \(A\) and thus consistency of the system of linear equations \(A\xv =\bv \)
requires that \(\bv \) is in the column space of \(A\).                                                                    □
</p>
</div>
                                                                                          
                                                                                          
<div class="tcolorbox thm" id="tcolobox-167">    
<div class="tcolorbox-title">
<!--l. 67--><p class="noindent" >Theorem 6.4  </p></div> 
<div class="tcolorbox-content"><!--l. 66--><p class="noindent" >Elementary row operations do not change the row rank of a matrix. </p> 
</div> 
</div>
<!--l. 72--><p class="noindent" >Determining the row rank and a basis of the row space. We can determine a basis of the row space as
follows: </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 74--><p class="noindent" >Bring the matrix to row echelon form.
      </p></li>
      <li class="itemize">
      <!--l. 75--><p class="noindent" >A basis for the row space consists of the family of non-zero rows of the matrix in echelon
      form, and the row rank is the number of non-zero row in the row echelon form.</p></li></ul>
<!--l. 86--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-168">    
<div class="tcolorbox-title">
<!--l. 86--><p class="noindent" >Example 6.5</p></div> 
<div class="tcolorbox-content"></p><!--l. 79--><p class="noindent" >Consider the following matrix: \begin {equation*}  A=\left (\begin {array}{cccc}1 &amp; 2 &amp; -1 &amp; 3 \\ 2 &amp; -1 &amp; 2 &amp; 1 \\ 4 &amp; 3 &amp; 0 &amp; 7\\ 0 &amp; 0 &amp; 1 &amp; 2\end {array}\right ).  \end {equation*}
After performing Gaussian elimination, we find the following row echelon form:
</p>
<div class="math-display" >
<img 
src="F17ZD_main76x.svg" alt="       1  2   − 1   3
∖lef t( 0  − 5  4   − 5 ∖right).
       0  0    1    2
       0  0    0    0
" class="math-display"  /></div>
<!--l. 85--><p class="noindent" >As the row vectors in the echelon form are linearly independent, we have that a basis of the row space
of \(A\) is \((1,2,-1,3),(0,-5,4,-5),(0,0,1,2)\), and in particular \(\rk _r(A)=3\). </p> 
</div> 
</div>                                                                                     
<!--l. 89--><p class="noindent" >Now consider determining the <span 
class="rm-lmsso-12">column </span>rank and a basis of the column space. We can determine the
column space of a matrix \(A\) and its column rank as follows: </p>
                                                                                          
                                                                                          
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 91--><p class="noindent" >Compute the transpose \(A^T\).
      </p></li>
      <li class="itemize">
      <!--l. 92--><p class="noindent" >Bring \(A^T\) to row echelon form.
      </p></li>
      <li class="itemize">
      <!--l. 93--><p class="noindent" >The column rank of \(A^T\) is the number of non-zero rows of the echelon form of \(A^T\), and a basis
      for the column space is given by the transpose of the non-zero rows of the echelon form.</p></li></ul>
<!--l. 104--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-169">    
<div class="tcolorbox-title">
<!--l. 104--><p class="noindent" >Example 6.6</p></div> 
<div class="tcolorbox-content"></p><!--l. 98--><p class="noindent" >Let us consider the same matrix as in the previous example. We bring its transpose to row echelon
form: \begin {equation*}  A=\left (\begin {array}{cccc}1 &amp; 2 &amp; -1 &amp; 3 \\ 2 &amp; -1 &amp; 2 &amp; 1 \\ 4 &amp; 3 &amp; 0 &amp; 7\\ 0 &amp; 0 &amp; 1 &amp; 2\end {array}\right )~,~~~ A^T=\left (\begin {array}{cccc}1 &amp; 2 &amp; 4 &amp; 0 \\ 2 &amp; -1 &amp; 3 &amp; 0 \\ -1 &amp; 2 &amp; 0 &amp; 1\\ 3 &amp; 1 &amp; 7 &amp; 2\end {array}\right )~~~\rightsquigarrow ~~~ \left (\begin {array}{cccc}1 &amp; 2 &amp; 4 &amp; 0 \\ 0 &amp; 1 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1\\ 0 &amp; 0 &amp; 0 &amp; 0\end {array}\right )~.  \end {equation*}
Thus, \(\big ((1,2,4,0)^T,(0,1,1,0)^T,(0,0,0,1)^T\big )\) is a basis for the column space of \(A\) and the column rank is \(\rk _c(A)=3\). </p> 
</div> 
</div>                                 
<!--l. 106--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">6.2   </span> <a 
 id="x1-470006.2"></a>Rank of a matrix.</h4>
<!--l. 107--><p class="noindent" >Again, we saw in the previous two examples that \(\rk _r(A)=\rk _c(A)=3\). This is a consequence of the following general
result:
</p>
<div class="tcolorbox thm" id="tcolobox-170">    
<div class="tcolorbox-title">
<!--l. 111--><p class="noindent" >Theorem 6.7  </p></div> 
<div class="tcolorbox-content"><!--l. 110--><p class="noindent" >For an \(n \times m\) matrix \(A\), we have
</p>
<div class="math-display" >
<img 
src="F17ZD_main77x.svg" alt="rk (A) = rk (A ).
  c        r
" class="math-display"  /></div>
<!--l. 110--><p class="noindent" >This quantity, simply denoted \(\rk (A)\), is called the <span 
class="rm-lmsso-12">rank </span>of the matrix \(A\). </p> 
</div> 
</div>
                                                                                          
                                                                                          
<div class="proof">
<!--l. 113--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>Let \((\ev _1,\ldots ,\ev _k)\) be a basis of the row space: \(span\{\rv _1,\ldots ,\rv _m\}=span\{\ev _1,\ldots ,\ev _k\}\). We then have: \begin {equation*}  \begin {aligned} &amp;A=\vecttdt {\rv _1}{\rv _2}{\rv _m}= \vecttdt {a_{11}\ev _1+\ldots +a_{1k}\ev _k}{a_{21}\ev _1+\ldots +a_{2k}\ev _k}{a_{m1}\ev _1+\ldots +a_{mk}\ev _k}\\ &amp;=\left (\hspace {-0.1cm}\begin {array}{cccc} a_{11}e_{11}+\ldots +a_{1k}e_{k1} &amp; a_{11}e_{12}+\ldots +a_{1k}e_{k2} &amp; \ldots &amp; a_{11}e_{1n}+\ldots +a_{1k}e_{kn}\\ \vdots &amp; \vdots &amp; &amp; \vdots \\ a_{m1}e_{11}+\ldots +a_{mk}e_{k1} &amp; a_{m1}e_{12}+\ldots +a_{mk}e_{k2} &amp; \ldots &amp; a_{m1}e_{1n}+\ldots +a_{mk}e_{kn} \end {array}\hspace {-0.1cm}\right )\,. \end {aligned}  \end {equation*}
The column space is thus spanned by \(\{(a_{11},\ldots ,a_{m1})^T,\ldots ,(a_{1k},\ldots ,a_{mk})^T\}\). It follows that \(\rk _r(A)\geq \rk _c(A)\). Interchanging rows and columns in this argument
leads to \(\rk _c(A)\geq \rk _r(A)\), and altogether, we have \(\rk _c(A)=\rk _r(A)\).                                                                           □
</p>
</div>
<!--l. 137--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-171">    
<div class="tcolorbox-title">
<!--l. 137--><p class="noindent" >Example 6.8</p></div> 
<div class="tcolorbox-content"></p><!--l. 129--><p class="noindent" >\begin {equation*}  \begin {aligned} &amp;\left (\begin {array}{ccc} 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0\end {array}\right )~~\mbox {has rank 0}~~~ &amp;\left (\begin {array}{ccc} 1 &amp; 1 &amp; 1 \\ 1 &amp; -1 &amp; 1 \\ 0 &amp; 0 &amp; 0\end {array}\right )~~\mbox {has rank 2}\\ &amp;\left (\begin {array}{ccc} 1 &amp; 1 &amp; 1 \\ 2 &amp; 2 &amp; 2 \\ 3 &amp; 3 &amp; 3\end {array}\right )~~\mbox {has rank 1}~~~ &amp;\left (\begin {array}{ccc} 1 &amp; 2 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1\end {array}\right )~~\mbox {has rank 3} \end {aligned}  \end {equation*}
</p> 
</div> 
</div>                                                                                                                      
<!--l. 139--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">6.3   </span> <a 
 id="x1-480006.3"></a>Rank and systems of linear equations.</h4>
<!--l. 141--><p class="noindent" >Let us reinterpret some old results in term of the rank. There is nothing new in this paragraph (and you
should convince yourself of it):
</p>
<div class="tcolorbox thm" id="tcolobox-172">    
<div class="tcolorbox-title">
<!--l. 146--><p class="noindent" >Theorem 6.9  </p></div> 
<div class="tcolorbox-content"><!--l. 144--><p class="noindent" >Let \(A\) be an \(n \times n\) matrix, Then:
</p>
<div class="math-display" >
<img 
src="F17ZD_main78x.svg" alt="A is invertible ⇐ ⇒ rk(A ) = n.
" class="math-display"  /></div>
 
</div> 
</div>
                                                                                          
                                                                                          
<div class="tcolorbox thm" id="tcolobox-173">    
<div class="tcolorbox-title">
<!--l. 150--><p class="noindent" >Theorem 6.10  </p></div> 
<div class="tcolorbox-content"><!--l. 149--><p class="noindent" >Let \(A\) be an \(m\times n\) matrix. Then the system of linear equations \(A\xv =\bv \) is consistent for all \(\bv \in \FR ^m\) if and only if
\(\rk (A)=m\). </p> 
</div> 
</div>
<div class="proof">
<!--l. 152--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>The statement that \(A\xv =\bv \) is consistent for all \(\bv \in \FR ^m\) is equivalent to the fact that the image of \(A\) is \(\FR ^m\)
and thus that the (column) rank of \(A\) is maximal: \(\rk (A)=m\).                                                         □
</p>
</div>
<div class="tcolorbox thm" id="tcolobox-174">    
<div class="tcolorbox-title">
<!--l. 164--><p class="noindent" >Theorem 6.11  </p></div> 
<div class="tcolorbox-content"><!--l. 158--><p class="noindent" >Let\(A\) be an \(m\times n\)-matrix. We have: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 160--><p class="noindent" >\(\rk (A)&lt;n\Rightarrow A\xv =\nv \) has infinitely many solutions.
        </p></li>
        <li class="itemize">
        <!--l. 161--><p class="noindent" >\(\rk (A)=n\Rightarrow A\xv =\nv \) has only one solution \(\xv =\nv \).
        </p></li>
        <li class="itemize">
        <!--l. 162--><p class="noindent" >\(\rk (A)&gt;n\) is not possible, as the row space of \(A\) is a subspace of \(\FR ^n\).</p></li></ul>
 
</div> 
</div>
<!--l. 169--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">6.4   </span> <a 
 id="x1-490006.4"></a>The Rank-Nullity Theorem for matrices.</h4>
<div class="tcolorbox df" id="tcolobox-175">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 175--><p class="noindent" >Definition 6.12  </p></div> 
<div class="tcolorbox-content"><!--l. 172--><p class="noindent" >Let \(A\) be an \(m\times n\) matrix. The <span 
class="rm-lmsso-12">kernel </span>(also called the <span 
class="rm-lmsso-12">nullspace</span>) of \(A\) is the subspace of \(\FR ^n\) consisting
of the solutions of the system of linear equations
</p>
<div class="math-display" >
<img 
src="F17ZD_main79x.svg" alt="A ⃗x = ⃗0.
" class="math-display"  /></div>
<!--l. 174--><p class="noindent" >Its dimension is called the <span 
class="rm-lmsso-12">nullity </span>of \(A\). </p> 
</div> 
</div>
<div class="tcolorbox thm" id="tcolobox-176">    
<div class="tcolorbox-title">
<!--l. 181--><p class="noindent" >Theorem 6.13  </p></div> 
<div class="tcolorbox-content"><!--l. 179--><p class="noindent" >Let \(A\) be an \(m\times n\) matrix. Then we have:
</p>
<div class="math-display" >
<img 
src="F17ZD_main80x.svg" alt="rk(A ) + dim ker(A ) = n.
" class="math-display"  /></div>
 
</div> 
</div>
<div class="proof">
<!--l. 183--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>Performing Gaussian elimination, we obtain a matrix with exactly \(\rk (A)\) pivots. In particular, \(\rk (A)\) of
the \(n\) variables are pivot variables, while the other \(n-\rk (A)\) are free variables. But we know from previous
results that the rank of a matrix is the number of pivot variables in any row echelon form, while
the nullity of a matrix is the number of free variables in any row echelon form.                     □
</p>
                                                                                          
                                                                                          
</div>
<!--l. 186--><p class="noindent" >We have already seen a rather different looking definition of the rank of a matrix in F17ZB.
</p>
<div class="tcolorbox df" id="tcolobox-177">    
<div class="tcolorbox-title">
<!--l. 198--><p class="noindent" >Definition 6.14  Rank of a matrix</p></div> 
<div class="tcolorbox-content"><!--l. 189--><p class="noindent" >The rank of a matrix \(A\) of size \(m \times n\) is the smallest \(k \leq \min (m,n)\) such that there exist vectors \(\uv _\ell \in \mathbb {R}^m\) and \(\vv _\ell = \mathbb {R}^n\) with \[ A = \sum _{\ell = 1}^k \uv _\ell \vv ^T_\ell . \]
Matrix \(A\) is said to be if full rank of \(k = \min (m,n)\). Since \[ A^T = \sum _{\ell = 1}^k \vv _\ell \uv ^T_\ell \] the rank of \(A\) and \(A^T\) are the same. </p> 
</div> 
</div>
<!--l. 201--><p class="noindent" >Let’s take the same example used there and see how Gaussian Elimination gives the same value for the
rank.
</p>
<!--l. 230--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-178">    
<div class="tcolorbox-title">
<!--l. 230--><p class="noindent" >Example 6.15</p></div> 
<div class="tcolorbox-content"></p><!--l. 204--><p class="noindent" >In F17ZB we considered the matrix \[ A = \begin {pmatrix} 2 &amp; 1 &amp; 0 &amp; 2\\ 3 &amp; 3 &amp; -1 &amp; 3\\ 4 &amp; 5 &amp; -2 &amp; 4\\ 5 &amp; 7 &amp; -3 &amp; 5. \end {pmatrix} \] and stated that this was a rank-2 matrix (not obvious at
all from just looking at it!) since \[ A = \begin {pmatrix} 1 \\ 1\\ 1\\ 1 \end {pmatrix} \begin {pmatrix} 1 &amp; -1 &amp; 1 &amp; 1 \end {pmatrix} + \begin {pmatrix} 1 \\ 2\\3\\ 4 \end {pmatrix} \begin {pmatrix} 1 &amp; 2 &amp; -1 &amp; 1. \end {pmatrix} \] </p> 
</div> 
</div>                                                                           
<!--l. 289--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-179">    
<div class="tcolorbox-title">
<!--l. 289--><p class="noindent" >Example 6.16</p></div> 
<div class="tcolorbox-content"></p><!--l. 232--><p class="noindent" > To determine the <span 
class="rm-lmsso-12">column rank </span>of \[ A= \begin {pmatrix} 2 &amp; 1 &amp; 0 &amp; 2\\ 3 &amp; 3 &amp; -1 &amp; 3\\ 4 &amp; 5 &amp; -2 &amp; 4\\ 5 &amp; 7 &amp; -3 &amp; 5 \end {pmatrix}. \] We row-reduce (row operations do not change linear relations
among the columns): \[ \left (\begin {array}{cccc} 2&amp;1&amp;0&amp;2\\ 3&amp;3&amp;-1&amp;3\\ 4&amp;5&amp;-2&amp;4\\ 5&amp;7&amp;-3&amp;5 \end {array}\right ) \;\xrightarrow {\substack {R_2\leftarrow 2R_2-3R_1\\[2pt] R_3\leftarrow R_3-2R_1\\[2pt] R_4\leftarrow 2R_4-5R_1}}\; \left (\begin {array}{cccc} 2&amp;1&amp;0&amp;2\\ 0&amp;3&amp;-2&amp;0\\ 0&amp;3&amp;-2&amp;0\\ 0&amp;9&amp;-6&amp;0 \end {array}\right ) \;\xrightarrow {\substack {R_3\leftarrow R_3-R_2\\[2pt] R_4\leftarrow R_4-3R_2}}\; \left (\begin {array}{cccc} 2&amp;1&amp;0&amp;2\\ 0&amp;3&amp;-2&amp;0\\ 0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0 \end {array}\right ). \] There are exactly two nonzero rows in echelon form, hence \[ \operatorname {rank}(A)=2. \] The pivot
columns are columns \(1\) and \(2\), so a basis for the column space is given by the corresponding
original columns: \[ \mathcal {B}_{\mathrm {col}(A)}= \left \{ \begin {pmatrix}2\\3\\4\\5\end {pmatrix}, \begin {pmatrix}1\\3\\5\\7\end {pmatrix} \right \}. \] Moreover, the remaining columns are dependent: \[ \mathbf {c}_4=\mathbf {c}_1, \qquad \mathbf {c}_3=\frac 13\,\mathbf {c}_1-\frac 23\,\mathbf {c}_2, \] so all columns lie in the
span of \(\mathbf {c}_1,\mathbf {c}_2\). </p> 
</div> 
</div>                                                                                                          
<!--l. 348--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-180">    
<div class="tcolorbox-title">
<!--l. 348--><p class="noindent" >Example 6.17</p></div> 
<div class="tcolorbox-content"></p><!--l. 292--><p class="noindent" > Let \[ A= \begin {pmatrix} 2 &amp; 1 &amp; 0 &amp; 2\\ 3 &amp; 3 &amp; -1 &amp; 3\\ 4 &amp; 5 &amp; -2 &amp; 4\\ 5 &amp; 7 &amp; -3 &amp; 5 \end {pmatrix}. \] From row-reduction we know that \(rank(A)=2\) and that the pivot columns are the first two columns.
Set \[ \mathbf c_1=\begin {pmatrix}2\\3\\4\\5\end {pmatrix},\qquad \mathbf c_2=\begin {pmatrix}1\\3\\5\\7\end {pmatrix},\qquad C=\begin {pmatrix}\mathbf c_1 &amp; \mathbf c_2\end {pmatrix}\in \mathbb R^{4\times 2}. \] Moreover, the remaining columns are linear combinations of \(\mathbf c_1,\mathbf c_2\): \[ \mathbf c_3=\frac 13\,\mathbf c_1-\frac 23\,\mathbf c_2, \qquad \mathbf c_4=\mathbf c_1. \] Therefore, if we record these
coefficients column-by-column, we obtain the \(2\times 4\) coefficient matrix \[ R= \begin {pmatrix} 1&amp;0&amp;\tfrac 13&amp;1\\[2pt] 0&amp;1&amp;-\tfrac 23&amp;0 \end {pmatrix}. \] Then \(A\) factors as \[ A = C\,R = \begin {pmatrix}\mathbf c_1 &amp; \mathbf c_2\end {pmatrix} \begin {pmatrix} 1&amp;0&amp;\tfrac 13&amp;1\\ 0&amp;1&amp;-\tfrac 23&amp;0 \end {pmatrix}. \]
</p><!--l. 336--><p class="noindent" >Equivalently, writing \(R\) in terms of its rows \[ \mathbf r_1^{\mathsf T}=\begin {pmatrix}1&amp;0&amp;\tfrac 13&amp;1\end {pmatrix}, \qquad \mathbf r_2^{\mathsf T}=\begin {pmatrix}0&amp;1&amp;-\tfrac 23&amp;0\end {pmatrix}, \] we obtain a decomposition of \(A\) as a sum of two outer
products (a sum of rank–\(1\) matrices): \[ A=\mathbf c_1\,\mathbf r_1^{\mathsf T}+\mathbf c_2\,\mathbf r_2^{\mathsf T}. \] Since this is a sum of two rank–\(1\) matrices, it also makes
it transparent that \(rank(A)\le 2\) (and in fact \(rank(A)=2\) because \(\mathbf c_1,\mathbf c_2\) are independent). </p> 
</div> 
</div>                                         
<!--l. 350--><p class="noindent" >The decomposition \[ A=\sum _{k=1}^2 \mathbf u_k\mathbf v_k^{\mathsf T} \] is highly non-unique in general.
</p><!--l. 353--><p class="noindent" >However, if you fix the left vectors to be a specific basis of the column space (e.g., the first two
columns), then the corresponding right vectors are unique (given that choice).
</p>
<div class="tcolorbox exercise" id="tcolobox-181">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 370--><p class="noindent" >Exercise 6.18 </p></div> 
<div class="tcolorbox-content"><!--l. 357--><p class="noindent" >Consider the matrix \[ A= \begin {pmatrix} 1&amp;2&amp;3\\ 1&amp;1&amp;1\\ 2&amp;4&amp;6 \end {pmatrix}. \]
        </p><dl class="enumerate-enumitem"><dt class="enumerate-enumitem">
  (a)  </dt><dd 
class="enumerate-enumitem">Use <span 
class="rm-lmsso-12">row operations </span>to determine a set of <span 
class="rm-lmsso-12">independent columns </span>of \(A\) and hence compute
        \(\mathrm {rank}(A)\).
        </dd><dt class="enumerate-enumitem">
  (b)  </dt><dd 
class="enumerate-enumitem">Compute \(\mathrm {rank}(A)\) by writing \(A\) as a sum of outer products (equivalently, a rank factorisation
        \(A=CR\)).</dd></dl>
 
</div> 
</div>
<!--l. 450--><p class="noindent" >
</p>
<div class="tcolorbox solution" id="tcolobox-182">    
<div class="tcolorbox-title">
<!--l. 450--><p class="noindent" >Solution: </p></div> 
<div class="tcolorbox-content"></p><!--l. 373--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">(a) Row operations to find independent columns. </span>Row-reduce \(A\) to echelon form: \[ \left (\begin {array}{ccc} 1&amp;2&amp;3\\ 1&amp;1&amp;1\\ 2&amp;4&amp;6 \end {array}\right ) \xrightarrow {\substack {R_2\leftarrow R_2-R_1\\[2pt] R_3\leftarrow R_3-2R_1}} \left (\begin {array}{ccc} 1&amp;2&amp;3\\ 0&amp;-1&amp;-2\\ 0&amp;0&amp;0 \end {array}\right ). \]
The leading (pivot) entries occur in columns \(1\) and \(2\). Therefore the <span 
class="rm-lmsso-12">corresponding original</span>
<span 
class="rm-lmsso-12">columns </span>of \(A\) form a basis for the column space: \[ \mathbf c_1= \begin {pmatrix}1\\1\\2\end {pmatrix}, \qquad \mathbf c_2= \begin {pmatrix}2\\1\\4\end {pmatrix}. \] Hence the column rank is the number of
pivot columns: \[ \mathrm {rank}(A)=2. \]
</p><!--l. 403--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">(b) Outer-product / rank factorisation. </span>Let the columns of \(A\) be \(\mathbf c_1,\mathbf c_2,\mathbf c_3\). From the matrix we
have \[ \mathbf c_3= \begin {pmatrix}3\\1\\6\end {pmatrix} = -\,\mathbf c_1 + 2\,\mathbf c_2, \] so every column of \(A\) lies in \(\mathrm {span}\{\mathbf c_1,\mathbf c_2\}\).
</p><!--l. 412--><p class="noindent" >Define \[ C=\begin {pmatrix}\mathbf c_1 &amp; \mathbf c_2\end {pmatrix} = \begin {pmatrix} 1&amp;2\\ 1&amp;1\\ 2&amp;4 \end {pmatrix}, \qquad R= \begin {pmatrix} 1&amp;0&amp;-1\\ 0&amp;1&amp;2 \end {pmatrix}. \] Then the columns of \(CR\) are \[ CR_{(:,1)}=\mathbf c_1,\qquad CR_{(:,2)}=\mathbf c_2,\qquad CR_{(:,3)}=-\mathbf c_1+2\mathbf c_2=\mathbf c_3, \] so \(A=CR\). Writing \(R\) by rows, \[ \mathbf r_1^{\mathsf T}=\begin {pmatrix}1&amp;0&amp;-1\end {pmatrix}, \qquad \mathbf r_2^{\mathsf T}=\begin {pmatrix}0&amp;1&amp;2\end {pmatrix}, \] we obtain the outer-product
decomposition \[ A=\mathbf c_1\,\mathbf r_1^{\mathsf T}+\mathbf c_2\,\mathbf r_2^{\mathsf T}. \] This expresses \(A\) as a sum of two rank–\(1\) matrices, so \(\mathrm {rank}(A)\le 2\). Since \(\mathbf c_1\) and \(\mathbf c_2\) are not
multiples of each other, they are independent, so \(\mathrm {rank}(A)\ge 2\). Therefore, \[ \mathrm {rank}(A)=2. \] </p> 
</div> 
</div>                                
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
<h3 class="sectionHead"><span class="titlemark">7   </span> <a 
 id="x1-500007"></a>Eigenvalues and diagonalisability</h3>
<!--l. 3--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.1   </span> <a 
 id="x1-510007.1"></a>A motivating problem: predicting the dynamics of a population</h4>
<!--l. 5--><p class="noindent" >A simple model to study a population of animals is the following: the population is divided into two age
groups: adults and juveniles. At the start of the observation, the population consists of \(a_0\) adults and \(j_0\)
juveniles. From one year to the next, the adults will each produce on average \(\gamma \) juveniles. Adults will
survive from one year to the next with probability \(\alpha \), while juveniles will survive into the next year and
become adults with the probability \(\beta \).
</p><!--l. 7--><p class="noindent" >We wish to understand the evolution of the population over time: does the population collapse, does
it converge to a stable state? And how does this behaviour depend on the fertility rate \(\gamma \)?
Notice that we have the following equations between the population in two consecutive years:
\begin {equation*}  \begin {aligned} a_{n+1}&amp;=\alpha a_n+\beta j_n~\\ j_{n+1}&amp;=\gamma a_n \end {aligned}  \end {equation*}
This can be rewritten as \begin {equation*}  \vectt {a_{n+1}}{j_{n+1}}=A\vectt {a_{n}}{j_{n}} ~~~ \mbox { with } A:= \left (\begin {array}{cc}\alpha &amp; \beta \\\gamma &amp; 0 \end {array}\right ).  \end {equation*}
In particular, we see by induction that \begin {equation*}  \vectt {a_{n}}{j_{n}}=A^n\vectt {a_{0}}{j_{0}}.  \end {equation*}
</p><!--l. 24--><p class="noindent" >To understand the population at a given time, we thus need to compute powers of the matrix \(A\). This is a
priori a non-trivial problem. There is however one case where computing powers poses no problem: the
case of diagonal matrices.
</p><!--l. 26--><p class="noindent" >A natural strategy would be to try to find a ‘simplest possible basis’ where the matrix \(A\) becomes
diagonal, compute the powers of the matrix in that basis, and go back to the original basis. If we restate
this problem in terms of matrices, we want to find an invertible matrix \(P\) and a diagonal matrix \(D\) such that
\(A= PDP^{-1}\). We have:
</p>
<div class="math-display" >
<img 
src="F17ZD_main81x.svg" alt="  n          −1 n        − 1     − 1     −1      −1       n − 1
A   = (P DP    ) =  PD  P◟-◝◜P◞DP    ⋅⋅⋅P◟-◝◜P◞DP     = P D  P   .
                         =I2            =I2
" class="math-display"  /></div>
                                                                                          
                                                                                          
<!--l. 28--><p class="noindent" >Since \(D^n\) is very easy to compute, it follows that computing \(A^n\) itself becomes much easier to compute,
provided we know how to compute \(P\). This leads to the following questions:
</p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 31--><p class="noindent" >Given a square matrix \(A\), does there always exist an invertible matrix \(P\) such that \(P^{-1}AP\) is diagonal?
      </p></li>
      <li class="itemize">
      <!--l. 32--><p class="noindent" >If so, how to compute such a matrix \(P\)?</p></li></ul>
<!--l. 35--><p class="noindent" >The goal of this chapter is to answer these questions and see applications to various problems.
</p><!--l. 39--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.2   </span> <a 
 id="x1-520007.2"></a>Eigenvectors and Eigenvalues</h4>
<div class="tcolorbox df" id="tcolobox-183">    
<div class="tcolorbox-title">
<!--l. 44--><p class="noindent" >Definition 7.1  </p></div> 
<div class="tcolorbox-content"><!--l. 43--><p class="noindent" >Let \(V\) be an \(n\)-dimensional vector space and let \(T: V \rightarrow V\) be a linear map. Then \(\lambda \in \FR \) is called a (real)
<span 
class="rm-lmsso-12">eigenvalue </span>of \(T\), if there is a vector \(\xv \in V\), \(\xv \neq \nv \), such that
</p>
<div class="math-display" >
<img 
src="F17ZD_main82x.svg" alt="T(⃗x) = λ⃗x.
" class="math-display"  /></div>
<!--l. 43--><p class="noindent" >The vector \(\xv \) is called an <span 
class="rm-lmsso-12">eigenvector </span>of \(T\) for the eigenvalue \(\lambda \). </p> 
</div> 
</div>
<!--l. 46--><p class="noindent" >Recall that an \(n \times n\) matrix \(A\) can be seen as a linear map \(T_A: \FR ^n \rightarrow \FR ^n, \xv \mapsto A\xv \). In particular, the notions of eigenvalues and
eigenvectors are well defined for matrices: \(\lambda \in \FR \) is an <span 
class="rm-lmsso-12">eigenvalue </span>of \(A\), if there is a vector \(\xv \in \FR ^n\), \(\xv \neq \nv \), such that \(A\xv =\lambda \xv \), and the
vector \(\xv \) is called an <span 
class="rm-lmsso-12">eigenvector </span>of \(A\) for the eigenvalue \(\lambda \).
</p>
                                                                                          
                                                                                          
<!--l. 53--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-184">    
<div class="tcolorbox-title">
<!--l. 53--><p class="noindent" >Example 7.2</p></div> 
<div class="tcolorbox-content"></p><!--l. 49--><p class="noindent" ></p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 50--><p class="noindent" >For \(A=I_n\) the identity matrix, every vector \(\xv \neq \nv \) is an eigenvector with eigenvalue 1.<br 
class="newline" />
        </p></li>
        <li class="itemize">
        <!--l. 51--><p class="noindent" >The rotation of angle \(\pi /2\), \(r_{\pi /2}:\FR ^2 \rightarrow \FR ^2\) does not have any eigenvalue, since \(r_{\pi /2}(\xv )\) is never collinear to \(\xv \) (but is
        orthogonal to it and of the same norm).</p></li></ul>
 
</div> 
</div>                                                                                                                      
<!--l. 65--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-185">    
<div class="tcolorbox-title">
<!--l. 65--><p class="noindent" >Example 7.3</p></div> 
<div class="tcolorbox-content"></p><!--l. 56--><p class="noindent" >Consider the following diagonal matrix: \begin {equation*}  D=\left (\begin {array}{cccc} d_1 &amp; 0 &amp; \ldots &amp; 0\\ 0 &amp; d_2 &amp; \ldots &amp; 0 \\ \vdots &amp; &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \ldots &amp; d_n \end {array}\right )~,~~~d_1,\ldots ,d_n\in \FR .  \end {equation*}
Then the eigenvalues of \(D\) are \(d_1,\ldots ,d_n\). Indeed, for a vector \(\xv = (x_1, \ldots , x_n)^T\), we have \(D\xv \) \(= (d_1x_1, \ldots , d_nx_n)^T\). If \(\xv \) is an eigenvector of \(D\) for an
eigenvalue \(\lambda \), then we also have \(D\xv = (\lambda x_1, \ldots , \lambda x_n)\). As one of the \(x_i\) is non-zero (since \(\xv \neq \nv \) ), it follows that \(\lambda = d_i\) for some
\(i\).
</p><!--l. 64--><p class="noindent" >Note that if the \(d_1, \ldots , d_n\) are pairwise distinct, then the eigenvectors associated to \(d_1, \ldots , d_n\) are of the form \((c_1,0,\ldots ,0)^T\), \(\ldots ,\) \((0,\ldots ,0,c_n)^T\)
respectively, where \(c_1,\ldots ,c_n\in \FR \). </p> 
</div> 
</div>                                                                                            
<!--l. 67--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.3   </span> <a 
 id="x1-530007.3"></a>Diagonalisability.</h4>
<div class="tcolorbox df" id="tcolobox-186">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 76--><p class="noindent" >Definition 7.4  </p></div> 
<div class="tcolorbox-content"><!--l. 70--><p class="noindent" >A linear map \(T: V \rightarrow V\) is called <span 
class="rm-lmsso-12">diagonalisable </span>if there exists a basis of eigenvectors.
</p><!--l. 72--><p class="noindent" >Equivalently, \(T\) is diagonalisable if there exists a basis \(\CB \) of \(V\) such that the associated matrix \([T]_\CB \)
is of the form:
</p>
<div class="math-display" >
<img 
src="F17ZD_main83x.svg" alt="              λ1   0  ...  0
               0  λ   ...  0
[T]B = ∖left(  .   2  .     . ∖right).
               ..       ..   ..
               0   0  ...  λn
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 91--><p class="noindent" >
</p><!--l. 91--><p class="noindent" ><a 
 id="x1-53002r5"></a><!--l. 79--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 7.5</span>                                                                           </p><!--l. 79--><p class="noindent" ></p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 80--><p class="noindent" >The rotation \(r_{\pi /4}: \FR ^2 \rightarrow \FR ^2\) is not diagonalisable as it has no (real) eigenvalue.
      </p></li>
      <li class="itemize">
      <!--l. 81--><p class="noindent" >The orthogonal projection \(s_{\pi /4}: \FR ^2 \rightarrow \FR ^2\) is diagonalisable as the matrix of \(s_{\pi /4}\) in the basis \(\CB _{\pi /4}\) is
      </p>
<div class="math-display" >
<img 
src="F17ZD_main84x.svg" alt="                   1  0
[sπ∕4]Bπ∕4 = ∖left( 0  0 ∖right).
" class="math-display"  /></div>                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 91--><p class="noindent" >      </li>
      <li class="itemize">
      <!--l. 85--><p class="noindent" >The map
      </p>
<div class="math-display" >
<img 
src="F17ZD_main85x.svg" alt="                                  ′
T  : Pn (ℝ ) → Pn(ℝ ), P (x) ↦→  xP (x)
" class="math-display"  /></div>
      <!--l. 86--><p class="noindent" >is diagonalisable. Indeed, in the basis \(\CB = (1, x, \ldots , x^n)\), we have
      </p>
<div class="math-display" >
<img 
src="F17ZD_main86x.svg" alt="              0
                 1
[T]  = ∖left(       2         ∖right).
   B                   .
                        ..
                            n
" class="math-display"  /></div>
      </li></ul>
 
</div> 
</div>                                                                                        
</p><!--l. 93--><p class="noindent" >By seeing an \(n \times n\) matrix as a linear map \(T_A:\FR ^n \ra \FR ^n\), we are led to the following definition: </p>
<div class="tcolorbox df" id="tcolobox-187">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 100--><p class="noindent" >Definition 7.6  </p></div> 
<div class="tcolorbox-content"><!--l. 95--><p class="noindent" >An \(n \times n\) matrix \(A\) is <span 
class="rm-lmsso-12">diagonalisable </span>(over \(\FR \)) if there exists a basis of \(\FR ^n\) made of eigenvectors of \(A\).
</p><!--l. 97--><p class="noindent" >Equivalently, \(A\) is diagonalisable if there exists an invertible \(n\times n\) matrix \(P\) such that \(P^{-1}AP\) is of the form:
</p>
<div class="math-display" >
<img 
src="F17ZD_main87x.svg" alt="                  λ1  0   ...  0
                  0   λ2  ...  0
P −1AP  =  ∖left( .       .    .  ∖right).
                  ..        ..  ..
                  0   0   ... λn
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 104--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-540007.3"></a><span 
class="rm-lmssbx-10x-x-120">Remark.</span></span>
The two definitions of diagonalisability for maps and matrices are compatible: a linear map \(T\) is
diagonalisable if and only if the associated matrix \([T]_\CB \) is diagonalisable for some (hence every) basis
\(\CB \).
</p>
<!--l. 107--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-550007.3"></a><span 
class="rm-lmssbx-10x-x-120">Distinct eigenvalues and linear independence.</span></span>
Here is an useful criterion:
</p>
<div class="tcolorbox thm" id="tcolobox-188">    
<div class="tcolorbox-title">
<!--l. 112--><p class="noindent" >Theorem 7.7  </p></div> 
<div class="tcolorbox-content"><!--l. 111--><p class="noindent" >An \(n \times n\) matrix has at most \(n\) eigenvalues. </p> 
</div> 
</div>
<!--l. 115--><p class="noindent" >This theorem is a direct consequence of the following result, together with the fact that a linearly
independent family in an \(n\)-dimensional vector space contains at most \(n\) vectors.
</p>
<div class="tcolorbox thm" id="tcolobox-189">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 120--><p class="noindent" >Theorem 7.8  </p></div> 
<div class="tcolorbox-content"><!--l. 119--><p class="noindent" >Let \(\xv _1,\ldots ,\xv _k\) be eigenvectors of a matrix \(A\) corresponding to distinct eigenvalues \(\lambda _1,\ldots ,\lambda _k\). Then \(\xv _1,\ldots ,\xv _k\) are linearly
independent. </p> 
</div> 
</div>
<div class="proof">
<!--l. 123--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>Assume that we have constants \(c_1,\ldots c_k\) such that \(c_1\uv _1+c_2\uv _2+\ldots +c_k\uv _k=\nv \). Let us denote this equation \((E_1)\). By applying \(A\) on
both sides of the equation, we get \(c_1\lambda _1\uv _1+c_2\lambda _2\uv _2+\ldots +c_k\lambda _k\uv _k=\nv \). Let us denote this equation \((E_2)\). Now by considering \((E_2) - \lambda _1 (E_1)\), we get
the equation \(c_2(\lambda _2-\lambda _1)\uv _2+\ldots +c_k(\lambda _k-\lambda _1)\uv _k=\nv \). We are thus back to an other linear combination of fewer eigenvectors, and we can
now prove the result by induction.                                                                             □
</p>
</div>
<!--l. 127--><p class="noindent" >Here is a useful corollary:
</p>
<div class="tcolorbox thm" id="tcolobox-190">    
<div class="tcolorbox-title">
<!--l. 136--><p class="noindent" >Theorem 7.9  </p></div> 
<div class="tcolorbox-content"><!--l. 134--><p class="noindent" >Let \(V\) be an \(n\)-dimensional vector space, and let \(T: V \ra V\) be a linear map that has exactly \(n\) distinct
real eigenvalues \(\lambda _1,\ldots ,\lambda _n\), then \(T\) is diagonalisable. </p> 
</div> 
</div>
<!--l. 138--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-560007.3"></a><span 
class="rm-lmssbx-10x-x-120">Remark.</span></span>
The previous result is not an equivalence, there exist diagonalisable maps that have fewer
than \(n\) eigenvalues. For instance, the identity map is diagonalisable, but its only eigenvalue is
\(1\).
</p><!--l. 140--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.4   </span> <a 
 id="x1-570007.4"></a>Eigenspaces and diagonalisability.</h4>
<div class="tcolorbox df" id="tcolobox-191">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 145--><p class="noindent" >Definition 7.10  </p></div> 
<div class="tcolorbox-content"><!--l. 143--><p class="noindent" >Given an eigenvalue \(\lambda \in \FR \), the associated <span 
class="rm-lmsso-12">eigenspace </span>is the following subset:
</p>
<div class="math-display" >
<img 
src="F17ZD_main88x.svg" alt="Eλ(T ) := {⃗x ∈ U | T (⃗x) = λ⃗x } = {⃗0} ∪ {⃗x ∈ U | ⃗x is an eigenvector for λ }.
" class="math-display"  /></div>
 
</div> 
</div>
<div class="tcolorbox thm" id="tcolobox-192">    
<div class="tcolorbox-title">
<!--l. 151--><p class="noindent" >Theorem 7.11  </p></div> 
<div class="tcolorbox-content"><!--l. 150--><p class="noindent" >An eigenspace is a subspace of \(V\). </p> 
</div> 
</div>
<div class="proof">
<!--l. 153--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>By  definition,  we  have  that  \(E_\lambda (T) = \ker (T - \lambda Id)\),  and  we  know  that  kernels  of  linear  maps  are  vector
subspaces.                                                                                                          □
</p>
</div>
<!--l. 157--><p class="noindent" >The proof of the following result is a slight variation on the proof of Proposition <a 
href="#x1-55002r7.3">7.3<!--tex4ht:ref: prop:distinct_eigen_lin_ind --></a>:
</p>
<div class="tcolorbox thm" id="tcolobox-193">    
<div class="tcolorbox-title">
<!--l. 161--><p class="noindent" >Theorem 7.12  </p></div> 
<div class="tcolorbox-content"><!--l. 160--><p class="noindent" >Let \(\lambda _1,\ldots ,\lambda _k\) be distinct eigenvalues of a linear map \(T\). Let \(\CB _1, \ldots , \CB _k\) be bases of the eigenspaces \(E_{\lambda _1}(T), \ldots , E_{\lambda _k}(T)\) respectively.
Then \(\CB _1\cup \ldots \cup \CB _k\) is a linearly independent family of vectors. </p> 
</div> 
</div>
<div class="tcolorbox thm" id="tcolobox-194">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 166--><p class="noindent" >Theorem 7.13  </p></div> 
<div class="tcolorbox-content"><!--l. 164--><p class="noindent" >Let \(T:V \rightarrow V\) be a linear map and let \(\lambda _1, \ldots , \lambda _k\) be the distinct eigenvalues of \(T\). Then \(T\) is diagonalisable if and
only if:
</p>
<div class="math-display" >
<img 
src="F17ZD_main89x.svg" alt="dim  Eλ1(T ) + ...+ dim Eλ (T ) = n.
                         k
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 168--><p class="noindent" >The next natural question to answer is: how do we find the eigenvalues of a given linear map or
matrix?
</p><!--l. 173--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.5   </span> <a 
 id="x1-580007.5"></a>Characteristic polynomial</h4>
<!--l. 176--><p class="noindent" >Finding an eigenvalue \(\lambda \) and an associated eigenvector \(\xv \) amounts to finding solutions to the equation \(T(\xv )=\lambda \xv \), or
equivalently \((T-\lambda I_n)\xv =\nv \). Up to choosing coordinates, we can assume that \(T\) is represented by an \(n\times n\) matrix \(A\). The
problem thus amounts to solving a linear system of equations \(A\xv = \lambda \xv \), or equivalently \((A-\lambda I_n)\xv = \nv .\) This is a homogeneous
system of \(n\) equations in \(n\) unknowns. It has a non-trivial solution if and only if \((A-\lambda I_n)\) is not invertible, and this is
equivalent to \(\det (A-\lambda I_n)=0\). This motivates the following definition:
</p>
<div class="tcolorbox df" id="tcolobox-195">    
<div class="tcolorbox-title">
<!--l. 189--><p class="noindent" >Definition 7.14  </p></div> 
<div class="tcolorbox-content"><!--l. 181--><p class="noindent" >Given an \(n\times n\)-matrix \(A\), the expression \begin {equation*}  \chi _A(\lambda ):= \det (A-\lambda I_n)= \det \left (\begin {array}{cccc} a_{11}-\lambda &amp; a_{12} &amp; \ldots &amp; a_{1n}\\ a_{21} &amp; a_{22}-\lambda &amp; \ldots &amp; a_{2n} \\ \vdots &amp; &amp; \ddots &amp; \vdots \\ a_{n1} &amp; a_{n2} &amp; \ldots &amp; a_{nn}-\lambda \end {array}\right )  \end {equation*}
is a polynomial of degree \(n\) in \(\lambda \), called the <span 
class="rm-lmsso-12">characteristic polynomial </span>of the matrix \(A\). </p> 
</div> 
</div>
<!--l. 191--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-590007.5"></a><span 
class="rm-lmssbx-10x-x-120">Remark.</span></span>
In this course, we will mostly compute characteristic polynomials for \(2\times 2\) and \(3\times 3\) matrices. When computing
characteristic polynomials in general, one compute this determinant using the expansion
technique.<br 
class="newline" />
</p><!--l. 194--><p class="noindent" >The definition of the characteristic polynomial is motivated by the following result:
</p>
<div class="tcolorbox thm" id="tcolobox-196">    
<div class="tcolorbox-title">
<!--l. 200--><p class="noindent" >Theorem 7.15  </p></div> 
<div class="tcolorbox-content"><!--l. 198--><p class="noindent" >Given an \(n\times n\)-matrix \(A\), we have:
</p>
<div class="math-display" >
<img 
src="F17ZD_main90x.svg" alt="λ is an eigenvalue of A ⇔ χA (λ) = 0.
" class="math-display"  /></div>
 
</div> 
</div>
<div class="proof">
<!--l. 202--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>We have: \begin {equation*}  \begin {aligned} \lambda \mbox { is an eigenvalue of } A &amp;\Leftrightarrow \exists \xv \neq \nv \mbox { such that } A\xv = \lambda \xv . \\ &amp;\Leftrightarrow \exists \xv \neq \nv \mbox { such that } (A-\lambda I_n)\xv = \nv . \\ &amp;\Leftrightarrow \ker (A-\lambda I_n) \neq \{\nv \}. \\ &amp;\Leftrightarrow A-\lambda I_n \mbox { is not invertible}. \\ &amp;\Leftrightarrow \det (A-\lambda I_n) = 0 \qedhere \\ \end {aligned}  \end {equation*}
                                                                                         □
</p>
</div>
<!--l. 224--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-197">    
<div class="tcolorbox-title">
<!--l. 224--><p class="noindent" >Example 7.16</p></div> 
<div class="tcolorbox-content"></p><!--l. 214--><p class="noindent" > Let us find the eigenvalues of the matrix \begin {equation*}  A=\left (\begin {array}{ccc} 3 &amp; 2 &amp; 4\\ 2 &amp; 0 &amp; 2 \\ 4 &amp; 2 &amp; 3 \end {array}\right )~.  \end {equation*}
The characteristic polynomial of \(A\) reads as (rule of Sarrus) \begin {equation*}  \det \left (\begin {array}{ccc} 3-\lambda &amp; 2 &amp; 4\\ 2 &amp; -\lambda &amp; 2 \\ 4 &amp; 2 &amp; 3-\lambda \end {array}\right )=-(3-\lambda )^2\lambda +16+16+16\lambda -4(3-\lambda )-4(3-\lambda )~,  \end {equation*}
or \(\chi _A(\lambda )=-\lambda ^3+6\lambda ^2+15\lambda +8\). We see that \(\lambda =-1\) is a root of this polynomial and factorise it as \(-(\lambda +1)(\lambda +1)(\lambda -8)\). That is, the eigenvalues of \(A\) are \(-1\) and
\(8\). </p> 
</div> 
</div>                                                                                                                    
<!--l. 226--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-600007.5"></a><span 
class="rm-lmssbx-10x-x-120">The characteristic polynomial seen as a polynomial over</span> \(\FC \)<span 
class="rm-lmssbx-10x-x-120">.</span></span>
                                                                                          
                                                                                          
By the fundamental theorem of algebra, the characteristic polynomial of an \(n\times n\) matrix \(A\) can be factorised
into \(n\) linear factors: \begin {equation*}  \chi _A(\lambda )=\pm (\lambda -\lambda _1)(\lambda -\lambda _2)\ldots (\lambda -\lambda _n)~,  \end {equation*}
where \(\lambda _i\) are (not necessarily distinct) <span 
class="rm-lmsso-12">complex</span>. These \(\lambda _i\) are called the <span 
class="rm-lmsso-12">complex eigenvalues </span>of \(A\), and the
eigenvalues we defined so far are the complex eigenvalues that are real. Since a polynomial of degree \(n \geq 1\)
has at most \(n\) roots, we recover the fact that \(A\) has at most \(n\) complex eigenvalues, hence at most \(n\) real
eigenvalues.
</p>
<!--l. 231--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-610007.5"></a><span 
class="rm-lmssbx-10x-x-120">Remark.</span></span>
A matrix may have no real eigenvalues, e.g. \begin {equation*}  A=\left (\begin {array}{cc} 0 &amp; 1 \\ -1 &amp; 0 \end {array}\right )~,~~~\det \left (\begin {array}{cc} -\lambda &amp; 1 \\ -1 &amp; -\lambda \end {array}\right )=\lambda ^2+1=(\lambda +\di )(\lambda -\di )=\chi _A(\lambda )  \end {equation*}
has a characteristic polynomial \(\chi _A(\lambda )\) without real roots. However, since a polynomial of degree \(\geq 1\) always has at
least one root by the fundamental theorem of algebra, \(A\) always has at least one complex
eigenvalue.<br 
class="newline" />
</p><!--l. 237--><p class="noindent" >There is one simple case where the eigenvalues can be read directly from a matrix:
</p>
<div class="tcolorbox thm" id="tcolobox-198">    
<div class="tcolorbox-title">
<!--l. 245--><p class="noindent" >Theorem 7.17  L</p></div> 
<div class="tcolorbox-content"><!--l. 240--><p class="noindent" >et
</p>
<div class="math-display" >
<img 
src="F17ZD_main91x.svg" alt="            d1  ∗   ⋅⋅⋅  ∗
            0   d2  ...   ...
A =  ∖left( .   .   .       ∖right) ∈ Mn (ℝ )
            ..    ..   ..  ∗
            0   ⋅⋅⋅  0   dn
" class="math-display"  /></div>
<!--l. 242--><p class="noindent" >be an upper-triangular \(n \times n\) matrix. Then the eigenvalues of \(A\) are \(d_1, \ldots , d_n.\)
</p><!--l. 244--><p class="noindent" >The same result holds for lower-triangular matrices. </p> 
</div> 
</div>
<div class="proof">
<!--l. 248--><p class="noindent" ><span class="head">
                                                                                          
                                                                                          
<span 
class="rm-lmsso-12">Proof.</span> </span>\(\chi _A(x) = (d_1-x)\cdots (d_n-x)\), and the result follows from Theorem <a 
href="#x1-59001r7.5">7.5<!--tex4ht:ref: thm:chi_eigen --></a>.                                                         □
</p>
</div>
<!--l. 252--><p class="noindent" >When computing the row echelon form of a matrix via Gaussian elimination, the diagonal entries in the
echelon form are <span 
class="rm-lmssbx-10x-x-120">not </span>the eigenvalues of \(A\) in general.
</p><!--l. 256--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.6   </span> <a 
 id="x1-620007.6"></a>Trace, determinants, and consistency check.</h4>
<div class="tcolorbox df" id="tcolobox-199">    
<div class="tcolorbox-title">
<!--l. 260--><p class="noindent" >Definition 7.18  </p></div> 
<div class="tcolorbox-content"><!--l. 259--><p class="noindent" >The <span 
class="rm-lmsso-12">trace </span>of a square matrix \(A\), denoted tr\((A)\), is the sum of its diagonal coefficients. </p> 
</div> 
</div>
<!--l. 263--><p class="noindent" >We have the following properties: </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 265--><p class="noindent" >The trace defines a linear map \(tr: M_n(\FR ) \ra \FR , A \mapsto \mathrm {tr}(A).\)
      </p></li>
      <li class="itemize">
      <!--l. 266--><p class="noindent" >We have tr\((AB) =\) tr\((BA)\) for every pair of matrices \(A, B \in M_n(\FR )\).</p></li></ul>
<!--l. 270--><p class="noindent" >In particular, we get that for every invertible matrix \(P\), tr\((PAP^{-1})= \) tr\((AP^{-1}P)=\) tr\((A)\). Similarly, we have \(\det (PAP^{-1}) = \det (P)\det (A)\det (P)^{-1} = \det (A).\) In other words, the
trace and the determinant remain the same when changing basis. If \(A\) is diagonalisable, then these
quantities can be computed in terms of the eigenvalues of \(A\) in an appropriate basis. This leads to the
following useful result.
</p>
<div class="tcolorbox thm" id="tcolobox-200">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 276--><p class="noindent" >Theorem 7.19  L</p></div> 
<div class="tcolorbox-content"><!--l. 273--><p class="noindent" >et \(A\) be a diagonalisable \(n \times n\) matrix, and let \(\lambda _1, \ldots , \lambda _n\) be its eigenvalues counted with multiplicity, i.e. the
roots of \(\chi _A\) counted with multiplicity. Then we have
</p>
<div class="math-display" >
<img 
src="F17ZD_main92x.svg" alt="det(A) = λ1 × ...×  λn.
" class="math-display"  /></div>
<!--l. 275--><p class="noindent" >
</p>
<div class="math-display" >
<img 
src="F17ZD_main93x.svg" alt="tr(A ) = λ1 + ...+ λn.
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 278--><p class="noindent" >This result is useful to check your computations, as the trace in particular is very easy to compute. Try
to always use this consistency check!
</p>
<!--l. 285--><p class="noindent" >
</p><!--l. 285--><p class="noindent" ><a 
 id="x1-62003r20"></a><!--l. 281--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 7.20</span>                                                                          </p><!--l. 281--><p class="noindent" >Let us go back to the matrix
</p>
<div class="math-display" >
<img 
src="F17ZD_main94x.svg" alt="            3  2  4
A  = ∖left( 2  0  2 ∖right)
            4  2  3
" class="math-display"  /></div>
<!--l. 283--><p class="noindent" >whose eigenvalues we computed previously: \(-1, -1, 8.\) In particular, we get that the sum of the eigenvalues
is \(6\), which does indeed coincide with tr\((A) = 3+0+3\). Similarly, the product of the eigenvalues is \(8\), which we
recover from Sarrus’ rule. We can thus be reasonably confident that we did not make mistakes
in computing the eigenvalues of \(A\). </p> 
</div> 
</div>                                                        
</p><!--l. 312--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.7   </span> <a 
 id="x1-630007.7"></a>Diagonalisability of a matrix</h4>
<div class="tcolorbox df" id="tcolobox-201">    
<div class="tcolorbox-title">
<!--l. 320--><p class="noindent" >Definition 7.21  </p></div> 
<div class="tcolorbox-content"><!--l. 315--><p class="noindent" >Let \(A \in M_n(\FR )\) and let \(\lambda \) be an eigenvalue of \(A\). </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 317--><p class="noindent" >the <span 
class="rm-lmsso-12">geometric multiplicity </span>of \(\lambda \) is the dimension of the corresponding eigenspace \(E_\lambda (A)\).
        </p></li>
        <li class="itemize">
        <!--l. 318--><p class="noindent" >the <span 
class="rm-lmsso-12">algebraic multiplicity </span>of \(\lambda \) is number of factors \((x-\lambda )\) that appear in \(\chi _A\).</p></li></ul>
 
</div> 
</div>
<!--l. 355--><p class="noindent" >
</p><!--l. 355--><p class="noindent" ><a 
 id="x1-63002r22"></a><!--l. 323--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 7.22</span>                                                                          </p><!--l. 323--><p class="noindent" >Let us consider again the matrix
</p>
<div class="math-display" >
<img 
src="F17ZD_main95x.svg" alt="            3  2  4

A =  ∖left( 2  0  2 ∖right).
            4  2  3
" class="math-display"  /></div>
<!--l. 325--><p class="noindent" >We have \(\chi _A(x) = (8-x)(x+1)^2\), so the eigenvalues of \(A\) are \(-1\) and \(8\), and their algebraic multiplicity are \(2\) and \(1\) respectively.
</p><!--l. 327--><p class="noindent" >Let us compute the dimension of the corresponding eigenspaces using Gaussian elimination. For
the eigenvalue \(8\), we obtain the system:
</p>
<div class="math-display" >
<img 
src="F17ZD_main96x.svg" alt="       3x + 2y + 4z  =   8x                           2x − 8y + 2z   =   0
∖lef t{   2x + 2z     =   8y  ∖right.     ⇝    ∖left{ − 5x + 2y + 4z  =   0 ∖right.

       4x + 2y + 3z  =   8z                           4x + 2y − 5z   =   0
" class="math-display"  /></div>
<!--l. 333--><p class="noindent" >We use Gaussian elimination: \begin {equation*}  \left (\begin {array}{ccc|c} 1 &amp; -4 &amp; 1 &amp; 0 \\ -5 &amp; 2 &amp; 4 &amp; 0 \\ 4 &amp; 2 &amp; -5 &amp; 0\end {array}\right )~~~ \rightsquigarrow ~~~\left (\begin {array}{ccc|c} 1 &amp; -4 &amp; 1 &amp; 0 \\ 0 &amp; -18 &amp; 9 &amp; 0 \\ 0 &amp; 0 &amp; 0&amp; 0\end {array}\right )~.  \end {equation*}
The row echelon form has exactly one free variable, so the dimension of the corresponding eigenspace
is \(1\), so the geometric dimension of the eigenvalue \(\lambda =8\) is \(1\).
</p><!--l. 343--><p class="noindent" >For the eigenvalue \(\lambda =-1\), we get the system of equations
</p>
<div class="math-display" >
<img 
src="F17ZD_main97x.svg" alt="       3x + 2y + 4z  =   − x                     4x + 2y + 4z  =   0
∖left{   2x +  2z    =   − y ∖right.⇝     ∖lef t{ 2x + y + 2z   =   0 ∖right.
       4x + 2y + 3z  =   − z                     4x + 2y + 4z  =   0
" class="math-display"  /></div>
<!--l. 351--><p class="noindent" >All of these equations are multiples of \(2x+y+2z=0\), so the system boils down to the equation \(2x+y+2z=0\). This system has
exactly two free variables (\(y\) and \(z\)), and so the dimension of the corresponding eigenspace is \(2\). In other
words, the geometric multiplicity of the eigenvalue \(\lambda = -1\) is \(2\).                                     
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 355--><p class="noindent" ></p><!--l. 353--><p class="noindent" >In particular, we see in this particular example that geometric and algebraic multiplicity coincide.
</p> 
</div> 
</div>                                                                                        
</p>
<!--l. 371--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-202">    
<div class="tcolorbox-title">
<!--l. 371--><p class="noindent" >Example 7.23</p></div> 
<div class="tcolorbox-content"></p><!--l. 358--><p class="noindent" >Consider the following matrix:
</p>
<div class="math-display" >
<img 
src="F17ZD_main98x.svg" alt="           1  1  1
A = ∖lef t( 0  1  1 ∖right).

           0  0  1
" class="math-display"  /></div>
<!--l. 362--><p class="noindent" >Since \(A\) is upper-triangular, we have \(\chi _A(x)= (1-x)^3.\) Thus, \(\lambda =1\) is the unique eigenvalue of \(A\) and its algebraic
multiplicity is \(3\).
</p><!--l. 364--><p class="noindent" >We find the geometric multiplicity by solving the system \((A-1\times I_3)\xv = \nv ,\) which yields the system:
</p>
<div class="math-display" >
<img 
src="F17ZD_main99x.svg" alt="               |
       0  1  1 |0
∖lef t( 0  0  1 |0 ∖right ).
       0  0  0 |0
               |
" class="math-display"  /></div>
<!--l. 368--><p class="noindent" >This system is already in row echelon form and has exactly one free variable. So the geometric
multiplicity of \(\lambda =1\) is \(1\).
</p><!--l. 370--><p class="noindent" >In this particular example, the algebraic and geometric multiplicity differ. </p> 
</div> 
</div>                          
                                                                                          
                                                                                          
<!--l. 374--><p class="noindent" >For every eigenvalue of \(A\), the geometric multiplicity is less than or equal to the algebraic
multiplicity.
</p>
<div class="tcolorbox thm" id="tcolobox-203">    
<div class="tcolorbox-title">
<!--l. 387--><p class="noindent" >Theorem 7.24  </p></div> 
<div class="tcolorbox-content"><!--l. 382--><p class="noindent" >An \(n\times n\) matrix \(A\) is diagonalisable (over \(\FR \)) if and only if: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 384--><p class="noindent" >all the complex roots of \(\chi _A\) are real numbers.
        </p></li>
        <li class="itemize">
        <!--l. 385--><p class="noindent" >for every eigenvalue \(\lambda \) of \(A\), the algebraic multiplicity of \(\lambda \) equals the geometric multiplicity
        of \(\lambda \).</p></li></ul>
 
</div> 
</div>
<!--l. 403--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-204">    
<div class="tcolorbox-title">
<!--l. 403--><p class="noindent" >Example 7.25</p></div> 
<div class="tcolorbox-content"></p><!--l. 389--><p class="noindent" >Here are examples of matrices which are not diagonalisable: \begin {equation*}  A=\left (\begin {array}{cc} 0 &amp; 1 \\ -1 &amp; 0 \end {array} \right )~,~~~ B=\left (\begin {array}{cc} 1 &amp; 1 \\ 0 &amp; 1 \end {array} \right )~.  \end {equation*}
We have \(\chi _A(x) = x^2 +1\), so the matrix \(A\) has no real eigenvalue (but two complex eigenvalues: \(i\) and \(-i\)), and therefore
cannot be diagonalised as a real matrix.
</p><!--l. 402--><p class="noindent" >We have \(\chi _B(x) = (x-1)^2\), so the only eigenvalue of the matrix \(B\) is \(1\) (note that \(B\) is upper triangular), which has
algebraic multiplicity \(2\), but the associated eigenspace has dimension \(1\), so in particular the
algebraic and geometric multiplicity do not coincide. Thus, \(B\) is not diagonalisable over \(\FR \).
</p> 
</div> 
</div>                                                                                                                      
<!--l. 438--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-205">    
<div class="tcolorbox-title">
<!--l. 438--><p class="noindent" >Example 7.26</p></div> 
<div class="tcolorbox-content"></p><!--l. 405--><p class="noindent" >Let us continue the study of the matrix \(A=\left (\begin {array}{ccc} 3 &amp; 2 &amp; 4\\ 2 &amp; 0 &amp; 2 \\ 4 &amp; 2 &amp; 3 \end {array}\right ).\) We already know that \(\chi _A(x) = (8-x)(x+1)^2\), so the eigenvalues of \(A\) are \(-1\) and
\(8\), and we showed in previous examples that the algebraic and geometric multiplicity coincide for
each eigenvalue. By the previous theorem, \(A\) is diagonalisable. </p> 
</div> 
</div>                                         
<!--l. 440--><p class="noindent" >We have the following method to determine whether a given matrix is diagonalisable (over \(\FR \)):
</p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 442--><p class="noindent" >Compute the characteristic polynomial \(\chi _A\) of \(A\) and compute its roots. For \(n&gt;3\), you will generally
      be given some eigenvalues to help you with the computations.
      </p></li>
      <li class="itemize">
                                                                                          
                                                                                          
      <!--l. 443--><p class="noindent" >If \(\chi _A\) possesses a non-real complex root, then \(A\) is not diagonalisable over \(\FR \).
      </p></li>
      <li class="itemize">
      <!--l. 444--><p class="noindent" >Otherwise, let \(\lambda _1, \ldots , \lambda _k\) be the (real) roots of \(\chi _A\). Compute the algebraic multiplicity of each eigenvalue
      by factorising \(\chi _A\).
      </p></li>
      <li class="itemize">
      <!--l. 445--><p class="noindent" >For each eigenvalue \(\lambda _i\), compute the geometric multiplicity of \(\lambda _i\) by computing the dimension
      of the solution space of the system \(A\xv - \lambda _i\xv = \nv \) using Gaussian elimination.
      </p></li>
      <li class="itemize">
      <!--l. 446--><p class="noindent" >If for some eigenvalue \(\lambda \), the geometric and algebraic multiplicities of \(\lambda \) differ, then \(A\) is not
      diagonalisable.
      </p></li>
      <li class="itemize">
      <!--l. 447--><p class="noindent" >To find a basis of eigenvectors, compute for each eigenvalue \(\lambda _i\) a basis \(\cB _i\) of the corresponding
      eigenspace, using Gaussian elimination. A basis of eigenvectors is then given by \(\CB _1 \cup \ldots \cup \CB _k\).</p></li></ul>
<!--l. 452--><p class="noindent" >In the previous theorem, it is enough to check that algebraic and geometric multiplicity coincide for
eigenvalues of algebraic multiplicity at least \(2\), as the algebraic and geometric multiplicity necessarily
coincide for eigenvalues of algebraic multiplicity \(1\)
</p>
<!--l. 494--><p class="noindent" >
</p><!--l. 494--><p class="noindent" ><a 
 id="x1-63007r27"></a><!--l. 454--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 7.27</span>                                                                          </p><!--l. 454--><p class="noindent" >Let us finish the study of the matrix  \(A=\left (\begin {array}{ccc} 3 &amp; 2 &amp; 4\\ 2 &amp; 0 &amp; 2 \\ 4 &amp; 2 &amp; 3 \end {array}\right ).\) We already know that  \(\chi _A(x) = (8-x)(x+1)^2\), so the eigenvalues of  \(A\) are  \(-1\) and  \(8\),
and we showed in previous examples that the algebraic and geometric multiplicity coincide for
each eigenvalue. By the previous theorem, \(A\) is diagonalisable.
</p><!--l. 456--><p class="noindent" >Let us compute a basis of eigenvectors. For the eigenvalue \(8\), we solve the system:
</p>
<div class="math-display" >
<img 
src="F17ZD_main100x.svg" alt="       3x + 2y + 4z  =   8x                           2x − 8y + 2z   =   0

∖lef t{   2x + 2z     =   8y  ∖right.     ⇝    ∖left{ − 5x + 2y + 4z  =   0 ∖right.
       4x + 2y + 3z  =   8z                           4x + 2y − 5z   =   0
" class="math-display"  /></div>
<!--l. 462--><p class="noindent" >We use Gaussian elimination: \begin {equation*}  {\tiny \left (\begin {array}{ccc|c} 1 &amp; -4 &amp; 1 &amp; 0 \\ -5 &amp; 2 &amp; 4 &amp; 0 \\ 4 &amp; 2 &amp; -5 &amp; 0\end {array}\right )~~~ \melt {R_2\rightarrow R_2+5R_1\\R_3\rightarrow R_3-4 R_1} ~~~\left (\begin {array}{ccc|c} 1 &amp; -4 &amp; 1 &amp; 0 \\ 0 &amp; -18 &amp; 9 &amp; 0 \\ 0 &amp; 18 &amp; -9 &amp; 0\end {array}\right ) ~~\melt {R_3\rightarrow R_2+R_3\\R_2\rightarrow R_2/9}~~~\left (\begin {array}{ccc|c} 1 &amp; -4 &amp; 1 &amp; 0 \\ 0 &amp; -2 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0\end {array}\right ) }  \end {equation*}
By substitution, the solutions of this homogeneous system are \(\xv =(\alpha ,\tfrac {1}{2}\alpha ,\alpha )^T\), so a basis for \(E_8(A)\) is \(\CB _8 = \{(1,\tfrac {1}{2},1)^T\}\).
</p><!--l. 474--><p class="noindent" >For the eigenvalue \(\lambda =-1\), we get the system of equations
</p>
<div class="math-display" >
<img 
src="F17ZD_main101x.svg" alt="        3x + 2y + 4z  =   − x
∖lef t{    2x + 2z     =   − y ∖right. ⇝    ∖left{ 2x + y + 2z   =  0 ∖right.
        4x + 2y + 3z  =   − z
" class="math-display"  /></div>
<!--l. 482--><p class="noindent" >By substitution, we find that the solution to this system of linear equations is of the form\((-\tfrac {1}{2}(2\alpha +\beta ),\beta ,\alpha )^T\) for \(\alpha , \beta \in \FR \). Thus, a
basis of \(E_{-1}(A)\) is given by \(\CB _{-1} = \{(-1,0,1)^T,(-\tfrac {1}{2},1,0)^T\}\). Finally, a basis of eigenvectors is \(\CB = \{ (1,\tfrac {1}{2},1)^T, (-1,0,1)^T,(-\tfrac {1}{2},1,0)^T\}.\)
</p><!--l. 486--><p class="noindent" >Seeing \(A\) as a linear map \(T_A: \FR ^3 \ra \FR ^3\), let us perform a change of basis. Let
</p>
<div class="math-display" >
<img 
src="F17ZD_main102x.svg" alt="             1   − 1  − 1
              1         2
P := ∖left( − 2   0    1  ∖right).
             1    1    0
" class="math-display"  /></div>
<!--l. 488--><p class="noindent" >be the matrix of change of basis from the standard basis to our basis of eigenvectors. In particular, the
matrix of \(T_A\) in the basis of eigenvectors \(\CB \) is:                                                 
                                                                                          
                                                                                          
</p><!--l. 494--><p class="noindent" ></p>
<div class="math-display" >
<img 
src="F17ZD_main103x.svg" alt="  −1              8   0   0
P   AP  =  ∖left( 0  − 1  0  ∖right ).
                  0   0  − 1
" class="math-display"  /></div>
 
</div> 
</div>                                                                                        
</p><!--l. 518--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.8   </span> <a 
 id="x1-640007.8"></a>Application: Dynamical systems</h4>
<div class="tcolorbox df" id="tcolobox-206">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 542--><p class="noindent" >Definition 7.28  </p></div> 
<div class="tcolorbox-content"><!--l. 538--><p class="noindent" >The <span 
class="rm-lmsso-12">dynamical system </span>associated to an \(n \times n\) matrix \(A\) and a choice of initial condition \(\xv _0 \in \FR ^n\) is the
sequence \((\xv _k)_{k \geq 0} \) of vectors of \(\FR ^n\) defined by:
</p>
<div class="math-display" >
<img 
src="F17ZD_main104x.svg" alt="⃗xk+1 = A ⃗xk.
" class="math-display"  /></div>
<!--l. 540--><p class="noindent" >In particular, we get by induction that:
</p>
<div class="math-display" >
<img 
src="F17ZD_main105x.svg" alt="       k
⃗xk = A  ⃗x0.
" class="math-display"  /></div>
 
</div> 
</div>
<div class="tcolorbox thm" id="tcolobox-207">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 549--><p class="noindent" >Theorem 7.29  </p></div> 
<div class="tcolorbox-content"><!--l. 545--><p class="noindent" >Let \(A\) be a diagonalisable \(n \times n\) matrix with a chosen basis of eigenvectors \(\ev _1, \ldots , \ev _n\) and corresponding
eigenvalues \(\lambda _1, \ldots , \lambda _n\). Let \((\xv _{k})\) be the dynamical system associated to a given initial condition \(\xv _0\in \FR ^n\). Let \(\alpha _1, \ldots , \alpha _k\in \FR \)
such that
</p>
<div class="math-display" >
<img 
src="F17ZD_main106x.svg" alt="⃗x =  α ⃗e  + ...+ α  ⃗e .
 0    1 1          n n
" class="math-display"  /></div>
<!--l. 547--><p class="noindent" >Then we have for every \(k \geq 0\),
</p>
<div class="math-display" >
<img 
src="F17ZD_main107x.svg" alt="       k         k              k
⃗xk =  A ⃗x0 =  α1λ1⃗e1 + ...+ αk λn⃗en.
" class="math-display"  /></div>
 
</div> 
</div>
<!--l. 551--><p class="noindent" >To determine the asymptotic behaviour (convergence, divergence, possible limit) of a dynamical system
associated to a \(n\times n\) diagonalisable matrix \(A\): </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 553--><p class="noindent" >Compute a basis of eigenvectors \(\ev _1, \ldots , \ev _n\) of \(A\), with corresponding eigenvalues \(\lambda _1, \ldots , \lambda _n\), as in the previous
      method.
      </p></li>
      <li class="itemize">
      <!--l. 554--><p class="noindent" >Using Gaussian elimination, find constants \(\alpha _1, \ldots , \alpha _n\) such that
                                                                                          
                                                                                          
      </p>
<div class="math-display" >
<img 
src="F17ZD_main108x.svg" alt="⃗x0 =  α1⃗e1 + ...+ αn⃗en.
" class="math-display"  /></div>
      </li>
      <li class="itemize">
      <!--l. 556--><p class="noindent" >We then have
      </p>
<div class="math-display" >
<img 
src="F17ZD_main109x.svg" alt="⃗xk = Ak ⃗x0 = α1 λk1⃗e1 + ...+ αkλkn⃗en.
" class="math-display"  /></div>
      <!--l. 557--><p class="noindent" >This formula can be used to determine the asymptotic behaviour of the dynamical system.</p></li></ul>
<!--l. 560--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.9   </span> <a 
 id="x1-650007.9"></a>Application: Dynamics of populations.</h4>
<!--l. 562--><p class="noindent" >Let us go back to the problem of understanding the dynamics of some animal population. We have:
\begin {equation*}  \vectt {a_{n+1}}{j_{n+1}}=A\vectt {a_{n}}{j_{n}} ~~~ \mbox { with } A:= \left (\begin {array}{cc}\alpha &amp; \beta \\\gamma &amp; 0 \end {array}\right ).  \end {equation*}
For simplicity and concreteness, assume that \(\alpha =\frac {1}{2}\), \(\beta =\frac {3}{10}\). The characteristic polynomial of \(A\) is:
</p>
<div class="math-display" >
<img 
src="F17ZD_main110x.svg" alt="χ  (x ) = (1-− x)(− x) − 3-γ =  x2 − 1x − -3-γ.
 A        2             10          2    10
" class="math-display"  /></div>
<!--l. 577--><p class="noindent" >The eigenvalues of \(A\) are \(\lambda _1=\frac {1}{20}(5-\sqrt {5}\sqrt {5+24\gamma })\) and \(\lambda _2=\frac {1}{20}(5+\sqrt {5}\sqrt {5+24\gamma })\). In particular, \(A\) has two distinct eigenvalues \(\lambda _1 &lt; \lambda _2\), hence it is diagonalisable by
Theorem <a 
href="#x1-55003r7.3">7.3<!--tex4ht:ref: thm:distinct_DZ --></a>. We can thus find a basis \(\ev _1, \ev _2\) of eigenvectors for \(\FR ^2\). If we write the initial vector in this basis
as:
</p>
<div class="math-display" >
<img 
src="F17ZD_main111x.svg" alt="∖left( a0 ∖right) = α1⃗e1 + α2⃗e2,
       j0
" class="math-display"  /></div>
<!--l. 578--><p class="noindent" >we get
</p>
<div class="math-display" >
<img 
src="F17ZD_main112x.svg" alt="       an             n       a0               n        n
∖left( j  ∖right) = A  ∖left( j  ∖right) = α1λ 1⃗e1 + α2 λ2⃗e2.
        n                      0
" class="math-display"  /></div>
<!--l. 579--><p class="noindent" >We thus see that the dynamics of this population depend on the eigenvalues, and hence on the fecundity
rate \(\gamma \): </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 581--><p class="noindent" >If \(\lambda _1, \lambda _2 &lt; 1\), that is, if \(\gamma &lt; \frac {5}{3}\), then \(a_n, j_n \ra 0\) and the population collapses.
      </p></li>
      <li class="itemize">
      <!--l. 583--><p class="noindent" >If \(\lambda _2 &gt;1\), that is, if \(\gamma &gt; \frac {5}{3}\), then \(a_n, j_n \ra \infty \) and the population explodes.
                                                                                          
                                                                                          
      </p></li>
      <li class="itemize">
      <!--l. 584--><p class="noindent" >Finally, if \(\lambda _2 = 1\) and \(\lambda _1 &lt;1\), that is, if \(\gamma =\frac {5}{3}\), then
      </p>
<div class="math-display" >
<img 
src="F17ZD_main113x.svg" alt="                                      3
∖left( an ∖right ) → α2⃗e2 = α2 ∖lef t( 5 ∖right).
       jn                             1
" class="math-display"  /></div>
      <!--l. 586--><p class="noindent" >In particular, the population converges to a stable population where the number \(a, j\) of adults
      and juveniles satisfy:
      </p>
<div class="math-display" >
<img 
src="F17ZD_main114x.svg" alt="     3
a =  -j.
     5
" class="math-display"  /></div>
      </li></ul>
<!--l. 594--><p class="noindent" >This mathematical model to understand the evolution of a population by dividing it into several age
groups is known as the <span 
class="rm-lmsso-12">Leslie model</span>.
</p>
<!--l. 598--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-660007.9"></a><span 
class="rm-lmssbx-10x-x-120">Application: The Fibonacci sequence.</span></span>
Consider the sequence satisfying the following recurrence relation:
                                                                                          
                                                                                          
</p>
<div class="math-display" >
<img 
src="F17ZD_main115x.svg" alt="an+1 =  an + an−1,
" class="math-display"  /></div>
<!--l. 600--><p class="noindent" >and with the initial values \(a_0=0\) and \(a_1=1\). The numbers \(a_i\) are known as the <span 
class="rm-lmsso-12">Fibonacci numbers</span>: \(0,1,1,2,3,5,8,13,\ldots \). Let us introduce
the following vector of \(\FR ^2\):
</p>
<div class="math-display" >
<img 
src="F17ZD_main116x.svg" alt="⃗xn := ∖lef t( an+1  ∖right)  for n ≥ 0.
              an
" class="math-display"  /></div>
<!--l. 602--><p class="noindent" >The recurrence relation gets encoded into the following matrix equation: \begin {equation*}  \xv _{n+1}=A\xv _n  \end {equation*}
with \begin {equation*}  A:=\left (\begin {array}{cc}1 &amp; 1\\1 &amp; 0 \end {array}\right ).  \end {equation*}
To compute \(a_{n+1}\) from given \(a_1\) and \(a_0\), we just have to apply \(A^n\) to the initial vector. The characteristic polynomial \(\chi _A\)
is
</p>
<div class="math-display" >
<img 
src="F17ZD_main117x.svg" alt="χA(x ) = (1 − x )(− x) − 1 = x2 − x − 1,
" class="math-display"  /></div>
<!--l. 612--><p class="noindent" >and its eigenvalues are \(\varphi := \frac {1 + \sqrt {5}}{2}\) and \(\varphi ':= \frac {1 - \sqrt {5}}{2}\). In particular, the \(2\times 2\) matrix \(A\) has two distinct eigenvalues, hence it is
diagonalisable by Theorem <a 
href="#x1-55003r7.3">7.3<!--tex4ht:ref: thm:distinct_DZ --></a>. We compute the corresponding eigenvectors \(\ev _\varphi =(\frac {1}{2}(1+\sqrt {5}),1)^T\) and \(\ev _{\varphi '}=(\frac {1}{2}(1-\sqrt {5}),1)^T\) . One
then writes the initial vector \(\xv _0\) in this basis of eigenvectors by solving the system of linear
                                                                                          
                                                                                          
equations
</p>
<div class="math-display" >
<img 
src="F17ZD_main118x.svg" alt="⃗x0 = a⃗eφ + b⃗eφ′,
" class="math-display"  /></div>
<!--l. 613--><p class="noindent" >for some \(a, b \in \FR \), using Gaussian elimination. One finds: \begin {equation*}  \xv _0 = \vectt {1}{0}=\frac {1}{\sqrt {5}}\ev _\varphi -\frac {1}{\sqrt {5}}\ev _{\varphi '}~.  \end {equation*}
If we apply \(A^n\) to our initial vector, we obtain \begin {equation*}  \xv _n = A^n\vectt {1}{0}=\frac {1}{\sqrt {5}}\varphi ^n\ev _\varphi - \frac {1}{\sqrt {5}}(\varphi ')^n\ev _{\varphi '}.  \end {equation*}
and by taking the second component of \(\xv _n\), one gets
</p>
<div class="math-display" >
<img 
src="F17ZD_main119x.svg" alt="       n     ′n
an = φ--−√(φ-)-.
           5
" class="math-display"  /></div>
<!--l. 637--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-670007.9"></a><span 
class="rm-lmssbx-10x-x-120">Sequences satisfying a linear recurrence relation.</span></span>
More generally, for a sequence \((a_n)\) satisfying a linear recurrence relation of the form
</p>
<div class="math-display" >
<img 
src="F17ZD_main120x.svg" alt="an+m  = cm− 1an+m − 1 + ...+ c0an
                                                                                          
                                                                                          
" class="math-display"  /></div>
<!--l. 639--><p class="noindent" >for some constants \(c_0, \ldots , c_m\in \FR \), we can compute \(a_n\) using the same procedure. First introduce the following vector of
\(\FR ^m\):
</p>
<div class="math-display" >
<img 
src="F17ZD_main121x.svg" alt="               an
⃗xn =  ∖left(    ...    ∖right)  for n ≥ 0,

             an+m− 1
" class="math-display"  /></div>
<!--l. 641--><p class="noindent" >And notice that we have
</p>
<div class="math-display" >
<img 
src="F17ZD_main122x.svg" alt="⃗xn+1 =  A⃗xn
" class="math-display"  /></div>
<!--l. 643--><p class="noindent" >with
</p>
<div class="math-display" >
<img 
src="F17ZD_main123x.svg" alt="           cm −1  cm−2  ...  c1  c0
             1      0   ...   0   0
             0      1    0   ...  0
A = ∖lef t(  .          .    .    . ∖right).
             ..           ..   ..  ..
             0     ...   0    1   0
" class="math-display"  /></div>
<!--l. 646--><p class="noindent" >One then compute the characteristic polynomial \(\chi _A\). One can show that
</p>
<div class="math-display" >
<img 
src="F17ZD_main124x.svg" alt="          m         m−1
χA(x) = x   − cm− 1x    − ...−  c1x −  c0.
" class="math-display"  /></div>
<!--l. 648--><p class="noindent" >One can then compute the eigenvalues and eigenvectors. If \(A\) turns out to be diagonalisable, with a basis \(\CB \)
of eigenvectors \(\ev _1, \ldots , \ev _m\) and associated eigenvalues \(\lambda _1, \ldots , \lambda _m\) respectively, one then express the initial vector \(\xv _0\) in the basis \(\CB \)
by solving the associated system of linear equations. If
</p>
<div class="math-display" >
<img 
src="F17ZD_main125x.svg" alt="⃗x0 = α1⃗e1 + ...+  αm⃗em,
" class="math-display"  /></div>
<!--l. 648--><p class="noindent" >then it follows that
</p>
                                                                                          
                                                                                          
<div class="math-display" >
<img 
src="F17ZD_main126x.svg" alt="        n        n              n
⃗xn = A  ⃗x0 = α1λ1⃗e1 + ...+ αnλm ⃗em,
" class="math-display"  /></div>
<!--l. 649--><p class="noindent" >and one finally obtains \(a_n\) by taking the first component of \(\xv _n\).
</p>
<!--l. 652--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-680007.9"></a><span 
class="rm-lmssbx-10x-x-120">Diagonalisability over</span> \(\FC \) <span 
class="rm-lmssbx-10x-x-120">and dynamical systems.</span></span>
In this course, we have mostly been dealing with real vector spaces and diagonalisability over \(\FR \). When it
comes to diagonalisability however, using complex numbers instead can be very useful: A matrix may be
diagonalisable over \(\FC \) (in which case the previous methods apply) even though the same matrix is not
diagonalisable over \(\FR \).
</p><!--l. 655--><p class="noindent" >Here is a concrete example. Consider a sequence satisfying the following linear recurrence
relation:
</p><!--l. 657--><p class="noindent" >
</p>
<div class="math-display" >
<img 
src="F17ZD_main127x.svg" alt="un+2 =  − un+1 − un.
" class="math-display"  /></div>
<!--l. 659--><p class="noindent" >By applying the previous method, we end up studying a dynamical system associated to a \(2\times 2\) matrix whose
characteristic polynomial is \(x^2 + x +1\). This polynomial does not admit any real root, so the associated matrix is
not diagonalisable <span 
class="rm-lmsso-12">over</span> \(\FR \). However, over \(\FC \), the characteristic polynomial has two roots, \(\lambda _1 = e^{2i\pi /3}\) and \(\lambda _2 = e^{- 2i\pi /3}.\) One can
show that the associated matrix is diagonalisable over \(\FC \), and as a result, there exists constants \(\alpha _1, \alpha _2 \in \FC \) such that
for every \(n \geq 0\), we have
</p>
                                                                                          
                                                                                          
<div class="math-display" >
<img 
src="F17ZD_main128x.svg" alt="         n      n
un = α1λ 1 + α2λ2.
" class="math-display"  /></div>
<!--l. 662--><p class="noindent" >We can then compute the values of \(\alpha _1, \alpha _2\) using the initial conditions \(u_0, u_1.\) For instance, for \(u_0 = 2\) and \(u_1= 1\), we find \(\alpha _1 = \alpha _2 = 1\), and
finally
</p>
<div class="math-display" >
<img 
src="F17ZD_main129x.svg" alt="u  = e2inπ∕3 + e− 2inπ∕3 = 2cos(2nπ ∕3).
 n
" class="math-display"  /></div>
<!--l. 666--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">7.10   </span> <a 
 id="x1-690007.10"></a>Powers of a matrix.</h4>
<!--l. 666--><p class="noindent" >Diagonalising a matrix helps to compute its powers. Assume that we have diagonalised an \(n\times n\) matrix \(A\): \(A=PDP^{-1}\),
where \(D\) is a diagonal matrix with entries \(d_1,\ldots ,d_n\). We then have \begin {equation*}  A^k=(PDP^{-1})^k=PDP^{-1}PDP^{-1}\ldots PDP^{-1}=PD^kP^{-1}~.  \end {equation*}
Here, \(D^k\) is simply the diagonal matrix with entries \(d_1^k,\ldots ,d_n^k\).
</p><!--l. 674--><p class="noindent" >We have the following algorithm to compute the powers of a diagonalisable matrix \(A\): </p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 676--><p class="noindent" >Using the previous method, compute a diagonal matrix \(D\) and an invertible matrix \(P\) such that
      \(A = PDP^{-1}\).
      </p></li>
      <li class="itemize">
      <!--l. 677--><p class="noindent" >We then have \(A^k = PD^kP^{-1}\) for every \(k \in \bbZ \).</p></li></ul>
<!--l. 735--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="tcolorbox example" id="tcolobox-208">    
<div class="tcolorbox-title">
<!--l. 735--><p class="noindent" >Example 7.30</p></div> 
<div class="tcolorbox-content"></p><!--l. 681--><p class="noindent" >Let \begin {equation*}  A=\left (\begin {array}{cc} 2 &amp; 2 \\ 1 &amp; 3 \end {array} \right )~.  \end {equation*}
Let us compute \(A^{20}\). <br 
class="newline" />We compute the characteristic polynomial \(\chi _A(\lambda ) = \lambda ^2 - 5\lambda + 4\), and so the eigenvalues of \(A\) are \(\lambda =1\) and \(\lambda =4\). In particular, \(A\) is a \(2\times 2\)
matrix with two distinct eigenvalues so it is diagonalisable. Using Gaussian elimination, we find that a
basis of \(E_1(A)\) is the vector \((2,-1)^T\) and a basis of \(E_4(A)\) is the vector \((1,1)^T\). We thus define the matrix of change of
basis
</p>
<div class="math-display" >
<img 
src="F17ZD_main130x.svg" alt="P  = ∖left(  2   1 ∖right)
            − 1  1
" class="math-display"  /></div>
<!--l. 695--><p class="noindent" >whose inverse we compute by Gaussian elimination:
</p>
<div class="math-display" >
<img 
src="F17ZD_main131x.svg" alt="  −1   1        1  − 1
P    = --∖left( 1   2  ∖right).
       3
" class="math-display"  /></div>
<!--l. 700--><p class="noindent" >We thus have \begin {equation*}  A= \left (\begin {array}{cc} 2 &amp; 2 \\ 1 &amp; 3 \end {array} \right )=PDP^{-1}=\left (\begin {array}{cc} 2 &amp; 1 \\ -1 &amp; 1 \end {array} \right )\left (\begin {array}{cc} 1 &amp; 0 \\ 0 &amp; 4 \end {array} \right )\frac {1}{3}\left (\begin {array}{cc} 1 &amp; -1 \\ 1 &amp; 2 \end {array} \right )~.  \end {equation*}
and it follows that \begin {equation*}  \begin {aligned} A^{20}=PD^{20}P^{-1}&amp;=\tfrac {1}{3}\left (\begin {array}{cc} 2 &amp; 1 \\ -1 &amp; 1 \end {array} \right )\left (\begin {array}{cc} 1 &amp; 0 \\ 0 &amp; 4^{20} \end {array} \right )\left (\begin {array}{cc} 1 &amp; -1 \\ 1 &amp; 2 \end {array} \right )\\&amp;=\tfrac {1}{3}\left (\begin {array}{cc} 2+4^{20} &amp; -2+2\times 4^{20} \\ -1+4^{20} &amp; 1+2\times 4^{20} \end {array} \right ) \end {aligned}  \end {equation*}
</p> 
</div> 
</div>                                                                                                                      
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
                                                                                          
<h3 class="sectionHead"><span class="titlemark">8   </span> <a 
 id="x1-700008"></a>Stochastic Matrices</h3>
<!--l. 5--><p class="noindent" >In the previous chapter, we introduced new tools to study (linear) dynamical systems such as dynamics
of animal population. In such a model, animals die and are born, and the total number of animals can
vary (and as we have seen, the asymptotic behaviour is closely related to the eigenavlues of the
associated matrix.)
</p><!--l. 7--><p class="noindent" >In this chapter, we focus on a class of dynamical systems where the total number of ‘elements’ under
study stays the same, modelling instead a phenomenon of <span 
class="rm-lmsso-12">redistribution</span>. As we will see in the following
motivating problem, many such dynamical systems reach some equilibrium, and the goal of this chapter
will be to understand why.
</p><!--l. 11--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">8.1   </span> <a 
 id="x1-710008.1"></a>A motivating problem: Optimising a distribution of bikes</h4>
<!--l. 13--><p class="noindent" >A company is planning to introduce to-rent bikes in Edinburgh. The plan is to have various spots
scattered in four main zones: Old Town (zone 1), Haymarket (zone 2), Fountainbridge (zone 3), and
Newington (zone 4). During a trial period, the company monitored the displacements of \(200\)
bikes to understand the behaviour of their possible customers. They noticed the following:
</p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 15--><p class="noindent" >On average, \(50\%\) of bikes in zone 1 are parked back in zone 1 at the beginning of the next day,
      \(20\%\) are parked in zone 2, \(20\%\) in zone 3, and \(10\%\) in zone 4.
      </p></li>
      <li class="itemize">
      <!--l. 16--><p class="noindent" >On average, \(20\%\) of bikes in zone 2 are parked back at zone 2 at the beginning of the next day,
      \(40\%\) are parked in zone 1, \(20\%\) in zone 3, and \(20\%\) in zone 4.
      </p></li>
      <li class="itemize">
      <!--l. 17--><p class="noindent" >On average, \(40\%\) of bikes in zone 3 are parked back in zone 3 at the beginning of the next day,
      \(30\%\) are parked in zone 1, \(20\%\) in zone 2, and \(10\%\) in zone 4.
      </p></li>
      <li class="itemize">
      <!--l. 18--><p class="noindent" >On average, \(50\%\) of bikes in zone 4 are parked back in zone 4 at the beginning of the next day,
      \(10\%\) are parked in zone 1, \(20\%\) in zone 2, and \(20\%\) in zone 3.</p></li></ul>
                                                                                          
                                                                                          
<!--l. 21--><p class="noindent" >A natural question (in order to decide the amount of parking slots needed) is to decide whether there
exists a stable distribution of bikes, that is, so that the number of bikes parked at the beginning of
every day remains constant over time. If we denote by \((x_1, \ldots , x_4)^T\) the vector whose components encode
the distribution of bikes in zone 1, \(\ldots \), 4 respectively, this amounts to solving the following
equation:
</p><!--l. 29--><p class="noindent" >\begin {equation*}  \begin {aligned} \left (\begin {array}{cccc} 0.5&amp; 0.4 &amp; 0.3 &amp; 0.1\\ 0.2 &amp; 0.2 &amp;0.2 &amp; 0.2 \\ 0.2&amp; 0.2&amp; 0.4 &amp; 0.2 \\ 0.1&amp; 0.2 &amp; 0.1 &amp; 0.5 \end {array}\right ) \vectttt {x_1}{x_2}{x_3}{x_4}= \vectttt {x_1}{x_2}{x_3}{x_4}. \end {aligned}  \end {equation*}
</p><!--l. 31--><p class="noindent" >This computation was done in Tutorial 1, and we found that the unique solution to this system was the
vector \((70, 40, 50, 40)^T\).
</p><!--l. 33--><p class="noindent" >However, the company might just have decided to start with an equal distribution of \(50\) bicycles at each
location. In that situation, one can study the evolution over time of the distribution of bicycles. Indeed,
if we denote by \(\xv _k\) the vector whose components encode the average distribution of bicycles in zones \(1,\) to \(4\)
after \(k\) days, then
</p>
<div class="math-display" >
<img 
src="F17ZD_main132x.svg" alt="            0.5  0.4  0.3  0.1                50
            0.2  0.2  0.2  0.2        k       50
⃗xk = ∖left( 0.2  0.2  0.4  0.2 ∖right) ∖left( 50 ∖right).

            0.1  0.2  0.1  0.5                50
" class="math-display"  /></div>
<!--l. 38--><p class="noindent" >We find:
</p>
<div class="math-display" >
<img 
src="F17ZD_main133x.svg" alt="            65                      68                       69.2                       69.7
            40                      40                        40                        40
⃗x1 = ∖left( 50 ∖right), ⃗x2 = ∖lef t( 50 ∖right ), ⃗x3 = ∖left(  50  ∖right), ⃗x4 ≃ ∖left(  50  ∖right),...

            45                      42                       40.8                       40.3
" class="math-display"  /></div>
<!--l. 41--><p class="noindent" >In particular, we see that, over time, the distribution seems to converge (rather quickly) to
the stable distribution we computed before. There are thus two natural questions to ask
oneself:
</p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 44--><p class="noindent" >Does that sort of system (and what sort of system exactly are we looking at?) always have
      a stable distribution?
      </p></li>
      <li class="itemize">
      <!--l. 45--><p class="noindent" >Does the distribution always evolve over time towards the same stable distribution, regardless
      of the initial conditions?</p></li></ul>
<!--l. 48--><p class="noindent" >The goal of this final chapter will be to answer in the affirmative these two questions, and to see further
applications.
</p><!--l. 50--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">8.2   </span> <a 
 id="x1-720008.2"></a>Markov diagrams and transition matrices</h4>
<!--l. 52--><p class="noindent" >The dynamical systems we wish to encode correspond to the evolution over time of a set of ‘elements’
(bikes, people, animals, etc.) that can be in a certain number of ‘states’. At every time, each element
can either stay in the same state or change to a different state with a given probability. The only
condition is that this process is <span 
class="rm-lmsso-12">without memory</span>, that is, the probability from moving to a state \(s_j\) to a
state \(s_i\) depends only on the states \(s_i, s_j\), and not of the states the process was at at previous times.
Such processes, heavily studied in probability theory and statistics, are called <span 
class="rm-lmsso-12">finite Markov</span>
<span 
class="rm-lmsso-12">chains</span>.
</p>
<!--l. 69--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="tcolorbox example" id="tcolobox-209">    
<div class="tcolorbox-title">
<!--l. 69--><p class="noindent" >Example 8.1</p></div> 
<div class="tcolorbox-content"></p><!--l. 64--><p class="noindent" >We give here some examples to illustrate the type of processes we are trying to model:
</p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 66--><p class="noindent" >The study of bikes mentioned in the introduction can be modelled in this way: the set of
        elements correspond to the \(200\) bikes considered, the states correspond to the four possible
        zones, and the averaged probabilities measured in the introduction give the probability of
        a bike to move from one state to another after one day.
        </p></li>
        <li class="itemize">
        <!--l. 67--><p class="noindent" >We flip a fair coin and record the occurrences of heads and tails. Here, the set of elements
        considered consists of the coin itself, and the two possible states are ‘heads’ and ’tails’.
        Since the coin is fair, the probability from moving from one state to the other is always
        \(\frac {1}{2}\).</p></li></ul>
 
</div> 
</div>                                                                                                                      
<!--l. 73--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-210">    
<div class="tcolorbox-title">
<!--l. 73--><p class="noindent" >Example 8.2</p></div> 
<div class="tcolorbox-content"></p><!--l. 72--><p class="noindent" >Here is an example of a process that cannot be modelled in this way. We play a game by flipping
a fair coin as follows: We start with \(10\) pounds. Every time we get heads we win \(1\) pound, and
every time we get tails we lose \(1\) pound. However, being reasonable, we decide to stop playing
if we lose three times in a row. One way we could try to model this game is by taking the set
of elements to consist of a single player, and the three states can be: ‘win’, ‘lose’, and ‘stop
playing’, and we go from one state to the other depending on the outcome of the coin flips.
This process however does not fit the previous description. Indeed, knowing that we just lost \(1\)
pound (i.e. we are currently on the ‘lose’ state), we cannot tell the probability to move to the
other states without knowledge of the prior events: if we had already lost twice before, then
we will stop playing and move to state ‘stop playing’ with probability \(1\). Otherwise, we will keep
playing and move to state ‘win’ or ‘lose’ with probability \(\frac {1}{2}\). </p> 
</div> 
</div>                                            
<!--l. 75--><p class="noindent" >Now that we have some intuition about the type of processes we are trying to model, we will present an
algebraic framework to study them. The key thing is to encode the various probabilities of
moving from one state to another in a workable way. This is done as follows, using a <span 
class="rm-lmsso-12">Markov</span>
<span 
class="rm-lmsso-12">diagram</span>.
</p>
<div class="tcolorbox df" id="tcolobox-211">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 85--><p class="noindent" >Definition 8.3  </p></div> 
<div class="tcolorbox-content"><!--l. 78--><p class="noindent" ><span 
class="rm-lmsso-12">Markov diagram </span>is a finite labelled oriented graph: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 81--><p class="noindent" >for each pair of vertices \(v_i\), \(v_j\), there is at most one oriented edge from \(v_j\) to \(v_i\), and the label
        of that edge is an element \(a_{ij} \in (0, 1]\).
        </p></li>
        <li class="itemize">
        <!--l. 82--><p class="noindent" >for each vertex \(v_j\), we have:
        </p>
<div class="math-display" >
<img 
src="F17ZD_main134x.svg" alt="∑n
   aij = 1.
i=1
" class="math-display"  /></div>
      </li></ul>
 
</div> 
</div>
<!--l. 87--><p class="noindent" >Intuitively, a Markov diagram encodes (in a diagrammatic way) the types of processes we are interested:
the vertices correspond to the states of our process, and an oriented edge with label \(a_{ij}\) from the
vertex \(v_j\) to the vertex \(v_i\) corresponds to a probability \(a_{ij}\) of moving from state \(v_j\) to state \(v_i\). We only
indicate non-zero probabilities in the diagram, hence the requirement that \(a_{ij} \in (0, 1]\). Moreover, the
second condition simply means that the diagram encodes all the possible transitions from one
state to the other: if we are at a state \(v_j\) at a given time, with probability \(1\) (\( = \sum _{i=1}^n a_{ij}\)) we will move to
one of the other states indicated in the diagram by following an oriented edge starting at
\(v_j\).
</p>
<!--l. 160--><p class="noindent" >
</p><!--l. 160--><p class="noindent" ><a 
 id="x1-72004r4"></a><!--l. 117--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 8.4</span>                                                                           </p><!--l. 117--><p class="noindent" >Let us consider again the example of the coin flip, and the coin is assumed to be fair: heads
and tails happen with probability \(\frac {1}{2}\). The Markov diagram for this process is the following:
</p>
<div class="center" 
>
<!--l. 124--><p class="noindent" >
</p><!--l. 125--><p class="noindent" ><img 
src="F17ZD_main135x.svg" alt="ht0000ea....ai5555dl
"  /></p></div>
<!--l. 142--><p class="noindent" >If instead we considered an unfair coin giving a head with a probability of \(60\%\), then the Markov diagram
would be: </p>
<div class="center" 
>
<!--l. 143--><p class="noindent" >
</p><!--l. 144--><p class="noindent" ><img 
src="F17ZD_main136x.svg" alt="ht0000ea....ai4664dl
"  /></p></div>
 
</div> 
</div>                                                                                        
</p><!--l. 163--><p class="noindent" >Another way to encode a this data is by means of a matrix, called a <span 
class="rm-lmsso-12">transition matrix</span>, which is
particularly useful to understand the evolution over time of the process.
</p>
<div class="tcolorbox df" id="tcolobox-212">    
<div class="tcolorbox-title">
<!--l. 168--><p class="noindent" >Definition 8.5  </p></div> 
<div class="tcolorbox-content"><!--l. 166--><p class="noindent" >The <span 
class="rm-lmsso-12">transition matrix </span>associated to a Markov diagram with \(n\) states is the \(n \times n\) matrix defined
by:
</p>
<div class="math-display" >
<img 
src="F17ZD_main137x.svg" alt="a  =  probablity of going from state s  at time n to state s at time n + 1.
 ij                                j                   i
" class="math-display"  /></div>
 
</div> 
</div>
                                                                                          
                                                                                          
<!--l. 183--><p class="noindent" >
</p><!--l. 183--><p class="noindent" ><a 
 id="x1-72006r6"></a><!--l. 173--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 8.6</span>                                                                           </p><!--l. 173--><p class="noindent" ></p>
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 174--><p class="noindent" >The transition matrix associated to the fair flip coin above is:
      </p>
<div class="math-display" >
<img 
src="F17ZD_main138x.svg" alt="A =  ∖lef t( 0.5 0.5 ∖right),
           0.5  0.5
" class="math-display"  /></div>
      <!--l. 176--><p class="noindent" >while the transition matrix associated to the unfair flip coin above is:
      </p>
<div class="math-display" >
<img 
src="F17ZD_main139x.svg" alt="           0.6  0.6
A =  ∖lef t(         ∖right),
           0.4  0.4
" class="math-display"  /></div>
      </li>
      <li class="itemize">
      <!--l. 178--><p class="noindent" >The introductory example involving bikes can be modelled using a Markov diagram with
      four states \(s_1, \ldots , s_4\) corresponding to the four zones \(1, \ldots , 4\) respectively. The associated transition matrix
      is:                                                                                
                                                                                          
                                                                                          
</p><!--l. 183--><p class="noindent" >      </p>
<div class="math-display" >
<img 
src="F17ZD_main140x.svg" alt="            0.5  0.4  0.3  0.1
A =  ∖left( 0.2  0.2  0.2  0.2 ∖right).
            0.2  0.2  0.4  0.2
            0.1  0.2  0.1  0.5
" class="math-display"  /></div>
      </li></ul>
 
</div> 
</div>                                                                                        
</p><!--l. 185--><p class="noindent" >Note that a transition matrix is a particular example of a stochastic matrix, as defined below:
</p>
<div class="tcolorbox df" id="tcolobox-213">    
<div class="tcolorbox-title">
<!--l. 193--><p class="noindent" >Definition 8.7  </p></div> 
<div class="tcolorbox-content"><!--l. 188--><p class="noindent" >An \(n \times n\) matrix is called a <span 
class="rm-lmsso-12">stochastic matrix </span>if the following holds: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 190--><p class="noindent" >\(a_{ij} \geq 0\) for every \( 1 \leq i, j \leq n\),
        </p></li>
        <li class="itemize">
        <!--l. 191--><p class="noindent" >for every column \(C_j\) of \(A\), the sum of its entries is \(1\).</p></li></ul>
 
</div> 
</div>
<!--l. 200--><p class="noindent" >The transition matrix can be used to model the evolution over time of the distribution of a given number
of elements taking at each time one of the possible states. Suppose that we start originally with \(x_1\)
elements in the state \(s_1\), \(x_2\) elements in the state \(s_2\), etc. We encode this by a vector \(\xv _0 = (x_1, \ldots , x_n)^T\), whose
coordinates record the initial distribution of objects under study. At each iteration of the process,
each object moves from one state to another with following the probabilities encoded in the
Markov diagram (or equivalently, in the transition matrix). At a given time, we have the
following:
</p>
<div class="tcolorbox thm" id="tcolobox-214">    
<div class="tcolorbox-title">
                                                                                          
                                                                                          
<!--l. 207--><p class="noindent" >Theorem 8.8  </p></div> 
<div class="tcolorbox-content"><!--l. 203--><p class="noindent" >Let \(\xv _0 = (x_1, \ldots , x_n)^T\) be the vector whose coordinates record the initial distribution of elements in states
\(s_1, \ldots , s_n\). Let us denote by \(\xv _k\) the vector whose components record the expected distribution after \(k\)
iterations of the process of elements in states \(s_1, \ldots , s_n\) respectively. We have for every \(k \geq 0\):
</p>
<div class="math-display" >
<img 
src="F17ZD_main141x.svg" alt="⃗x    = A ⃗x  for every k ≥ 0.
 k+1      k
" class="math-display"  /></div>
<!--l. 205--><p class="noindent" >In particular, we get by induction:
</p>
<div class="math-display" >
<img 
src="F17ZD_main142x.svg" alt="       k
⃗xk = A  ⃗x0.
" class="math-display"  /></div>
 
</div> 
</div>
<div class="tcolorbox df" id="tcolobox-215">    
<div class="tcolorbox-title">
<!--l. 211--><p class="noindent" >Definition 8.9  </p></div> 
<div class="tcolorbox-content"><!--l. 210--><p class="noindent" >A vector \(\xv \) such that \(A\xv =\xv \) is called a <span 
class="rm-lmsso-12">stationary distribution</span>. In other words, it is an eigenvector
of \(A\) for the eigenvalue \(1\). </p> 
</div> 
</div>
<!--l. 213--><p class="noindent" >A stationary distribution represents the distribution over the possible states when the system is at
equilibrium, i.e. does not change over time.
</p>
<!--l. 216--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-216">    
<div class="tcolorbox-title">
<!--l. 216--><p class="noindent" >Example 8.10</p></div> 
<div class="tcolorbox-content"></p><!--l. 215--><p class="noindent" >In the Markov chain modelling the distribution of bikes mentioned in the introduction, the
vector \((70, 40, 50, 40)^T\) is a stationary distribution. </p> 
</div> 
</div>                                                                          
                                                                                          
                                                                                          
<!--l. 219--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">8.3   </span> <a 
 id="x1-730008.3"></a>The Perron-Frobenius Theorem for stochastic matrices</h4>
<div class="tcolorbox df" id="tcolobox-217">    
<div class="tcolorbox-title">
<!--l. 230--><p class="noindent" >Definition 8.11  </p></div> 
<div class="tcolorbox-content"><!--l. 222--><p class="noindent" >Let \(A\) be an \(n \times n\) matrix. </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 224--><p class="noindent" >We say that \(A\) is <span 
class="rm-lmsso-12">non-negative </span>if \(a_{i j} \geq 0\) for every \(i, j\), and <span 
class="rm-lmsso-12">positive </span>if \(a_{i, j}&gt;0\) for every \(i, j\).
        </p></li>
        <li class="itemize">
        <!--l. 225--><p class="noindent" >We say that a non-negative matrix \(A\) is <span 
class="rm-lmsso-12">regular </span>if \(A^k \) is positive for some integer \(k \geq 1\).</p></li></ul>
 
</div> 
</div>
<!--l. 232--><p class="noindent" >The transition matrix associated to a Markov diagram is non-negative, but not necessarily positive as
there may be a probability \(0\) of going from some state \(s_i\) to some other state \(s_j\).
</p><!--l. 235--><p class="noindent" >The integer \(k\) in the definition above depends on the matrix. In particular, for every \(k\) it is possible to
construct a regular matrix \(A\) such that \(A^k\) is positive but \(A, A^2, \ldots , A^{k-1}\) are not positive. </p>
<div class="tcolorbox thm" id="tcolobox-218">    
<div class="tcolorbox-title">
<!--l. 240--><p class="noindent" >Theorem 8.12  </p></div> 
<div class="tcolorbox-content"><!--l. 237--><p class="noindent" >The transition matrix associated to a Markov diagram is regular if and only if there exists
an integer \(n \geq 1\) such that one can go from any vertex of its Markov diagram to any other by a
sequence of exactly \(n\) oriented edges. </p> 
</div> 
</div>
<div class="proof">
<!--l. 243--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>The \((i, j)\) term of \(A^k\) is given by:
</p>
                                                                                          
                                                                                          
<div class="math-display" >
<img 
src="F17ZD_main143x.svg" alt="   k          ∑
(A  )i,j =            ai,i1ai1,i2 ⋅⋅⋅aik−2,ik−1aik− 1,j.
          1≤i1,...,ik−1≤n
" class="math-display"  /></div>
<!--l. 245--><p class="noindent" >Since all the coefficients \(a_{i, j}\) are non-negative, \(A^k\) is positive if and only if for every \(i, j\), there exists \(1 \leq i_1, \ldots , i_{k-1} \leq n\) such
that \(a_{i_{}, i_{1}}a_{i_{1}, i_{2}}\cdots a_{i_{k-2}, i_{k-1}}a_{i_{k-1}, j}\) are all non-zero. Since a directed edge in the Markov diagram corresponds to a positive
probability to move from one state to the other, this is equivalent to saying that for every \(i, j\), there
exists a sequence of \(n\) directed edges from the state \(i\) to the state \(j\).                                     □
</p>
</div>
<!--l. 252--><p class="noindent" >
</p>
<div class="tcolorbox example" id="tcolobox-219">    
<div class="tcolorbox-title">
<!--l. 252--><p class="noindent" >Example 8.13</p></div> 
<div class="tcolorbox-content"></p><!--l. 249--><p class="noindent" >The following matrix is a regular stochastic matrix:
</p>
<div class="math-display" >
<img 
src="F17ZD_main144x.svg" alt="A :=  ∖left( 1∕2  1∕2 ∖right ).
            1∕2  1∕2
" class="math-display"  /></div>
<!--l. 251--><p class="noindent" >Indeed, it is clearly stochastic. Moreover, since \(A = A^1\) is positive, it is also regular. </p> 
</div> 
</div>                      
<!--l. 289--><p class="noindent" >
</p><!--l. 289--><p class="noindent" ><a 
 id="x1-73004r14"></a><!--l. 254--><p class="noindent" ><span 
class="rm-lmssbx-10x-x-120">Example 8.14</span>                                                                          </p><!--l. 254--><p class="noindent" >The following matrix is a regular stochastic matrix:
</p><!--l. 256--><p class="noindent" >
</p>
<div class="math-display" >                                                                                        
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p><!--l. 289--><p class="noindent" ><img 
src="F17ZD_main145x.svg" alt="            1∕3  1∕3    0   1∕3
            1∕3  1∕3  1 ∕3   0
A := ∖left(                     ∖right).
             0   1∕3  1 ∕3  1∕3
            1∕3   0   1 ∕3  1∕3
" class="math-display"  /></div>
<!--l. 257--><p class="noindent" >Indeed, the fact that \(A\) is stochastic is clear, and we have:
</p>
<div class="math-display" >
<img 
src="F17ZD_main146x.svg" alt="            1∕3  2∕9   2∕9  2∕9
            2∕9  1∕3   2∕9  2∕9
A2 = ∖left(                     ∖right),
            2∕9  2∕9   1∕3  2∕9
            2∕9  2∕9   2∕9  1∕3
" class="math-display"  /></div>
<!--l. 259--><p class="noindent" >which is positive. The corresponding Markov diagram is the following (where for simplicity we
omit the probabilities, and we represent arrows from \(s_i\) to \(s_j\) and from \(s_j\) to \(s_i\) simply by a double
arrow):
</p>
<div class="center" 
>
<!--l. 262--><p class="noindent" >
</p><!--l. 263--><p class="noindent" ><img 
src="F17ZD_main147x.svg" alt="ssss
  1234
"  /></p></div>
<!--l. 288--><p class="noindent" >and we see that it is indeed possible to find a directed path of length \(2\) between any pair of states.
</p> 
</div> 
</div>                                                                                        
</p><!--l. 291--><p class="noindent" >The key theorem of this chapter is the following:
</p>
<div class="tcolorbox thm" id="tcolobox-220">    
<div class="tcolorbox-title">
<!--l. 305--><p class="noindent" >Theorem 8.15  </p></div> 
<div class="tcolorbox-content"><!--l. 294--><p class="noindent" >Perron-Frobenius Theorem for stochastic matrices Let \(A\) be a regular stochastic matrix. Then the
following holds: </p>
        <ul class="itemize1">
        <li class="itemize">
        <!--l. 297--><p class="noindent" >There exists stationary distributions for \(A\). In other words, \(1\) is an eigenvalue of \(A\).
        </p><!--l. 299--><p class="noindent" >Moreover, there exists a stationary distribution all of whose components are strictly
        positive.
        </p></li>
        <li class="itemize">
        <!--l. 300--><p class="noindent" >For every vector \(\xv = (x_1, \ldots , x_n)^T\) such that \(\sum _i x_i \neq 0 \), we have that the sequence of vectors \(A^k\xv \) converges to a
        stationary distribution of \(A\).
        </p></li>
        <li class="itemize">
        <!--l. 302--><p class="noindent" >\(\dim E_1(A) = 1\).
        </p></li>
        <li class="itemize">
        <!--l. 303--><p class="noindent" >For every other eigenvalue \(\lambda \) of \(A\), we have \(|\lambda |&lt;1\).</p></li></ul>
 
</div> 
</div>
<!--l. 308--><p class="noindent" >The proof of the Perron-Frobenius is rather long and technical (you can find the proof on the internet,
or come and ask me after class/tutorials.). I will just present the proofs of two of the fours
                                                                                          
                                                                                          
results, that illustrate nicely some of the concepts we have encountered in the course so
far.
</p><!--l. 312--><p class="noindent" >For any square matrix, we have
</p>
<div class="math-display" >
<img 
src="F17ZD_main148x.svg" alt="det(AT ) = det(A).
" class="math-display"  /></div>
<!--l. 315--><p class="noindent" >A square matrix and its transpose have the same eigenvalues.
</p>
<div class="proof">
<!--l. 319--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>Indeed, since \(\det (A) = \det (A^T)\) for any square matrix, it follows that
</p>
<div class="math-display" >
<img 
src="F17ZD_main149x.svg" alt="                T                       T
χAT (λ) = det(A  −  λIn) = det((A − λIn ) ) = det(A − λIn) = χA (λ).
" class="math-display"  /></div>
<!--l. 320--><p class="noindent" >In particular,
</p>
                                                                                          
                                                                                          
<div class="math-display" >
<img 
src="F17ZD_main150x.svg" alt="                                                                                T
λ is an eigenvalue of A ⇐ ⇒  χA(λ ) = 0 ⇐ ⇒ χAT (λ) = 0 ⇐ ⇒  λ is an eigenvalue of A .
" class="math-display"  /></div>
                                                                                         □
</div>
<div class="proof">
<!--l. 325--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>(i) Notice that, since \(A\) is stochastic, the entries in each column of \(A\) add up to \(1\). This can
be reformulated as follows:
</p>
<div class="math-display" >
<img 
src="F17ZD_main151x.svg" alt="          1                  1
AT ∖lef t(  ..∖right ) = ∖lef t( ..∖right ).
           .                  .
          1                  1
" class="math-display"  /></div>
<!--l. 327--><p class="noindent" >In particular, we see that \(1\) is an eigenvalue of \(A^T\). But \(A\) and \(A^T\) have the same eigenvalues by the above
theorem. Since \(1\) is an eigenvalue of \(A^T\), it is also an eigenvalue of \(A\).
</p><!--l. 333--><p class="noindent" >(ii) Let us prove this statement in the case where \(A\) is diagonalisable. We choose a basis \(\ev _1, \ldots , \ev _n\) of
eigenvectors for the eigenvalues \(1 = \lambda _1, \lambda _2, \ldots , \lambda _n\) respectively. Let \(\xv \in \FR ^n\) and let us write \(\xv = \alpha _1\ev _1 + \ldots + \alpha _n\ev _n\), for some \(\alpha _1, \ldots , \alpha _n \in \FR \). We then have for \(k \geq 1\):
\begin {equation*}  \begin {aligned} A^k\xv &amp;= \alpha _1A^k\ev _1 + \alpha _2A^k\ev _2 + \ldots + \alpha _nA^k \ev _n \\ &amp; = \alpha _1\ev _1 + \alpha _2\lambda _2^k\ev _2 + \ldots + \alpha _n\lambda _n^k \ev _n. \end {aligned}  \end {equation*}
Since \(|\lambda _2|, \ldots , |\lambda _n| &lt; 1\) by (iv), it follows that \(A^k \xv \) converges to \(\alpha _1\ev _1\). To conclude that the limit is an eigenvector (for the
eigenvalue \(\lambda _1=1\)), it remains to show that \(\alpha _1\ev _1 \neq \nv \). (an eigenvector cannot be the null vector.) But since \(A\) is
stochastic, the sum of the components of \(A^k \xv \) remains the same at every stage. Since we started from a
vector \(\xv \) such that \(\sum _i x_i \neq 0\), it follows that the sum of the components of \(\alpha _1 \ev _1\) is non-zero, and in particular
\(\alpha _1\ev _1 \neq \nv \).                                                                                                                      □
                                                                                          
                                                                                          
</p>
</div>
<!--l. 345--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">8.4   </span> <a 
 id="x1-740008.4"></a>Application: stationary distributions in dynamical systems.</h4>
<!--l. 348--><p class="noindent" >Let us go back to the problem stated in the introduction. The problem of modelling the changes in the
distribution of bicycles can be modelled using a Markov diagram, whose transition matrix \(A\) was given in
the introduction. This associated transition matrix is regular, as it is positive. Let us denote by \(\xv _0 = (50, 50, 50, 50)^T\) the
vector corresponding to the initial distribution of bikes. After \(k\) days, the expected distribution of bikes is
given by \(A^k\xv _0\). By (iv), this sequence of vectors converges towards a stationary distribution \(\xv \). This
stationary distribution is a vector whose components add up to \(200\), since each vector \(A^k\xv _0\) satisfies the
same property (intuitively, the bikes are just redistributed, there is no gain nor loss of bike in
the process). By (ii), since the eigenspace \(E_1(A)\) is a line (i.e. has dimension \(1\)), there is a unique
vector of \(E_1(A)\) whose components add up to \(200\), and this is exactly our stationary distribution \(\xv \). Note
that this characterisation of \(\xv \) does not depend on the original distribution \(\xv _0\). This explains in
hindsight why the stationary distribution is unique, and does not depend on the original vector
\(\xv _0\).
</p><!--l. 353--><p class="noindent" >One natural question would be to understand the speed of convergence. When the transition \(A\)
matrix is diagonalisable, then there exists a basis \(\ev _1, \ldots , \ev _n\) of \(\FR ^n\) for the associated eigenvalues \(1 = \lambda _1 &gt; \lambda _2 \geq \ldots \geq \lambda _n &gt; -1\). In
particular, if we write our initial vector \(\xv \) as a linear combination of the \(\ev _i\), as \(\xv = \alpha _1\ev _1 + \ldots + \alpha _n\ev _n\), we have for
\(k \geq 1\):
</p>
<div class="math-display" >
<img 
src="F17ZD_main152x.svg" alt="Ak⃗x =  α1⃗e1 + α2λk2⃗e2 + ...+ αn λkn⃗en.
" class="math-display"  /></div>
<!--l. 355--><p class="noindent" >In particular, we see that \(A^k\xv \ra _\infty \alpha _1\ev _1\) and that \(||A^k\xv - \alpha _1\ev _1|| \) converges to zero exponentially, and as fast as \(\mu ^k\) (up to a
multiplicative constant), where
                                                                                          
                                                                                          
</p>
<div class="math-display" >
<img 
src="F17ZD_main153x.svg" alt="μ =  max  |λi|.
     2≤i≤n
" class="math-display"  /></div>
<!--l. 357--><p class="noindent" >Rephrased using Landau’s notations, we have:
</p>
<div class="math-display" >
<img 
src="F17ZD_main154x.svg" alt="||Ak ⃗x − ⃗e1|| = O (μk)
" class="math-display"  /></div>
<!--l. 359--><p class="noindent" >If \(A\) is not diagonalisable, a slightly weaker inequality holds, namely:
</p>
<div class="math-display" >
<img 
src="F17ZD_main155x.svg" alt="   k              k
||A  ⃗x − ⃗e1|| = o(μ )
" class="math-display"  /></div>
<!--l. 361--><p class="noindent" >for every \( \underset {2 \leq i \leq n}{\mbox {max}}|\lambda _i| &lt; \mu &lt; 1.\) We will not prove this stronger statement in this course.
                                                                                          
                                                                                          
</p><!--l. 368--><p class="noindent" >
</p>
<h4 class="subsectionHead"><span class="titlemark">8.5   </span> <a 
 id="x1-750008.5"></a>Application: The PageRank algorithm</h4>
<!--l. 373--><p class="noindent" >One can imagine the internet as a (very large) oriented graph. Vertices correspond to pages and oriented
edges correspond to links from a page to another.
</p><!--l. 375--><p class="noindent" >To this oriented graph, we can associate a finite Markov chain whose states correspond to the pages, by
assuming that a person on a given webpage will click with equal probability on one of the links of the
page. To construct the associated transition matrix, we denote by \(n_{ij}\) the number of links from page \(j\) to
page \(i\), and by \(d_j\) the total number of links on page \(j\), then the probability \(a_{ij}\) that someone on page \(j\) clicks on
a link leading to page \(i\) is:
</p>
<div class="math-display" >
<img 
src="F17ZD_main156x.svg" alt="      nij
aij = dj .
" class="math-display"  /></div>
<!--l. 377--><p class="noindent" >Let us consider a small example of a very small network consisting of four webpages \(p_1, \ldots , p_4\). The table
representing the links between pages is given below, where the coefficient in row \(i\) and column \(j\) represents
the number of links from page \(j\) to page \(i\):
</p>
<div class="center" 
>
<!--l. 379--><p class="noindent" >
</p>
<div class="tabular"> <table id="TBL-3" class="tabular" 
 
><colgroup id="TBL-3-1g"><col 
id="TBL-3-1" /></colgroup><colgroup id="TBL-3-2g"><col 
id="TBL-3-2" /><col 
id="TBL-3-3" /><col 
id="TBL-3-4" /><col 
id="TBL-3-5" /></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-3-1-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-1"  
class="td11">  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-2"  
class="td11"> \(p_1\)  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-3"  
class="td11"> \(p_2\)  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-4"  
class="td11"> \(p_3\)  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-1-5"  
class="td11"> \(p_4\)  </td></tr><tr class="hline" style="border-top:1px solid #000"><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td><td><hr /></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-2-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-1"  
class="td11">  \(p_1\) </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-2"  
class="td11"> 0 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-3"  
class="td11"> 0 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-4"  
class="td11"> 2 </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-5"  
class="td11"> 2</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-3-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-1"  
class="td11"> \(p_2\)  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-2"  
class="td11"> 2  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-3"  
class="td11"> 0  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-4"  
class="td11"> 2  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-3-5"  
class="td11"> 1  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-4-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-1"  
class="td11"> \(p_3\)  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-2"  
class="td11"> 1  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-3"  
class="td11"> 1  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-4"  
class="td11"> 0  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-5"  
class="td11"> 2  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-5-"><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-1"  
class="td11"> \(p_4\)  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-2"  
class="td11"> 1  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-3"  
class="td11"> 1  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-4"  
class="td11"> 1  </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-5"  
class="td11"> 0  </td>
</tr></table></div></div>
<!--l. 387--><p class="noindent" >The transition matrix associated to this Markov chain is:
</p><!--l. 389--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="math-display" >
<img 
src="F17ZD_main157x.svg" alt="             0    0   2∕5  2 ∕5
            1∕2   0   2∕5  1 ∕5
A =  ∖left( 1∕4  1∕2   0   2 ∕5 ∖right).

            1∕4  1∕2  1∕5    0
" class="math-display"  /></div>
<!--l. 396--><p class="noindent" >However, this model fails to take into account that quite often people will go to a webpage not because
it was linked from a previous page, but because they started thinking of something completely different,
independently of the content of the current page. This can be modelled by another Markov process
where at each time, the probability of going from any page to any other page is exactly \(1/n\), where \(n\) is the
number of pages (in our example, \(n=4\)). This finite Markov chain has a transition matrix a \(n \times n\) matrix of the
form
</p><!--l. 398--><p class="noindent" >
</p>
<div class="math-display" >
<img 
src="F17ZD_main158x.svg" alt="            1∕n  ⋅⋅⋅  1∕n
U =  ∖lef t(  ...   ...   ...  ∖right).

            1∕n  ⋅⋅⋅  1∕n
" class="math-display"  /></div>
<!--l. 400--><p class="noindent" >What Google does to take both processes into consideration is to consider that what is going on is a
combination of the previous cases. There exists a constant \(\alpha \), called <span 
class="rm-lmsso-12">damping factor</span>, such that with
probability \(\alpha \), a person will choose the next page by clicking on one of the links, and with probability \((1-\alpha )\) that
person will choose the next page completely at random. A standard value for \(\alpha \) is \(\alpha =0.85\). This corresponds
to a Markov process whose transition matrix is given by the following barycenter of \(A\) and
\(U\):
                                                                                          
                                                                                          
</p>
<div class="math-display" >
<img 
src="F17ZD_main159x.svg" alt="G := αA  + (1 − α)U.
" class="math-display"  /></div>
<!--l. 402--><p class="noindent" >Note that this transition matrix is now strictly positive, and in particular regular. We can thus apply the
Perron-Frobenius Theorem, which tells us that there exists a stationary distribution with positive
components and with sum \(1\), called the <span 
class="rm-lmsso-12">ranking vector</span>. By the convergence property, this corresponds to
the expected distribution of people on pages when this process has been repeated infinitely many times.
Pages with a comparatively high distribution correspond to ‘popular’ pages, and are thus ranked
highly by Google. This ranking vector is (a simplified version of) what Google uses to rank
pages.
</p>
<div class="tcolorbox df" id="tcolobox-221">    
<div class="tcolorbox-title">
<!--l. 408--><p class="noindent" >Definition 8.16  </p></div> 
<div class="tcolorbox-content"><!--l. 405--><p class="noindent" >A <span 
class="rm-lmsso-12">ranking vector </span>used in the Google PageRank algorithm is a stationary distribution of the
‘Google matrix’ \(G = \alpha A + (1-\alpha ) U\) with positive components. (Such a vector is unique up to multiplication
by a positive constant, by the Perron-Frobenius Theorem)
</p><!--l. 407--><p class="noindent" >The ranking of the webpages is obtained by ranking the components of that vector. </p> 
</div> 
</div>
<!--l. 410--><p class="noindent" >Let us go back to our simple example. Taking the damping factor to be \(0.85\), the new transition matrix for
the process is \begin {equation*}  \begin {aligned} G ~~~&amp;= ~~~0.85 \left (\begin {array}{cccc} 0&amp; 0&amp; 2/5 &amp; 2/5\\ 1/2 &amp; 0 &amp;2/5 &amp; 1/5 \\ 1/4&amp; 1/2&amp; 0 &amp; 2/5 \\ 1/4&amp; 1/2 &amp; 1/5 &amp; 0 \end {array}\right ) + 0.15 \left (\begin {array}{cccc} 1/4&amp; 1/4 &amp; 1/4 &amp; 1/4\\ 1/4&amp; 1/4 &amp; 1/4 &amp; 1/4 \\ 1/4&amp; 1/4 &amp; 1/4 &amp; 1/4 \\ 1/4&amp; 1/4 &amp; 1/4 &amp; 1/4 \end {array}\right )\\ &amp;= ~~~ \left (\begin {array}{cccc} 0.0375&amp; 0.0375&amp; 0.3775 &amp; 0.3775\\ 0.4625 &amp; 0.0375 &amp;0.3775 &amp; 0.2075 \\ 0.25&amp; 0.4625&amp; 0.0375 &amp; 0.3775 \\ 0.25&amp; 0.4625 &amp; 0.2075 &amp; 0.0375 \end {array}\right ). \end {aligned}  \end {equation*}
</p><!--l. 424--><p class="noindent" >Finding the stationary distribution corresponds to solving the system \(G\xv = \xv \), which we do as usual using
Gaussian elimination, with a computer as the computations are too cumbersome. The unique stationary
distribution \(\rv \) whose components are positive and add up to \(1\) is (with approximations to the second
digit)
</p>
<div class="math-display" >
<img 
src="F17ZD_main160x.svg" alt="             0.21
             0.26
⃗r  ≃  ∖left( 0.28  ∖right).

             0.24
" class="math-display"  /></div>
<!--l. 426--><p class="noindent" >So the ranking of these pages, from most popular to least popular, is: page \(3\), page \(2\), page \( 4\), page
\( 1\).
</p>
<!--l. 428--><p class="noindent" ><span class="paragraphHead"><a 
 id="x1-760008.5"></a><span 
class="rm-lmssbx-10x-x-120">Speed of convergence (extra-curricular).</span></span>
We know from the Perron-Frobenius Theorem that a way to obtain a ranking vector is to look at the
asymptotic behaviour of \(G^k \xv \) for an arbitrary vector \(\xv \) with \(\sum x_i =1\). We wish to find an upper bound on the norm \( ||G^k\xv - \rv ||\).
However,instead of looking at this norm, we will be looking at a slightly different one that is easier to
use in this situation:
</p><!--l. 430--><p class="noindent" >
</p>
<div class="math-display" >
<img 
src="F17ZD_main161x.svg" alt="||⃗x|| = ∑  |x |.
    1    i   i
" class="math-display"  /></div>
<!--l. 431--><p class="noindent" >Note that we still have the triangle identity \(||\xv + \yv ||_1 \leq ||\xv ||_1 + ||\yv ||_1\). Moreover, for every vector \(\xv = (x_1, \ldots , x_n)^T\), we have:
</p>
<div class="math-display" >
<img 
src="F17ZD_main162x.svg" alt="         ∑   ∑           ∑  ∑           ∑       ∑        ∑
||A ⃗x||1 =    |    aijxj| ≤       aij|xj| ≤    |xj|(   aij) ≤    |xi| = ||⃗x||1.
          i   j           i  j           j       i        i
                                                                                          
                                                                                          
" class="math-display"  /></div>
<div class="tcolorbox thm" id="tcolobox-222">    
<div class="tcolorbox-title">
<!--l. 437--><p class="noindent" >Theorem 8.17  F</p></div> 
<div class="tcolorbox-content"><!--l. 435--><p class="noindent" >or every \(k \geq 1\) and every positive vector \(\xv \) whose components add up to \(1\), we have:
</p>
<div class="math-display" >
<img 
src="F17ZD_main163x.svg" alt="   k             k
||G  ⃗x − ⃗r||1 ≤ 2 α .
" class="math-display"  /></div>
 
</div> 
</div>
<div class="proof">
<!--l. 439--><p class="noindent" ><span class="head">
<span 
class="rm-lmsso-12">Proof.</span> </span>We have:
</p><!--l. 441--><p class="noindent" >
</p>
<div class="math-display" >
<img 
src="F17ZD_main164x.svg" alt="                                               1 ∕n
                                                 .
G ⃗x = αA ⃗x + (1 − α)U ⃗x = αA ⃗x + (1 − α )∖lef t(  ..  ∖right).
                                               1 ∕n
" class="math-display"  /></div>
<!--l. 443--><p class="noindent" >In particular, for non-negative vectors \(\xv , \yv \), we get
</p><!--l. 445--><p class="noindent" >
                                                                                          
                                                                                          
</p>
<div class="math-display" >
<img 
src="F17ZD_main165x.svg" alt="G⃗x − G ⃗y = αA (⃗x − ⃗y ),
" class="math-display"  /></div>
<!--l. 446--><p class="noindent" >and
</p>
<div class="math-display" >
<img 
src="F17ZD_main166x.svg" alt="||G ⃗x − G⃗y ||1 = α||A (⃗x − ⃗y)||1 ≤ α ||⃗x − ⃗y||1.
" class="math-display"  /></div>
<!--l. 447--><p class="noindent" >By induction, we get
</p>
<div class="math-display" >
<img 
src="F17ZD_main167x.svg" alt="||Gk ⃗x − Gk⃗y ||1 ≤ αk||⃗x − ⃗y||1
" class="math-display"  /></div>
<!--l. 449--><p class="noindent" >and thus, for every positive vector \(\xv \) whose components add up to \(1\), we get:
</p>
                                                                                          
                                                                                          
<div class="math-display" >
<img 
src="F17ZD_main168x.svg" alt="   k              k      k       k             k                    k
||G ⃗x − ⃗r ||1 = ||G  ⃗x − G ⃗r||1 ≤ α ||⃗x − ⃗r||1 ≤ α (||⃗x ||1 + ||⃗r||1) ≤ 2α  .
" class="math-display"  /></div>
                                                                                         □
</div>
<!--l. 451--><p class="noindent" >Note in particular that this speed of convergence is independent on the size of the matrix. For instance,
for \(\alpha = 0.85\), then the power method gives an approximation of the ranking vector with error less than \(10^{-6}\) after \( 90\)
iterations.
                                                                                          
                                                                                          
                                                                                          
                                                                                          
</p>
 
</body> 
</html>
                                                                                          
                                                                                          
                                                                                          


